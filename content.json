{"meta":{"title":"Hexo","subtitle":"","description":"","author":"Niklaus Yu","url":"http://yoursite.com","root":"/"},"pages":[{"title":"分类","date":"2020-04-22T16:03:17.000Z","updated":"2020-04-30T16:13:48.413Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"友情鏈接","date":"2020-04-30T15:46:22.000Z","updated":"2020-04-30T15:48:58.934Z","comments":true,"path":"link/index.html","permalink":"http://yoursite.com/link/index.html","excerpt":"","text":""},{"title":"标签","date":"2020-04-22T15:59:11.000Z","updated":"2020-04-22T16:01:38.386Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""},{"title":"archives","date":"2020-04-22T16:19:32.000Z","updated":"2020-04-22T16:20:25.385Z","comments":true,"path":"archives/index.html","permalink":"http://yoursite.com/archives/index.html","excerpt":"","text":""}],"posts":[{"title":"纠删码技术","slug":"ErasureCodes","date":"2020-05-16T14:40:09.000Z","updated":"2020-05-16T14:42:08.104Z","comments":true,"path":"2020/05/16/ErasureCodes/","link":"","permalink":"http://yoursite.com/2020/05/16/ErasureCodes/","excerpt":"","text":"","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"Untitled","slug":"Untitled","date":"2020-05-16T14:38:08.175Z","updated":"2020-05-16T14:38:08.175Z","comments":true,"path":"2020/05/16/Untitled/","link":"","permalink":"http://yoursite.com/2020/05/16/Untitled/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"SparkSql","slug":"SparkSql","date":"2020-05-15T16:00:00.000Z","updated":"2020-05-16T14:37:47.023Z","comments":true,"path":"2020/05/16/SparkSql/","link":"","permalink":"http://yoursite.com/2020/05/16/SparkSql/","excerpt":"","text":"","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"推荐系统","slug":"RecommendedSystem","date":"2020-05-15T11:38:20.000Z","updated":"2020-05-16T14:44:58.809Z","comments":true,"path":"2020/05/15/RecommendedSystem/","link":"","permalink":"http://yoursite.com/2020/05/15/RecommendedSystem/","excerpt":"","text":"","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"Machine Learning","slug":"MachineLearning","date":"2020-05-13T12:28:37.316Z","updated":"2020-05-15T15:07:13.155Z","comments":true,"path":"2020/05/13/MachineLearning/","link":"","permalink":"http://yoursite.com/2020/05/13/MachineLearning/","excerpt":"","text":"机器学习","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"GC","slug":"GC","date":"2020-05-12T03:12:09.000Z","updated":"2020-05-15T15:06:13.307Z","comments":true,"path":"2020/05/12/GC/","link":"","permalink":"http://yoursite.com/2020/05/12/GC/","excerpt":"","text":"Garbage Collection","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"算法","slug":"arithmetic","date":"2020-05-12T00:27:00.000Z","updated":"2020-05-13T14:38:20.655Z","comments":true,"path":"2020/05/12/arithmetic/","link":"","permalink":"http://yoursite.com/2020/05/12/arithmetic/","excerpt":"","text":"排序算法定义排序是将一组数据，依指定的顺序进行排列的过程。排序的分类： 内部排序指将需要处理的所有数据都加载到内部存储器中进行排序 外部排序法数据量过大，无法全部加载到内存中，需要借助外部存储进行排序算法的时间复杂度度量一个程序（算法）执行时间的两种方法 事后统计的方法 事前估算的方法通过分析某个算法的时间复杂度来判断哪个算法更优 常见的时间复杂度常数阶O（1）无论代码执行了多少行，只要没有循环等复杂结构，那这个代码的时间复杂度就是O（1） 冒泡排序（BubbleSort）（O（n^2））1234567891011121314151617181920212223public class BubbleSort &#123; public static void main(String[] args) &#123; boolean flag = false;//标记是否执行 int[] arr = &#123;3, 9, -1, 10, -2&#125;; for (int i = 0; i &lt; arr.length - 1; i++) &#123; for (int j = 0; j &lt; arr.length - 1 - i; j++) &#123; if (arr[j] &gt; arr[j + 1]) &#123; flag=true; int tmp = arr[j]; arr[j] = arr[j + 1]; arr[j + 1] = tmp; &#125; &#125; if (flag==false)&#123; break; &#125;else &#123; flag=false;//重置false &#125; &#125; System.out.println(Arrays.toString(arr)); &#125;&#125; 选择排序（O（n^2））12345678910111213141516171819202122//选择排序public class SelectSort &#123; public static void main(String[] args) &#123; int[] arr = &#123;3, 9, -1, 10, -2&#125;; SelectSort.selectSort(arr); &#125; public static void selectSort(int[] arr) &#123; for (int i = 0; i &lt; arr.length - 1; i++) &#123; int minIndex = i; int min = arr[i]; for (int j = i + 1; j &lt; arr.length; j++) &#123; if (min &gt; arr[j]) &#123; min = arr[j]; minIndex = j; &#125; &#125; arr[minIndex] = arr[i]; arr[i] = min; &#125; System.out.println(Arrays.toString(arr)); &#125;&#125; 插入排序插入排序（Insertion Sorting）的思想：把n个待排序的元素看成为一个有序表和一个无序表，开始时有序表中只包含一个元素，无序表包含n-1个元素，排序过程中每次从无序中取出第一个元素，把它的排序码依次与有序表元素的排序码进行比较，将它插入到有序表中的适当位置，使之成为新的有序表。 123456789101112131415161718192021222324252627//插入排序public class InsertSort &#123; public static void main(String[] args) &#123; int[] arr = &#123;101, 34, 119, 1&#125;; insertSort(arr); &#125; public static void insertSort(int[] arr) &#123; for (int i = 1; i &lt; arr.length; i++) &#123; //定义带插入的数 int insertVal = arr[i]; int insertIndex = i - 1; //给insertVal找到插入位置 while (insertIndex &gt;= 0 &amp;&amp; insertVal &lt; arr[insertIndex]) &#123; arr[insertIndex + 1] = arr[insertIndex]; insertIndex--; &#125; //当退出while循环时，说明 arr[insertIndex + 1] = insertVal; &#125; System.out.println(Arrays.toString(arr)); &#125;&#125;","categories":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/categories/Java/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"Spark","slug":"Spark","date":"2020-05-08T04:22:20.000Z","updated":"2020-05-11T10:22:34.762Z","comments":true,"path":"2020/05/08/Spark/","link":"","permalink":"http://yoursite.com/2020/05/08/Spark/","excerpt":"","text":"简介spark是一个开源免费的，通用的，可靠性的分布式计算框架，可用于处理海量的数据Apache Spark™是用于大规模数据处理的统一分析引擎。 常见的计算框架 MapReduce -&gt; 离线批处理 Spark -&gt;离线批处理+实时处理 Storm -&gt;实时处理 Flink -&gt;实时处理 Hadoop MapReduce 存在问题当发生shuffle时底层会产生大量的磁盘I/O，会降低整体的性能。此外还会产生大量的排序操作，耗费cpu，并且不能做到对中间结果集的服用，导致大量的重新计算，也会降低整体性能 所以spark在设计时，吸取了MapReduce的经验教训，做出相应的改进和优化，比如尽量减少shuffle的产生，减少不必要的排序，以及支持对结果集的复用 spark的模块构成 从上面可以看出，spark团队的目标是能够一栈式处理大数据所有场景： 离线批处理 数据交互式查询以及数据仓库 实时计算 复杂的数据分析 spark的使用模式常见的有三种 local模式：本地单机模式。一般用于练习或测试 Standlone模式：Spark集群模式。集群的资源管理由spark自身来管理 On Yarn模式：spark集群模式。集群的资源由yarn管理 RDD弹性分布式数据集。初学时，把RDD看做是一种集合，用于存储数据，类比于Array或List。RDD的特点在于： RDD有分区机制：可以分布式并行的处理同一个RDD数据集，从而极大提高处理效率。RDD分区数量由程序员自己定 RDD有容错机制，分区数据丢失可以进行恢复 RDD是Spark最核心的概念，需要把数据封装到RDD中，然后通过spark计算引擎进行处理创建RDD的两种途径： 将一个普通的集合（Array或List）转变成RDD val r1=sc.parallelize(a1,2) sc：spark context a1 ：普通集合 2:RDD的分区数r1.partitions.size 查看分区数r1.glom.collect :查看每个分区的数据 val r2 =sc.makeRDD(List(1,2,3,4),2)2. 读取外部存储系统文件（Linux，windows，hdfs等等）将整个文件的数据变为RDD以性为单位进行处理，即一行是一条数据3. 读取本地文件 val r4 = sc.textFile(“file:///home/1.txt”,2)4. 读取hdfs文件：val r5 = sc.textFile(“hdfs://hadoop01:9000/1.txt”,2) RDD的操作RDD的操作总分有三种 Transformation 变换操作，变换操作都是懒操作（懒方法），即调用方法后，不是马上执行，当触发某一个Action方法时才会真正执行，比如textFile就是一个懒方法。此外，每当调用一个懒方法，就会返回一个新的RDD Action执行操作 Controller控制操作 Spark的DAG（有向无环图）12//单词统计data.flatMap(_.split(\" \")).map((_,1)).reduceByKey(_+_) DAG中记录了RDD之间的依赖关系，借助RDD之间的依赖关系可以实现数据容错，即某一个子分区数据丢失时，找他对应的父分区结合依赖关系进行恢复 RDD的依赖关系 窄依赖 ：对于窄依赖操作，它们只是将Partition的数据根据转换的规则进行转化，并不涉及其他的处理，可以简单地认为只是将数据从一个形式转换到另一个形式。所以对于窄依赖，并不会引入昂贵的Shuffle。所以执行效率非常高。如果整个DAG中存在多个连续的窄依赖，则可以将这些连续的窄依赖整合到一起连续执行，中间不执行shuffle 从而提高效率，这样的优化方式称之为流水线优化。此外，针对窄依赖，如果子RDD某个分区数据丢失，只需要找到父RDD对应依赖的分区，恢复即可。但如果是宽依赖，当分区丢失时，最糟糕的情况是要重算所有父RDD的所有分区。 宽依赖，对于groupByKey这样的操作，子RDD的所有Partition（s）会依赖于parent RDD的所有Partition（s），子RDD的Partition是parent RDD的所有Partition Shuffle的结果。宽依赖会产生shuffle，底层会发生磁盘I/O以及网络数据通信较为复杂过程，所以性能上会降低，spark也是会产生shuffle，窄依赖就没有shuffle，所以会提高整体的执行效率 shuffle（洗牌） 即按照分组（分区条件）将数据分发到正确分区 Stage的划分1）Spark在执行Transformation类型操作时都不会立即执行，而是懒执行（计算）2）执行若干步的Transformation类型的操作后，一旦遇到Action类型操作时，才会真正触发执行（计算）3）执行时，从当前Action方法向前回溯，如果遇到的是窄依赖则应用流水线优化，继续向前找，直到碰到某一个宽依赖4）因为宽依赖必须要进行shuffle，无法实现优化，所以将这一次段执行过程组装为一个stage5）再从当前宽依赖开始继续向前找。重复刚才的步骤，从而将整个DAG还分为若干的stage stage（阶段），本质上一组task的集合，一个分区对应一个task，连续窄依赖会出现流水线优化一个分区对应一个task，一个action执行对应一个job一个app（Application）应用程序，一个Ap至少有一个job spark集群搭建修改conf下的spark-env.shSpark集群提交jar包以及运行的流程 Spark集群架构 Spark Context底层任务调度管理模块 Spark集群架构图细节补充 Spark Shuffle Manager![截屏2020-05-11 下午6.16.56](/Users/yuxiangrui/blog/source/picture/截屏2020-05-11 下午6.16.56.png) 一、Hash Based Shuffle Manager![截屏2020-05-11 下午6.18.38](/Users/yuxiangrui/blog/source/picture/截屏2020-05-11 下午6.18.38.png) 二、Sort Based Shuffle Manager![截屏2020-05-11 下午6.18.58](/Users/yuxiangrui/blog/source/picture/截屏2020-05-11 下午6.18.58.png) RDD的持久化机制![截屏2020-05-11 下午6.19.06](/Users/yuxiangrui/blog/source/picture/截屏2020-05-11 下午6.19.06.png) Spark的Checkpoint机制![截屏2020-05-11 下午6.19.26](/Users/yuxiangrui/blog/source/picture/截屏2020-05-11 下午6.19.26.png)","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"redis","slug":"redis","date":"2020-04-29T12:12:20.000Z","updated":"2020-04-29T13:01:26.974Z","comments":true,"path":"2020/04/29/redis/","link":"","permalink":"http://yoursite.com/2020/04/29/redis/","excerpt":"","text":"什么是 Redis? redis是由Apache提供的高性能的非关系型数据库 redis的特点 redis支持数据的持久化，支持将内存中的数据持久到磁盘上，这样可以避免重启或服务器宕机引起数据的丢失 redis不仅仅支持key-value这样类型的数据，还支持list ，set ，zset，hash等数据结构的存储 redis常常以集群的方式部署在节点上，采用的主从结构，支持数据的备份，即master-slave模式的数据备份 Redis 优势 redis是基于内存存储数据的，所以有高性能的读写速度 有丰富的数据类型 - redis支持 strings lists hashes sets数据类型的草错 原子性 –redis所有的操作都是原子性的，即要么同时成功，要么同时失败 丰富的特性 - redis支持的发布订阅 ，通知 和key过期等特性 Redis 与其他 key-value 存储有什么不同？ redis支持丰富的数据类型的数据的存储，如list ，set ，zset hash等 redis支持数据的持久化，可以将数据持久化到磁盘上防止数据丢失 redis提供原子性操作 redis在内存中操作数据更加的简单，以及有更快的读写效率 Redis支持的数据类型 支持5种数据类型 string hash list set zset（sorted set 有序集合） Redis 是单进程单线程的？ redis是单进程单线程的，redis利用队列技术将并发访问编程串行访问，消除了传统数据库串行控制的开销 Redis 的持久化机制是什么？各自的优缺点？ RDB 以数据集快照方式把持久化模式，记录redis数据库的所有键值对，在某个时间点将数据写入一个临时文件，持久化结束后，用这个临时文件替换上次持久化的文件达到数据恢复优点： 只有一个dump.rdb方便持久化 容灾性好，一个文件可以保存到安全的磁盘上 能最大化，fork 子进程来完成写操作，让主进程继续处理命令，所以是 IO最大化。使用单独子进程来进行持久化，主进程不会进行任何 IO 操作，保证了 redis的高性能) 4.相对于数据集大时，比 AOF 的启动效率更高。缺点： 数据安全性会降低，RDB会隔一段时间持久化，在持久化的过程中如果redis出现故障，则会造成数据的丢失 AOF（Append-only file）持久化方式： 是指所有的命令行记录以 redis 命令请求协议的格式完全持久化存储)保存为 aof 文件。 优点 数据安全，aof 持久化可以配置 appendfsync 属性，有 always，每进行一次命令操作就记录到 aof 文件中一次。 通过 append 模式写文件，即使中途服务器宕机，可以通过 redis-check-aof工具解决数据一致性问题。 AOF 机制的 rewrite 模式。AOF 文件没被 rewrite 之前（文件过大时会对命令进行合并重写），可以删除其中的某些命令（比如误操作的 flushall）) 缺点 AOF文件比RDB文件大，且恢复速度慢 数据集大的时候，比rdb启动效率低","categories":[{"name":"面试","slug":"面试","permalink":"http://yoursite.com/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"Kafka","slug":"Kafka","date":"2020-04-28T16:00:00.000Z","updated":"2020-04-30T12:16:12.183Z","comments":true,"path":"2020/04/29/Kafka/","link":"","permalink":"http://yoursite.com/2020/04/29/Kafka/","excerpt":"","text":"简介概述 Kafka是LinkedIn（领英）公司开发后来贡献给了Apache提供的一套分布式的消息系统（message system），是一个分布式流平台 流平台具有三个关键功能： 发布和订阅记录流，类似于消息队列或企业消息传递系统。 以容错的持久方式存储记录流。 处理记录流。 应用场景 建立实时流数据管道，以可靠地在系统或应用程序之间获取数据 构建实时流应用程序，以转换或响应数据流 Kafka本身是由Scala来完成的，所以Kafka天然支持高并发和分布式 Kafka会将生产者添加的数据放到磁盘存储，但是Kafka的读写速度在60-80M/S，Kafka底层采用了0拷贝 —指的是没有状态的转换 Kafka是纯粹的发布订阅的消息队列，数据可以被重复的消费 实际开发中，Kafka节点的数量一般是3-5个。以3个为例，Kafka集群的读写速度150-220M/S；如果是5个节点，那么集群的读写速度 offset机制的作用就是未来防止数据被同一个消费者重复读取 Kafka会把收集的数据放在log文件中记录，同时会建立index文件构成索引目录，使用的是稀疏索引（从log文件中随机选择数据建立索引），正因为这个样子。一般索引文件只能确定范围。在Kafka中一个log文件必定对应一个索引文件，log+index=segment 在查询index文件的时候，会利用二分查找快速锁定要查询的索引的所在范围 基本概念 broker：经纪人 一个Kafka节点就是一个broker 每一个broker都需要编号，只需要&gt;=0整数即可 producer：生产者 生产数据，将数据放到Kafka队列中 consumer：消费者 消费数据 将数据从Kafka队列中取出 topic:主题/话题 topic 的作用 对数据进行分类 在Kafka中,每一条数据必须对应一个topic 一个topic至少一个partition 在执行删除命令的时候，topic并不会立即删除，而是先标记为删除状态，之后在某一时刻才会被删除，如果需要删除命令立即生效需要讲server.properties 中的delete.topic.enable属性设置为true partition 有几个分区就会有几个目录 平均分配到每一个Kafka节点 保证数据请求不会集中在一个节点上，保证数据和请求的负载均衡 分区是有编号的，编号从0开始 replication-factor :副本因子 指定备份数量,来对数据备份 副本数量不能大于节点数量 consumer group :消费者组 在Kafka中，可以将一个消费者和多个消费者绑定在同一个组中，如果不指定，一个消费者就是一个消费者组, 数据在消费者组之间是共享的，在组内是竞争的 leader和follower leader和follower指的不是节点，而是副本关系 一个Kafka节点既可以有leader副本，也可以有follower副本 在Kafka启动的时候，会在zookeeper的控制喜爱来自动选举出leader副本 producer和consumer只会与leader副本交互 当producer向leader副本写入数据之后，leader副本写完之后就会等待，等待follower的询问，follower会不断的给leader副本发信息来询问是否有新数据需要同步；如果leader有新数据需要同步，那么leader就会将数据返回给follower；当follower同步完成之后，会给leader返回一个ack信号表示同步成功 leader在收到ack信号之后，会将返回ack的follower的brokerid放入队列中存储到zookeeper上，这个过程称之为ISR controller 当Kafka集群启动的时候，zookeeper会在某一个Kafka节点来启动一个进程controller controller负责对副本进行选举，在副本中选择出一个leader副本 在选举leader的时候controller会遵循： 1.如果集群是刚刚启动或者是重新启动，controller会优先选择先恢复过来的副本 2.当leader所在的节点宕机的时候，controller会优先从ISR中选择节点成为leader 3.如果ISR中存在多个节点，那么会从ISR中获取队头的节点上的副本切换为leader 消息队列/系统概述分类 Poll to Poll –&gt; p2p - 点对点模式 消息队列中的数据可以被每一个消费者消费 数据获取可以是pull或者push方法 publish/subscribe -&gt;P/S - 发布订阅模式 消息队列中的数据可以被多个消费者订阅 一般会提供持久化机制 作用 实现消费者和生产者之间的解耦。 在分布式系统下具备异步,削峰,负载均衡等一系列高级功能 拥有持久化的机制，进程消息，队列中的信息也可以保存下来。 CAP Consistency - 一致性 当访问集群时，能够获取到最新的数据 强一致性 在集群中，一个节点产生变化，其他节点会立即感知器变化，并随之改变 弱一致性 在集群中，一个节点产生变化 ，其他节点感知到部分变化或者没有感知 最终一致性：在集群中，无论各个节点怎么变化，处理完成后最终的数据是一样的 A - availability 可用性 ：在集群节点存活的情况下，访问集群能够立即获取有效的结果 P - Partition tolerance - 分区容忍性 在集群中，不会因为一个或者几个节点的宕机导致整个集群不对外提供服务 在分布式系统中，首先要考虑满足P，无法同时满足C和A，最多只能满足两个无法满足三个，所以分布式系统要么AP要么CP Consistency的方案： M/S - Master/Slave，例如Hadoop、HBase等 PAXOS算法及其变种，例如Zookeeper等 WNR策略，例如Cassandra等","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"MongoDB","slug":"MongoDB","date":"2020-04-27T22:48:00.000Z","updated":"2020-04-29T00:48:05.741Z","comments":true,"path":"2020/04/28/MongoDB/","link":"","permalink":"http://yoursite.com/2020/04/28/MongoDB/","excerpt":"","text":"简介 MongoDB是为快速开发互联网Web应用而设计的数据库系统 MongoDB的设计目标时极简、灵活、作为Web应用栈的一部分 MongoDB的数据模型是面向文档的，所谓的文档是一种类似JSON的结构，简单理解MongoDB这个数据库中存的是各种各样的JSON。（BSON ）-&gt;binary json 二进制的JSON 特点：面向集合存储，易存储对象类型的数据；模式自由；支持动态查询；支持完全索引，包含内部对象；支持查询；支持复制和故障恢复；使用高效的二进制数据存储，包括大型对象（如视频）；自动处理碎片，以支持云计算层次的扩展性；支持RUBY，PYTHON，JAVA，C++，PHP，C#等多种语言；文件存储格式为BSON（一种JSON的扩展）；可通过网络访问。 适用场景：网站数据；缓存；大尺寸，低价值的数据；高伸缩性的场景；用于对象及JSON 数据的存储。 不适用场景：高度事务性的系统；传统的商业智能应用；需要SQL 的问题。 聚合管道将文档在一个管道处理完毕后将结果传递给下一个管道处理，管道操作是可以重复的引用式文档查询：需要两次查询 第一次查询用户地址的对象id（ObjectId）；第二次通过查询的id获取用户的详细地址信息。 集合中索引不能超过64个；索引名的长度不能超过125个字符；一个复合索引最多可以有31个字段。mongorestore 命令来恢复备份的数据。","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"HBase","slug":"HBase","date":"2020-04-25T06:23:09.000Z","updated":"2020-04-29T02:45:24.624Z","comments":true,"path":"2020/04/25/HBase/","link":"","permalink":"http://yoursite.com/2020/04/25/HBase/","excerpt":"","text":"简介概述 HBase是有Apache提供的基于Hadoop的分布式，可扩展的非关系型数据库 HBase可以管理很大的数据的表 - billions of rows X millions of columns HBase是Doug Cutting根据Google的BigTable来实现，所以HBase和BigTable的原理一模一样，只是BigTable是用C语言实现的，HBase是Java语言实现的 本身是非关系型数据库，底层是利用键值对来存储 在HBase中也不支持多表关联 HBase中数据类型只支持字符串和数字 适合存储稀疏数据 - 结构化数据和半结构化数据 在put的时候表名列族以及行键都一样，再一次put是更新的效果 hbase作为一个数据库提供了完整的增删改查的功能，但是hbase是基于hdfs来进行存储的，hdfs上的数据是允许一次写入多次读取不允许修改允许追加的，hbase如何实现“改”的功能 - 当执行“改”操作的时候，HBase并不是修改原来的数据，而是在HDFS中存储的文件的尾端来追加数据，每一条数据都会自动添加时间戳，这个时间戳就是版本 通过时间戳，每一次默认返回最新的数据 HBase中的表默认只保留一个版本的数据，也只返回一个版本的数据，如果保留版本，需要建表时需要手动指定 基本概念 Rowkey - 行键 在HBase中没有主键的概念，而是采用行键 行键不属于任意列族 建表的时候不需要指定行键，行键是在添加数据的时候手动指定 默人字典排序 Column Family - 列族/列族 在HBase中，一个表中至少包含一个列族，可以多个列族 理论上来说，列族的数量不限定，但一个表一般不超过三个。 一个列族中可以包含0到多个列 - HBase，列是可以动态增删的，建表的时候不需要指定列。 Cell - 单元 在HBase中，通过行键+列族+列+版本可以确定唯一数据，这个结构成为cell单元 namespace - 名称空间 类似于mysql中的database 默认自带两个名称空间default和hbase 如果没有指定默认使用default 基本指令 命令 解释 status 查看HBase状态 version 查看hbase的版本 whoami 查看当前用户 create ‘person’ ,{NAME =&gt;’basic’},{NAME =&gt; ‘info’},{NAME =&gt; ‘other’} 建表方式 List 查看已经建立的表 put ‘person’ ,’p1’ , ‘basic:name’ ,’Reason’ 表示向person表中的basic列族name列添加行键p1的数据Reason get ‘person’, ‘p1’, ‘basic:name’ 获取basic 列族name列的值 get ‘person’, ‘p1’, ‘basic’ 获取basic 列族所有值 get ‘person’ , ‘p1’ 获取行键p1所有的值 scan ‘person’ ,{COLUMNS =&gt; ‘basic:name’} 获取basic所有name列的值 scan ‘person’ ,{COLUMNS =&gt; ‘basic’} 获取basic列族所有列的值 scan ‘person’ 查看person表中的所有数据 delete ‘person’ ,’p2’ ,’other:phone’ 删除行键p2 other列族的phone字段 deleteall ‘person’ ,’p2’ 删除行键p2的所有数据 create ‘student’ , {NAME =&gt; ‘basic’ ,VERSIONS =&gt;3} ,{NAME =&gt; ‘info’ ,VERSIONS =&gt;5} 创建一个student表有basic 和info两个列族，basic保留三个版本，info保留5个版本 get ‘student’,’s1’ ,{COLUMN =&gt;’basic:age’,VERSIONS =&gt; 4} 查询student表列族为basic age字段的4个版本的值，但建表的时候指定三个版本，只能拿到最新的三个版本的值 disable ‘student’ 先禁用表 drop ‘student’ 再删除表 describe ‘person’ or desc ‘person’ 描述person表的信息 enable ‘person’ 启用person表 exits ‘person’ 判断person表是否存在 create ‘demo:person’ ,’basic’ ,’expand’ 在指定工作空间demo下创建person list_namespace_tables ‘demo’ 查看指定名称空间的表 list_namespace 查看所有的名称空间 create namespace ‘demo’ 创建名称空间demo disable_all ‘demo:.*’ 禁用demo空间下所有的表 drop_all ‘demo:.*’ 删除demo中所有的表 drop_namespace ‘demo’ 删除工作空间demo 理论HRegion 在HBase，会从行键方向上将一个表拆分成一个或者多个HRegion 每一个HRegion都会交会交由某一个HRegionServer来进行管理 由于行键是字典排序，所以HRegion之间的范围是不交叉的，也因此客户端在请求的时候会根据行键去访问不同的HRegionServer HRegion实际上是行键排序（默认是字典排序）后的按规则分割的连续的存储空间 一张Hbase表，可能有多个HRegion，每个HRegion达到一定大小（默认是10GB）时，进行分裂。 HRegion包含了一个到多个HStore，HStore的数量由列族来决定 HStore中包含一个1个memStore可能包含0个或多个storefile/hfile，hfile会落地到HDFS上进行存储 Zookeeper 当HBase第一次启动的时候，会在Zookeeper上注册一个持久节点 当HBase启动时候，HMaster会自动在zookeeper来注册一个/hbase/master节点 Backup HMaster会自动在Zookeeper的/hbase/backup-masters下自动注册临时节点 zookeeper在HBase中的作用注册中心的作用/统一注册HMaster HMaster的状态分为：Active（活跃）和Backup（备份） 在任意一个安装了HBase去启动hmaster，sh hbase-daemon.sh start master 在HBase中不限制Hmaster 的个数，先开启的HMaster会自动成为active状态，后开启的为backup状态 当Active HMaster收到请求之后需要将这个请求和Backup HMaster之间进行备份，也因此在实际开发过程中Hamster的个数一般不超过三个1active Hmaster +2 backup hmaster Active HMaster会定时给Zookeeper发送心跳，同时监控/hbase/backup-masters下的子节点个数变化，如果发现/hbase/backup-masters下的字节点个数增加说明新增了Backup Hmaster；反之说明有节点产生丢失 backup hmaster会定时给zookeeper发送心跳，同时监控/hbase/master节点是否存在，如果不存在意味着Active HMaster丢失 HMaster的作用 管理HRegionServer，主要是HRegion在HRegionServer之间的负载和转移 管理HBase中的表结构（DDL操作），但是HMaster不负责表数据（DML操作）的管理 集群结构角度的读写流程 在HBase0.96之前 从HBase0.96开始 去掉了-ROOT- 文件 客户端从zookeeper中获取.meta.文件的位置之后会自动进行缓存，从第二次操作开始，就不需要再去访问zookeeper 客户端从.meta.文件中读取数据之后也会自动进行缓存，但是如果发生了HRegion的分裂转移或者客户端宕机，那么缓存就会失效就需要重新建立缓存 HBase命令组 DDL(Data Definition Language)数据库模式定义语言， 是用于描述数据库中要存储的现实世界实体的语言。 DML（Data Manipulation Language）数据操纵语言，用户通过它可以实现对数据库的基本操作。 HRegionServer HRegionServer是HBase中的从节点，作用是用于管理HRegion，HRegion中的数据最终会以HFile形式存储到datanode 官方文档中，一个HRegionServer大概能管理1000个HRegion 一个HRegionServer由1个WAL(HLog)、1个BlockCache和0到多个HRegion来构成 WAL WAL - write ahead log 也称 Hlog HRegionServer在收到写操作之后，会先将这个写操作记录到WAL中，然后再将数据更新到对应的memStore中，这样的设计是为了防止数据的丢失 WAL是维系在磁盘中，当达到指定条件的时候，WAL就会产生滚动，产生一个新的WAL，原来的WAl会变成OLD_WAL，OLD_WAL会在某一时刻被清除，这样做的目的是为了节省磁盘空间 在HBase0.94版本之前，WAL是串行写；从HBase0.94版本开始，WAL引入了管道机制，所以允许进行并行写 在实际开发过程中，如果能够容忍一定的数据库的数据产生丢失，可以关闭WAL提高写入效率 BlockCache - （数据块）缓存 BlockCache是一个读缓存，即会讲读取出来的数据进行缓存 BlockCache在进行数据缓存的时候，采取了“局部性”原理 —— “局部性”原理：实际就是一个猜测的过程，根据一定条件猜测来提高命中率 时间局部性：在HBase中，如果一条数据被读取过，那么HBase就会把这条数据放入缓存 空间局部性：在HBase中，如果一条数据被读取过，那么HBase会把与这条数据相邻的数据发入缓存中 BlockCache会将数据维系在内存中，BlockCache大小是128M BlockCache在填满之后，采取LRU策略（Least recently used）抛弃最长时间不用的数据 HRegion HRegion由一个到HStore来构成的 ，HStore的数量由列族的数量来决定 每一个HStore中包含一个memStore以及0到多个HFile memStore是一个写缓存，维系在内存中，大小是128M memStore在达到指定条件之后，会产生冲刷，冲刷出一个HFile，HFile最终会落地到HDFS上 memStore冲刷条件： a) 当memStore满了之后，会冲刷产生一个HFile b）当我的WAL达到1G大小的时候，会冲刷产生一个HFile c) 当所有的memStore所占用的内存之和占到总内存的35%的时候，就会自动冲刷几个比较大的memstore 第三个条件在实际开发过程中产生的频率最高，这会使HDFS产生大量的小文件 Compaction机制 HBase中提供了Compaction机制，允许将多个HFile合并成一个HFile 分类： minor compact ：初次合并，在合并的时候，会讲相邻的几个小的HFile合并成一个较大的HFile不合并，合并完成之后依然存在多个HFile major compact ：主合并，在合并的时候。HFile无论多大，都会参与合并，最终会合并成一个HFile 在HBase中，如果不指定的情况下，默认使用minor compact 相对而言，minor compact 的效率更高一些，major compact虽然合并力度更大，但是在实际开发中，每次进行major compact都需要花费5-6个小时，在合并过程中，集群的大量资源(I/O、cpu等)倾斜到合并上，所以此时集群对外服务的效率会大大降低，所以major compact一般放在周末的凌晨进行 在合并的过程中，舍弃掉被标记为删除数据或者过时的数据，所以删除是在合并的过程中发生的 读写流程写流程 当HRegionServer在收到写请求之后，会将写请求记录到到WAl中，会将数据更新到memStore中 数据在memStore中会进行排序：行键字典排序—&gt;列族字典排序-&gt;列字典排序-&gt;时间戳倒序 当memStore达到冲刷条件的时候，会冲刷产生一个新的HFile，这个HFile是有序的 - 所有的HFile之间是局部有序的 正因为HFile是有序的，所以每一个HFile都把持了各自的起始行键和结束行键 HFile最终会落地到HDFS中以block形式存储 HFile的v1格式 DataBlock：存储数据，可以一个可以多个 包含一个magic和多个key value magic: 魔数 本质上一个随机数，用于校验的 KeyValue：DataBlock中存储的数据都是以键值对 最小存储单位datablock * MetaBlock：存储元数据，一般只出现在.meta.文件中，其他的HFile中的一般没有这一块 * FileInfo： 文件信息，对当前HFile的描述，例如文件大小信息 * DataIndex：DataBlock的索引值，记录每一个DataBlock在文件中的起始字节和结束字节位置 * MetaIndex：MetaBlock的索引值，记录每一个MetaBlock在文件中的起始字节和结束字节位置 * Trailer：在文件末尾，固定占4个字节大小，其中前2个字节记录DataIndex在文件的起始字节位置，后2个字节记录MetaIndex在文件的起始字节位置读流程 当HRegionServer收到请求之后，会首先从blockcahe中读取数据，如果没有读到，会从memstore中读 如果以上都没有读到，就会从HFile中读，会根据行键范围删选掉不符合范围的Hfile，再利用布隆过滤器筛选出一定没有的，缩小范围。","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"Hive","slug":"Hive","date":"2020-04-21T14:11:43.660Z","updated":"2020-04-25T08:34:55.612Z","comments":true,"path":"2020/04/21/Hive/","link":"","permalink":"http://yoursite.com/2020/04/21/Hive/","excerpt":"","text":"简介一、概述 Apache Hive™数据仓库软件有助于使用SQL读取，写入和管理驻留在分布式存储中的大型数据集。可以将结构投影到已经存储的数据上。提供了命令行工具和JDBC驱动程序以将用户连接到Hive。 提供了类SQL（HQL）语言来管理HDFS上的数据，底层会将sql转化为MapReduce执行，Hive适用于离线分析 在Hive中，每一个database在HDFS上对应一个目录 在Hive中没有主键的概念 一个表一旦建立之后，这个表中字段之间的间隔符就不能修改了 命令 create table p2 like person; 建立一个p2表和person表一样结构 insert overwrite local directory ‘/home/hivedemo’ row format delimited fields terminated by ‘ ‘ select * from person; 将查询数据传给本地目录中，以空格间隔，要求目录不存在，否则会被覆盖 表结构内部表和外部表概念 内部表:先在hive里建一张表，然后向这个表插入数据(用insert可以插入数据，也可以通过加载外部文件方式来插入数据)，这样的表称之为hive的内部表 外部表:HDFS里已经有数据了，然后，通过hive创建一张表来管理这个文件数据。则这样表称之为外部表。需要注意的是，hive外部表管理的是HDFS里的某一个目录下的文件数据外部表创建命令进入hive，执行:create external table stu (id int,name string) row format delimited fields terminated by ‘ ‘ location区别 对于内部表，在删除该表的时候，HDFS对应的目录节点会被删除 2. 对于外部表，在删除该表的时候，HDFS对应的目录节点不会删除 分区表 分区表的作用往往是对数据进行分类 基本操作建表create table cities(id int, name string) partitioned by(province string) row format delimited fields terminated by ‘ ‘;加载hebei分区数据load data local inpath ‘/home/hivedata/hebei.txt’ into table cities partition(province = ‘hebei’);加载guangdong分区数据load data local inpath ‘/home/hivedata/guangdong.txt’ into table cities partition(province = ‘guangdong’);加载jiangsu分区数据load data local inpath ‘/home/hivedata/jiangsu.txt’ into table cities partition(province = ‘jiangsu’); 每一个分区对应一个目录 如果在HDFS上新建了目录作为分区，那么需要在Hive中来手动添加分区alter table cities add partition(province=’shandong’) location ‘/user/hive/warehouse/hivedemo.db/cities/province=shandong’;修复表，但是注意这句SQL执行不稳定，有时候会执行失败msck repair table cities; 在Hive的分区表中，如果根据分区字段进行查询，那么只查询分区对应的目录从而提高查询效率；如果跨分区查询，查询效率反而会降低 在分区表中，分区字段在原始数据中是不存在的，是需要在加载数据的时候手动指定的 如果分区字段在原始文件中已经存在，那么需要进行动态分区 先建立一张表管理原始数据create table c_tmp(cid int, cname string, cpro string) row format delimited fields terminated by ‘ ‘; 加载数据load data local inpath ‘/home/hivedata/cities.txt’ into table c_tmp; 开启动态分区机制 - 从一张未分区表中查询数据放到一张已分区的表中，这个过程称之为动态分区set hive.exec.dynamic.partition.mode=nonstrict; 指定分区字段进行动态分区insert into table cities partition(province) select cid, cname, cpro from c_tmp distribute by cpro; 删除分区：alter table cities drop partition(province = ‘liaoning’); 在Hive中，支持多字段分区，前边的字段会包含后边的字段。指定了几个字段，就会形成一个几级的目录。当需要对数据进行多级分类的时候，那么这个时候可以多字段分区 - 实际开发中，往往是需要对数据进行多级分类建表语句create table product(id int, name string) partitioned by(kind string, sub string, subkind string) row format delimited fields terminated by ‘ ‘;加载数据load data local inpath ‘/home/hivedata/shirt.txt’ into table product partition(kind = ‘clothes’, sub=’coat’, subkind=’Tshirt’);load data local inpath ‘/home/hivedata/sports.txt’ into table product partition(kind = ‘clothes’, sub = ‘shoes’, subkind = ‘sports’);分桶表 分桶表的作用是对数据进行抽样 在Hive中，分桶表默认是不开启的，所以需要手动开启分桶机制 案例开启分桶机制set hive.enforce.bucketing=true;建立分桶表 - 表示根据name字段进行分桶 - 在分桶的时候会计算name字段的哈希码，然后利用哈希码进行二次计算确定放在哪个桶中create table c_bucket(id int, name string) clustered by(name) into 6 buckets row format delimited fields terminated by ‘ ‘;向分桶表中添加数据insert overwrite table c_bucket select id, name from cities;抽样select * from c_bucket tablesample (bucket 1 out of 3 on name); bucket x out of y：x表示起始桶编号，从1开始计算，y表示的是步长。bucket 1 out of 3表示从第1个桶开始抽取数据，每隔3个桶抽取一次。其中x&lt;=y 数据类型 Hive分为基本类型和复杂类型 复杂类型 array：数组类型，对应了Java中的数组或者集合建表table arr(nums1 array, nums2 array) row format delimited fields terminated by ' ' collection items terminated by ',';```1234567891011121314151617181920212223#### 加载数据load data local inpath &#39;&#x2F;home&#x2F;hivedata&#x2F;nums&#39; into table arr;#### 非空查询select nums1[5] from arr where nums1[5] is not null;1. map：映射类型。对应了Java中的映射Map#### 建表create table infos (id int, info map&lt;string,int&gt;) row format delimited fields terminated by &#39; &#39; map keys terminated by &#39;,&#39;;#### 加载数据load data local inpath &#39;&#x2F;home&#x2F;hivedata&#x2F;map.txt&#39; into table infos;#### 查询tom对应的值select info[&#39;tom&#39;] from infos where info[&#39;tom&#39;] is not null;1. struct：结构体类型。对应了Java中的对象#### 看作一个对象的四个属性来建表create external table scores(s struct&lt;name:string, chinese:int, math:int, english:int&gt;) row format delimited collection items terminated by &#39; &#39; location &#39;&#x2F;score&#39;;#### 查询每一个人的语文成绩select s.chinese from scores;### explode1. explode会将数组中的每一个元素取出来作为单独的一行处理2. 案例：单词统计1. 方式一#### 建表管理HDFS存在的words.txt文件&#96;&#96;&#96;create external table words(word array&lt;string&gt;) row format delimited collection items terminated by &#39; &#39; location &#39;&#x2F;words&#39;; 建表存储单词create table ws(w string);将words表中拆分出来的单词查询出来放到ws表中insert into table ws select explode(*) from words;统计每一个单词出现的次数select w, count(w) from ws group by w; 方式二建表管理HDFS存在的words.txt文件external table words(word array) row format delimited collection items terminated by ' ' location '/words';```12345678910111213141516171819202122232425262728#### 统计每一个单词出现的次数select w, count(w) from (select explode(*) w from words)ws group by w;### UDF1. UDF(User-Defined Function)是Hive提供一套用于自定义函数的机制#### 2. 案例1. 写一个类继承UDF类，覆盖其中的evaluate方法2. 打成jar包放到Linux上3. 在Hive中添加jar包：add jar &#x2F;home&#x2F;hivedata&#x2F;hive-1.0-SNAPSHOT.jar;4. 定义一个临时函数，并且给函数绑定类：create temporary function repeatstring as &#39;cn.tedu.udf.RepeatString&#39;;### join1. 如果在join的时候没有指定，那么默认就是inner join2. 案例#### 先建立orders表create external table orders(orderid int, orderdate string, productid string, num int) row format delimited fields terminated by &#39; &#39; location &#39;&#x2F;orders&#39;;#### 建立products表create external table products(productid int, name string, price double) row format delimited fields terminated by &#39; &#39; location &#39;&#x2F;products&#39;;#### 统计每一天的售出的商品的总价select o.orderdate, sum(o.num * p.price) from orders o join products p on o.productid &#x3D; p.productid group by orderdate;1. Hive中，支持inner&#x2F;left&#x2F;right&#x2F;full outer join，还支持left semi join。如果a left semi join b，那么表示查询a表中哪些数据在b表中出现过：select * from products p left semi join orders o on p.productid &#x3D; o.productid;### Serde1. Serde是Hive提供的一套针对不规则数据进行处理的机制2. 在Serde中，利用正则的捕获组来对应字段，正则表达式中的每一个捕获组对应了Hive表中的一个字段3. 案例#### 建表&#96;&#96;&#96;create table logs(ip string, time string, timezone string, request_way string, resource string, protocol string, stateid int) row format serde &#39;org.apache.hadoop.hive.serde2.RegexSerDe&#39; with serdeproperties ( &quot;input.regex&quot; &#x3D; &quot;(.*) \\-\\- \\\\[(.*) (.*)\\\\] \\&quot;([A-Z]*) (.*) (.*)\\&quot; ([0-9]*) \\-&quot; ) stored as textfile; 加载数据load data local inpath ‘/home/hivedata/server.log’ into table logs; beeline 在实际开发过程中，应该是远程连接公司的Hive服务器，那么此时就需要去进行远程访问 - beeline步骤 1.在公司服务器的开启Hive的后台服务进程 hive --service hiveserver2 &```123如果开启成功应该出现RunJar进程 1.远程连接公司的Hive服务器 ```sh beeline -u jdbc:hive2://10.9.162.133:10000/hivedemo -n root 视图 一个表中可以包含很多字段，但是每一个字段的使用频率并不是相同的，那么可以考虑将其中比较常用的字段抽取出来形成子表，这个子表可以用视图来表示 如果将抽取出来的视图存到磁盘上，那么此时称之为是物化视图；如果将抽取出来的视图存到内存中，那么此时称之为是虚拟视图 数据库中一般支持两种视图，但是Hive中只支持虚拟视图 建立视图：create view o_view as select orderid, num from orders;这句话表示将orders表中的orderid和num两个字段进行抽取，封装到o_view视图中 建立视图的时候封装的select语句并没有执行，而是需要在第一次使用视图的时候才会触发这句select 索引 数据库中，因为存在主键，所以会自动针对主键建立索引，数据库中采用的索引是B+tree机制。在Hive中，因为不存在主键，所以也不会自动建立索引 在Hive中可以手动建立索引，并且可以针对任意字段来建立索引 案例建立索引表create index o_index on table orders(orderid) as ‘org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler’ with deferred rebuild in table order_index;建立索引，索引建立完成之后不会自动更新 - 即orders表中产生变化的时候索引表不会变alter index o_index on orders rebuild;查询索引数据select * from order_index;删除索引drop index o_index on orders; 如果一个表比较常用，那么可以针对这个表中的常用的字段来建立索引而并不是针对所有字段/表建立索引 元数据 用于描述数据的数据就是元数据 Hive中的元数据包括database名、table名、字段名、分区、分桶信息等都属于元数据Hive中的元数据默认是存储在关系型数据中的，目前支持的元数据的数据库是Derby和MySQL，如果不指定默认使用的是Derby。因为Derby是单连接数据库，所以需要将Hive的元数据库更换为MySQL 调优map side join： 如果一个小表和一个大表进行join查询，那么可以考虑将小表放在缓存中然后处理大表，如果需要用到小表的数据可以从内存中查询 需要执行set hive.auto.convert.join=true;才会开启小表的缓存机制 默认要求小表要放在join的左边，即小表 join 大表 默认只要小表不超过25M，那么就会放入内存中，这个大小可以通过hive.mapjoin.smalltable.filesize来设置如果在join的时候附带了查询条件，那么考虑先用子查询执行where来降低数据量，然后再进行join group by在有多个ReduceTask的情况下可能会产生Reduce端的数据倾斜，可以通过二阶段聚合解决这个问题。在Hive中可以通过set hive.groupby.skewindata=true;来自动开启二阶段聚合 distinct和聚合函数的优化，聚合函数最终只能利用一个ReduceTask来进行计算，所以如果distinct和聚合函数同时使用，那么考虑先用子查询来进行去重，最后再进行聚合 调整切片数：如果数据结构相对比较复杂或者处理逻辑相对比较复杂，那么可以考虑将切片调小来增多MapTask的数量；如果数据结构相对比较简单或者处理逻辑相对比较简单，那么可以考虑将切片调大来减少MapTask的数量 - 任务复杂就增多线程，任务简单就减少线程 数据处理流程数据仓库和数据库 数据库 数据仓库 数据量 &lt;=GB &gt;=TB 来源 相对比较单一，往往来源于单一的web应用 来源相对丰富，可以是日志，爬虫，网页埋点等 种类 数据结构相对单一 - 结构化数据 数据结构相对丰富 - 结构化数据，半结构化数据，非结构化数据 操作 提供了完整的增删改查的能力 往往只提供增和查的操作而不是提供删和改的操作 事务 强调事务，存在ACID四个特性 不强调事务，往往是若事务甚至不支持事务 冗余 精简，尽量避免冗余 人为制造冗余 - 副本 数据场景 往往是用于线上实时捕获数据 离线历史数据和线上实时处理 系统 OLTP - Online Transaction Processor 联机事务处理 - 强调事务的完整性 OLAP - Online Analysis Processor - 联机分析处理 - 强调数据分析过程 面向对象 程序员，DBA，运维等技术人员 面向领导，市场，客户，销售等非技术人员，辅助决策","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"Flume","slug":"Flume","date":"2020-04-17T16:00:00.000Z","updated":"2020-04-22T01:55:18.847Z","comments":true,"path":"2020/04/18/Flume/","link":"","permalink":"http://yoursite.com/2020/04/18/Flume/","excerpt":"","text":"Flume概述 Flume是一种分布式，可靠且可用的服务，用于有效地收集，聚合和移动大量日志数据。它具有基于流数据流的简单灵活的体系结构。它具有可调整的可靠性机制以及许多故障转移和恢复机制，具有强大的功能和容错能力。它使用一个简单的可扩展数据模型，允许在线分析应用程序。 Flume最主要的作用就是，实时读取服务器本地磁盘的数据，将数据写到HDFS中 Flume的版本更新相对稳定： Flume1.0:Flume-ng Flume-ng和Flume-og不兼容。现在开发过程中一般使用的是Flume-ng Flume0.9:Flume-og。Flume-og是单线程执行任务 数据流模型单级流动 多剂流 合并 复流 基本概念 Event： 传输单元，Flume传输数据的基本单元，以Event的形式将数据从源头送至目的地。Event由Header和Body两部分组成，Header用来存放该evevt的一些属性，为K-V结构，Body用来存放该条数据，形式为字节数组。 一个Event就是一条日志，即在Flume中会将收集到的每一条日志封装成一个Event Event本质上是一个json串，即Flume将收集到的日志封装成json，这个Event固定包含header和body两个部分 Agent：是Flume中构成流动模型的基本单位 Source： 从数据源采集数据 Channel ：临时存储数据 Sink： 将数据写到目的地 Source AVRO Source：接收被AVRO序列化之后的数据，可以结合AVRO Sink可以实现多级、扇入、扇出流动 Exec Source：将命令的执行结果作为日志进行收集 NetCat Source：监听指定的端口接收TCP请求，将TCP请求的数据作为日志进行收集 Sequence Generator Source：不断产生递增的数字 Spooling Directory Source：监听指定的目录，如果监听的目录中产生了新的文件，那么会自动收集新文件中的内容 HTTP Source：监听HTTP请求，只能监听GET和POST请求，但是实际使用过程中，对GET请求的监听不稳定，所以一般只用这个Source来监听POST请求 扩展：自定义Source：如果实际生产中遇到的场景，Flume提供的Source无法解决，就需要自定义Source。如果需要自定义Source，需要写一个类实现EventDrivenSource/PollableSource EventDrivenSource：事件驱动Source，被动型Source。需要自己定义线程获取数据发送数据 PollableSource：拉取Source，主动型Source。已经提供了预定义的线程来获取和发送数据 Channel Memory Channel:内存通道。将Source收集到的数据临时存储在内存中。Memory Channel的读写速度相对较快，但并不可靠，适合于高并发不要求可靠性的场景 File Channel ： 文件通道, 将Source收集到的数据临时存储在磁盘上。File Channel的读写速度相对比较慢，但是可靠，适用于要求可靠性但是传输速度相对较低的场景 Sink Logger Sink：将收集到的日志以指定的日志级别来进行记录或者打印。在记录的时候为了防止内容过多将屏幕占满，所以一旦数据超过16个字节那么就会自动省略后边的内容 File_roll Sink：将收集到的数据存放到指定的目录中。如果不指定，那么会在指定的目录下，默认每隔30s生成一个小文件，致使生成大量的小文件，所以实际开发中会将这个属性来进行设置 HDFS Sink：将收集到的数据写到HDFS上。如果不指定，会在HDFS上每隔30s生成一个小文件 AVRO Sink：将数据用AVRO机制进行序列化之后传输到下一个节点上。AVRO Sink结合AVRO Source实现多级、扇入、扇出流动效果 扩展：自定义Sink。如果实际生产过程中，需要按照指定格式将数据输出，这个时候就需要使用自定义Sink。如果需要自定义Sink，那么需要写一个类实现Sink、Configurable接口 组件 Selector - 选择器 Selector是Source的子组件 Selector存在2种模式：replicating和multiplexing replicating：复制模式。一个节点在收到数据之后，会将数据进行复制然后分发给每一个节点，从而导致每一个节点收到的数据都是相同的 multiplexing：路由模式/多路复用模式。监听headers中指定的字段的值，根据值来确定发往哪个节点，所以每一个节点收到的数据是不同的 如果不配置，默认是复制模式 Interceptor - 拦截器 Interceptor是Source的子组件 Interceptor可以有多个，构成拦截器链 常用的Interceptor： Timestamp：在数据的headers中添加一个时间戳字段，记录数据收集的时间。结合HDFS Sink实现日志的按天收集和存放 Host：在数据的headers中添加一个host字段，记录数据来源的主机 Static：在数据的headers中添加一个指定的字段 UUID：在数据的headers中添加一个id字段用于标记唯一性 Search and Replace：利用正则来搜索数据中符合格式的数据，然后将数据替换为指定的符号 Regex Filtering：利用正则来对数据进行过滤。只要数据符合正则的指定格式就会被拦截 Sink Group一、概述 只接受一个 Sink 这是默认的策略。即如果不配置Processor，用的是这个策略 模式：复制模式，12a1.sinkgroups &#x3D; g1a1.sinkgroups.g1.sinks &#x3D; k1 k2 a1.sinkgroups.g1.processor.type &#x3D; failover a1.sinkgroups.g1.processor.priority.k1 &#x3D; 5 a1.sinkgroups.g1.processor.priority.k2 &#x3D; 10 a1.sinkgroups.g1.processor.maxpenalty &#x3D; 10000 Failover Sink Processor一、概述 维护一个sink们的优先表。确保只要一个是可用的就事件就可以被处理 失败处理原理是，为失效的sink指定一个冷却时间，在冷却时间到达后再重新使用 sink们可以被配置一个优先级，数字越大优先级越高 如果sink发送事件失败，则下一个最高优先级的sink将会尝试接着发送事件 如果没有指定优先级，则优先级顺序取决于sink们的配置顺序，先配置的默认优先级高于后配置的 在配置的过程中，设置一个group processor ，并且为每个sink都指定一个优先级 优先级必须是唯一的 另外可以设置maxpenalty属性指定限定失败时间","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"Mybatis","slug":"Mybatis","date":"2020-04-16T16:00:00.000Z","updated":"2020-04-29T01:08:40.509Z","comments":true,"path":"2020/04/17/Mybatis/","link":"","permalink":"http://yoursite.com/2020/04/17/Mybatis/","excerpt":"","text":"什么是 Mybatis? Mybatis是一个半ORM（对象关系映射）的框架，它内部封装了JDBC，开发时只需要关注sql语句本身，不需要花费精力去处理加载驱动、创建连接，创建statement等复杂过程。程序员直接编写原生态的sql，严格控制sql的执行性能，灵活度高。 Mybatis可以通过xml和注解的方式来配置和映射原生信息，将普通JavaBeans映射成数据库中的记录，避免了几乎所有的JDBC代码和手动设置参数以及获取结果集。 通过xml文件或者注解的方式执行各种statement配置起来，并通过java对象和statement中的sql的动态参数进行映射生成最终执行的sql语句，最后由mybatis框架执行sql并将结果映射为java对象并返回。（从执行sql到返回result的过程） Mybaits 的优点: 基于sql语句编程相当的灵活，不会对应用程序或者数据库的现有设计造成任何的影响，SQL写在xml中，解决了sql与程序代码的耦合，便于统一管理；提供xml标签，支持编写动态的sql语句，并可重用。 与JDBC相比，减少了50%以上的代码量，消除了JDBC大量冗余的代码，不需要手动的建立连接和关闭连接 很好的与各种数据库兼容（因为MyBatis使用JDBC连接数据库，所以只要JDBC支持的数据库mybatis都支持 能够与Spring很好的集成； 提供映射标签，支持对象与数据库的ORM字段关系映射，提供对象关系映射标签支持对象关系组件映射 Mybatis框架的缺点 sql语句的编写工作量较大，一些关联查询，多表查询，需要对sql语句进行优化，对开发人员的sql功底有一定的要求 sql语句依赖于数据库，导致数据库的移植性差，不能随意的更换数据库 MyBatis 框架适用场合: Mybatis只关注sql的本身，是一个足够灵活的DAO层解决方案 对性能要求很高，或者需求变化较多的项目，如互联网项目，mybatis将是不错的选择 MyBatis 与 Hibernate 有哪些不同? mybatis时半对象关系映射的框架，它需要开发人员自己编写sql语句 Mybatis直接编写原生态sql，严格控制sql的执行性能，灵活度高，非常适合对关系数据模型要求不高的软件开发，因为这类软件需求变化频繁，一但需求变化要求迅速输出成果。但灵活的前提是mybatis无法做到数据库无关性，如果需要实现支持多种数据库的软件，则需要自定义多套sql映射文件，工作量大。 {}和${}的区别是什么? #{}是预编译处理，${}是字符串替换 Mybatis在处理#{}时，会将#{}替换成？号，调用PrepareStatement的set方法来赋值； Mybatis在处理${},就是把${}替换成变量的值没有预编译的过程 使用#{}可以有效的防止sql注入，提高系统的安全性 当实体类中的属性名和表中的字段名不一样 ，怎么办 ?第1种：通过在查询的sql语句中定义字段名的别名，让字段名的别名和实体类的属性名一致。 12345678&lt;select id=\"getOrder\" parameterType=\"int\" resultMap=\"orderresultmap\"&gt;select * from orders where order_id=#&#123;id&#125; &lt;/select&gt;&lt;resultMap type=\"me.gacl.domain.order\" id=\"orderresultmap\"&gt;&lt;!–用 id 属性来映射主键字段–&gt;&lt;id property=\"id\" column=\"order_id\"&gt;&lt;!–用 result 属性来映射非主键字段，property 为实体类属性名，column 为数据表中的属性–&gt;&lt;result property = “orderno\" column =”order_no\"/&gt;&lt;result property=\"price\" column=\"order_price\" /&gt; &lt;/reslutMap&gt; 第二种通过&lt;resultMap&gt;来映射字段名和实体类属性名一一对应的关系 123&lt;select id=”selectorder” parametertype=”int” resultetype=” me.gacl.domain.order”&gt;select order_id id, order_no orderno ,order_price price form orders where order_id=#&#123;id&#125;;&lt;/select&gt; 模糊查询 like 语句该怎么写? 第 1 种:在 Java 代码中添加 sql 通配符。1234string wildcardname = “%smi%”;list&lt;name&gt; names = mapper.selectlike(wildcardname);&lt;select id=”selectlike”&gt;select * from foo where bar like #&#123;value&#125; 第二种：在sql语句中拼接通配符，会引起sql注入12345string wildcardname = “smi”;list&lt;name&gt; names = mapper.selectlike(wildcardname);&lt;select id=”selectlike”&gt;select * from foo where bar like \"%\"#&#123;value&#125;\"%\" &lt;/select&gt; 通常一个 Xml 映射文件，都会写一个 Dao 接口与之对应， 请问，这个 Dao 接口的工作原理是什么?Dao 接口里的方法， 参数不同时，方法能重载吗? Dao接口即Mapper接口，接口的全限名，就是映射文件中namespace的值；接口的方法名，就是映射文件中Mapper的StateMent的id值，接口方法内的参数，就是传递给 sql 的参数。 Mapper 接口是没有实现类的，当调用接口方法时，接口全限名+方法名拼接字符 串作为 key 值，可唯一定位一个 MapperStatement。在 Mybatis 中，每一个 &lt;select&gt;、&lt;insert&gt;、&lt;update&gt;、&lt;delete&gt;标签，都会被解析为一个 MapperStatement 对象。 Mapper 接口里的方法，是不能重载的，因为是使用 全限名+方法名 的保存和寻 找策略。Mapper 接口的工作原理是 JDK 动态代理，Mybatis 运行时会使用 JDK 动态代理为 Mapper 接口生成代理对象 proxy，代理对象会拦截接口方法，转而 执行 MapperStatement 所代表的 sql，然后将 sql 执行结果返回。 Mybatis 是如何进行分页的?分页插件的原理是什么? Mybatis 使用 RowBounds 对象进行分页，它是针对 ResultSet 结果集执行的内 存分页，而非物理分页。可以在 sql 内直接书写带有物理分页的参数来完成物理分 页功能，也可以使用分页插件来完成物理分页。 分页插件的基本原理是使用 Mybatis 提供的插件接口，实现自定义插件，在插件 的拦截方法内拦截待执行的 sql，然后重写 sql，根据 dialect 方言，添加对应的物 理分页语句和物理分页参数。 Mybatis 是如何将 sql 执行结果封装为目标对象并返回的? 都有哪些映射形式? 使用&lt;resultMap&gt;标签，逐一定义数据库列名和对象属性名之间的映 射关系。 使用 sql 列的别名功能，将列的别名书写为对象属性名。 有了列名与属性名的映射关系后，Mybatis 通过反射创建对象，同时使用反射给 对象的属性逐一赋值并返回，那些找不到映射关系的属性，是无法完成赋值的。 如何执行批量插入?首先,创建一个简单的 insert 语句: 12&lt;insert id=”insertname”&gt;insert into names (name) values (#&#123;value&#125;) &lt;/insert&gt; 然后在 java 代码中像下面这样执行批处理插入: 123456789101112list &lt; string &gt; names = new arraylist(); names.add(“fred”); names.add(“barney”); names.add(“betty”); names.add(“wilma”);// 注意这里 executortype.batchsqlsession sqlsession = sqlsessionfactory.opensession(executortype.batch); try &#123;namemapper mapper = sqlsession.getmapper(namemapper.class); for (string name: names) &#123;mapper.insertname(name); &#125;sqlsession.commit(); &#125; catch (Exception e) &#123;e.printStackTrace();sqlSession.rollback();throw e; &#125;finally &#123; sqlsession.close();&#125; 如何获取自动生成的(主)键值? insert 方法总是返回一个 int 值 ，这个值代表的是插入的行数。如果采用自增长策略，自动生成的键值在 insert 方法执行完后可以被设置到传入 的参数对象中。示例：12345678&lt;insert id=”insertname” usegeneratedkeys=”true” keyproperty=” id”&gt;insert into names (name) values (#&#123;name&#125;)&lt;/insert&gt;name name = new name(); name.setname(“fred”);int rows = mapper.insertname(name);// 完成后,id 已经被设置到对象中system.out.println(“rows inserted = ” + rows); system.out.println(“generated key value = ” + name.getid()); 在 mapper 中如何传递多个参数? 1.第一种： DAO层的函数 12345public UserselectUser(String name,String area);//对应的 xml,#&#123;0&#125;代表接收的是 dao 层中的第一个参数，#&#123;1&#125;代表 dao 层中第二 参数，更多参数一致往后加即可。&lt;select id=\"selectUser\"resultMap=\"BaseResultMap\"&gt; select * fromuser_user_t whereuser_name = #&#123;0&#125;anduser_area=#&#123;1&#125;&lt;/select&gt; 第二种：使用 @param 注解:123public interface usermapper &#123;user selectuser(@param(“username”) stringusername,@param(“hashedpassword”) string hashedpassword); &#125; 12345&lt;select id=”selectuser” resulttype=”user”&gt; select id, username, hashedpasswordfrom some_tablewhere username = #&#123;username&#125;and hashedpassword = #&#123;hashedpassword&#125;&lt;/select&gt; 第三种:多个参数封装成 map123456789101112131415try &#123;//映射文件的命名空间.SQL 片段的 ID，就可以调用对应的映射文件中的SQL//由于我们的参数超过了两个，而方法中只有一个 Object 参数收集，因此我们使用 Map 集合来装载我们的参数Map &lt; String, Object &gt; map = new HashMap(); map.put(\"start\", start);map.put(\"end\", end);return sqlSession.selectList(\"StudentID.pagination\", map);&#125; catch (Exception e) &#123; e.printStackTrace(); sqlSession.rollback();throw e;&#125; finally &#123; MybatisUtil.closeSqlSession();&#125; Mybatis 动态 sql 有什么用?执行原理?有哪些动态 sql? Mybatis 动态 sql 可以在 Xml 映射文件内，以标签的形式编写动态 sql，执行原理 是根据表达式的值 完成逻辑判断并动态拼接 sql 的功能。 Mybatis 提供了 9 种动态 sql 标签:trim | where | set | foreach | if | choose | when | otherwise | bind。 Xml 映射文件中，除了常见的 select|insert|updae|delete 标签之外，还有哪些标签?答:&lt;resultMap&gt;、&lt;parameterMap&gt;、&lt;sql&gt;、&lt;include&gt;、 &lt;selectKey&gt;，加上动态 sql 的 9 个标签，其中&lt;sql&gt;为 sql 片段标签，通过 &lt;include&gt;标签引入 sql 片段，&lt;selectKey&gt;为不支持自增的主键生成策略标 签。 Mybatis 的 Xml 映射文件中，不同的 Xml 映射文件，id 是否可以重复? 不同的 Xml 映射文件，如果配置了 namespace，那么 id 可以重复;如果没有配 置 namespace，那么 id 不能重复; 原因就是 namespace+id 是作为 Map&lt;String, MapperStatement&gt;的 key 使用的，如果没有 namespace，就剩下 id，那么，id 重复会导致数据互相覆盖。 有了 namespace，自然 id 就可以重复，namespace 不同，namespace+id 自然 也就不同。 为什么说 Mybatis 是半自动 ORM 映射工具?它与全自动 的区别在哪里?Hibernate 属于全自动 ORM 映射工具，使用 Hibernate 查询关联对象或者关联 集合对象时，可以根据对象关系模型直接获取，所以它是全自动的。而 Mybatis 在查询关联对象或关联集合对象时，需要手动编写 sql 来完成，所以，称之为半自 动 ORM 映射工具。 一对一、一对多的关联查询 ?12345678910111213141516171819202122232425262728&lt;mapper namespace=\"com.lcb.mapping.userMapper\"&gt; &lt;!--association 一对一关联查询 --&gt; &lt;select id=\"getClass\" parameterType=\"int\" resultMap=\"ClassesResultMap\"&gt;select * from class c,teacher t where c.teacher_id=t.t_id and c.c_id=#&#123;id&#125;&lt;/select&gt;&lt;resultMap type=\"com.lcb.user.Classes\" id=\"ClassesResultMap\"&gt; &lt;!-- 实体类的字段名和数据表的字段名映射 --&gt;&lt;id property=\"id\" column=\"c_id\"/&gt;&lt;result property=\"name\" column=\"c_name\"/&gt;&lt;association property=\"teacher\" javaType=\"com.lcb.user.Teacher\"&gt;&lt;id property=\"id\" column=\"t_id\"/&gt;&lt;result property=\"name\" column=\"t_name\"/&gt; &lt;/association&gt;&lt;/resultMap&gt;&lt;!--collection 一对多关联查询 --&gt;&lt;select id=\"getClass2\" parameterType=\"int\" resultMap=\"ClassesResultMap2\"&gt;select * from class c,teacher t,student s where c.teacher_id=t.t_id and c.c_id=s.class_id and c.c_id=#&#123;id&#125;&lt;/select&gt;&lt;resultMap type=\"com.lcb.user.Classes\" id=\"ClassesResultMap2\"&gt; &lt;id property=\"id\" column=\"c_id\"/&gt;&lt;result property=\"name\" column=\"c_name\"/&gt;&lt;association property=\"teacher\"javaType=\"com.lcb.user.Teacher\"&gt;&lt;id property=\"id\" column=\"t_id\"/&gt;&lt;result property=\"name\" column=\"t_name\"/&gt; &lt;/association&gt;&lt;collection property=\"student\" ofType=\"com.lcb.user.Student\"&gt;&lt;id property=\"id\" column=\"s_id\"/&gt;&lt;result property=\"name\" column=\"s_name\"/&gt; &lt;/collection&gt;&lt;/resultMap&gt; &lt;/mapper&gt; MyBatis 实现一对一有几种方式?具体怎么操作的? 有联合查询和嵌套查询,联合查询是几个表联合查询,只查询一次, 通过在 resultMap 里面配置 association 节点配置一对一的类就可以完成; 嵌套查询是先查一个表，根据这个表里面的结果的 外键 id，去再另外一个表里面 查询数据,也是通过 association 配置，但另外一个表的查询通过 select 属性配置。 MyBatis 实现一对多有几种方式,怎么操作的?有联合查询和嵌套查询。联合查询是几个表联合查询,只查询一次,通过在 resultMap 里面的 collection 节点配置一对多的类就可以完成;嵌套查询是先查 一个表,根据这个表里面的 结果的外键 id,去再另外一个表里面查询数据,也是通过 配置 collection,但另外一个表的查询通过 select 节点配置。 Mybatis 是否支持延迟加载?如果支持，它的实现原理是 什么?答:Mybatis 仅支持 association 关联对象和 collection 关联集合对象的延迟加 载，association 指的就是一对一，collection 指的就是一对多查询。在 Mybatis 配置文件中，可以配置是否启用延迟加载 lazyLoadingEnabled=true|false。 它的原理是，使用 CGLIB 创建目标对象的代理对象，当调用目标方法时，进入拦 截器方法，比如调用 a.getB().getName()，拦截器 invoke()方法发现 a.getB()是 null 值，那么就会单独发送事先保存好的查询关联 B 对象的 sql，把 B 查询上来， 然后调用 a.setB(b)，于是 a 的对象 b 属性就有值了，接着完成 a.getB().getName() 方法的调用。这就是延迟加载的基本原理。 当然了，不光是 Mybatis，几乎所有的包括 Hibernate，支持延迟加载的原理都 是一样的。 Mybatis 的一级、二级缓存: 一级缓存: 基于 PerpetualCache 的 HashMap 本地缓存，其存储作用域为 Session，当 Session flush 或 close 之后，该 Session 中的所有 Cache 就 将清空，默认打开一级缓存。 二级缓存与一级缓存其机制相同，默认也是采用 PerpetualCache，HashMap 存储，不同在于其存储作用域为 Mapper(Namespace)，并且可自定义存储源， 如 Ehcache。默认不打开二级缓存，要开启二级缓存，使用二级缓存属性类需要 实现 Serializable 序列化接口(可用来保存对象的状态),可在它的映射文件中配置 ; 对于缓存数据更新机制，当某一个作用域(一级缓存 Session/二级缓存 Namespaces)的进行了 C/U/D 操作后，默认该作用域下所有 select 中的缓存将 被 clear。 ##24、什么是 MyBatis 的接口绑定?有哪些实现方式?接口绑定，就是在 MyBatis 中任意定义接口,然后把接口里面的方法和 SQL 语句绑 定, 我们直接调用接口方法就可以,这样比起原来了SqlSession提供的方法我们可 以有更加灵活的选择和设置。 接口绑定有两种实现方式,一种是通过注解绑定，就是在接口的方法上面加上 @Select、@Update 等注解，里面包含 Sql 语句来绑定;另外一种就是通过 xml 里面写 SQL 来绑定, 在这种情况下,要指定 xml 映射文件里面的 namespace 必须 为接口的全路径名。当 Sql 语句比较简单时候,用注解绑定, 当 SQL 语句比较复杂 时候,用 xml 绑定,一般用 xml 绑定的比较多。 使用 MyBatis 的 mapper 接口调用时有哪些要求?1、Mapper 接口方法名和 mapper.xml 中定义的每个 sql 的 id 相同;2、Mapper 接口方法的输入参数类型和 mapper.xml 中定义的每个 sql 的 parameterType 的类型相同;3、Mapper 接口方法的输出参数类型和 mapper.xml 中定义的每个 sql 的 resultType 的类型相同;4、Mapper.xml 文件中的 namespace 即是 mapper 接口的类路径。 Mapper 编写有哪几种方式?第一种:接口实现类继承 SqlSessionDaoSupport:使用此种方法需要编写 mapper 接口，mapper 接口实现类、mapper.xml 文件。 在 sqlMapConfig.xml 中配置 mapper.xml 的位置1234&lt;mappers&gt;&lt;mapper resource=\"mapper.xml 文件的地址\" /&gt; &lt;mapper resource=\"mapper.xml 文件的地址\" /&gt;&lt;/mappers&gt; 定义 mapper 接口 实现类集成 SqlSessionDaoSupportmapper 方法中可以 this.getSqlSession()进行数据增删改查。 spring 配置1234&lt;bean id=\" \" class=\"mapper 接口的实现\"&gt; &lt;property name=\"sqlSessionFactory\"ref=\"sqlSessionFactory\"&gt;&lt;/property&gt; &lt;/bean&gt; 第二种:使用 org.mybatis.spring.mapper.MapperFactoryBean: 1、在 sqlMapConfig.xml 中配置 mapper.xml 的位置，如果 mapper.xml 和 mappre 接口的名称相同且在同一个目录，这里可以不用配置 1234&lt;mappers&gt;&lt;mapper resource=\"mapper.xml 文件的地址\" /&gt;&lt;mapper resource=\"mapper.xml 文件的地址\" /&gt;&lt;/mappers&gt; 2、定义 mapper 接口:1、mapper.xml 中的 namespace 为 mapper 接口的地址2、mapper 接口中的方法名和 mapper.xml 中的定义的 statement 的 id 保持一 致3、Spring 中定义 1234&lt;bean id=\"\" class=\"org.mybatis.spring.mapper.MapperFactoryBean\"&gt;&lt;property name=\"mapperInterface\" value=\"mapper 接口地址\" /&gt;&lt;property name=\"sqlSessionFactory\" ref=\"sqlSessionFactory\" /&gt;&lt;/bean&gt; 2、定义 mapper 接口:注意 mapper.xml 的文件名和 mapper 的接口名称保持一致，且放在同一个目录 3、配置 mapper 扫描器: 123456789&lt;bean id=\"\" class=\"org.mybatis.spring.mapper.MapperFactoryBean\"&gt;&lt;property name=\"mapperInterface\" value=\"mapper 接口地址\" /&gt; &lt;property name=\"sqlSessionFactory\" ref=\"sqlSessionFactory\" /&gt;&lt;/bean&gt;&lt;bean class=\"org.mybatis.spring.mapper.MapperScannerConfigurer\"&gt;&lt;property name=\"basePackage\" value=\"mapper 接口包地址\"&gt;&lt;/property&gt;&lt;property name=\"sqlSessionFactoryBeanName\"value=\"sqlSessionFactory\"/&gt; &lt;/bean&gt; 4、使用扫描器后从 spring 容器中获取 mapper 的实现对象。 简述 Mybatis 的插件运行原理，以及如何编写一个插件。答:Mybatis 仅可以编写针对 ParameterHandler、ResultSetHandler、 StatementHandler、Executor 这 4 种接口的插件，Mybatis 使用 JDK 的动态代 理，为需要拦截的接口生成代理对象以实现接口方法拦截功能，每当执行这 4 种 接口对象的方法时，就会进入拦截方法，具体就是 InvocationHandler 的 invoke() 方法，当然，只会拦截那些你指定需要拦截的方法。 编写插件:实现 Mybatis 的 Interceptor 接口并复写 intercept()方法，然后在给 插件编写注解，指定要拦截哪一个接口的哪些方法即可，记住，别忘了在配置文 件中配置你编写的插件。","categories":[{"name":"面试","slug":"面试","permalink":"http://yoursite.com/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"MapReduce","slug":"MapReduce","date":"2020-04-11T06:23:09.000Z","updated":"2020-04-17T13:16:06.589Z","comments":true,"path":"2020/04/11/MapReduce/","link":"","permalink":"http://yoursite.com/2020/04/11/MapReduce/","excerpt":"","text":"MapReduce定义 MapReduce是一个分布式运算程序放入编程框架，是用户开发“基于Hadoop的数据分析应用”的核心框架。 MapReduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个hadoop集群上。 MapReduce的优缺点1. MapReduce易于编程它简单的实现一些接口，就可以完成一个分布式程序，这个分布式程序可以分布到大量廉价的PC机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特点是的MapReduce编程变得非常流行。2.良好的扩展性当你的计算资源不能得到满足的时候，你可以通过简单的增加机器来扩展它的计算能力。3.高容错性MapReduce设计的初衷就是使程序能够部署在廉价的PC机器上，这就要求它具有很高的容错性。比如其中一台机器宕机了，他可以把上面的计算任务转移到另一个节点上运行，不至于这个任务执行失败，而且这个过程不需要人工参与，而完全是由Hadoop内部完成4.适合PB级以上海量数据的离线处理可以实现上千台服务器集群并发工作，提高数据处理能力。缺点： 不擅长实时计算—— MapReduce无法像Mysql一样，在毫秒或者秒级内返回结果。 不擅长流式计算—— 流式计算的输入数据是动态的，而MapReduce的输入数据集是静态的，不能动态变化。这是因为MapReduce自身的设计特点决定了数据源必须是静态的 不擅长DAG（有向图计算） 基本理论一、数据本地化的策略 MapReduce的物理结构：JobTracker（Master）和TaskTracker（slave） JobTracker在接受到任务之后会向NameNode发送请求获取文件信息，而JobTracker在收到文件信息之后会对文件进行切片（Split）。切片不是真正将数据切开，而是划分任务量。每一个切片就需要对应一个MapTask 实际开发中，Split=Block/n，默认split和block大小一样 数据本地化： 为了减少跨集群的数据传输，往往会将DataNode和TaskTracker部署在相同的节点上，即一个节点既是DataNode又是TaskTracker JobTracker在分配任务的时候，会考虑哪个节点有要处理的数据就将任务分给谁，这样分配的目的是为了能够在处理数据的时候从节点上读取而不需要跨节点读取 如果文件为空，那么整个文件作为一个切片来进行处理 在MapReduce中，文件分为可切与不可切。如果不指定，则文件默认可切。如果文件不可切，则整个文件作为一个切片来进行处理 如果需要调小splitSize，那么需要减小maxSize -FileInputFormat.setMaxInputSplitSize();如果需要增大splitSize，那么需要增大minSize - FileInputFormat.setMinInputSplitSize() 在切片的时候要考虑阈值SPLIT_SLOP(1.1) 二、Job/MapReduce执行流程提交流程 检查job指定的输入输出路径 计算切片 如果需要，为这个job设置分布式缓存的存根账户信息 将这个job的jar包和配置信息提交到HFDFS上 将job提交到jobtracker，并且选择监控这个job的执行状态 执行流程 拆分任务：JobTracker在收到Job任务之后，会将Job任务拆分成MapTask和ReduceTask。MapTask的数量由切片数量来决定，ReduceTask的数量由分区数量来决定 分配任务：JobTracker在拆分完成之后会等待TaskTracker的心跳。当JobTracker收到TaskTracker会拆分出来的子任务分配给taskTracker，注意每一个TaskTracker不一定只分配分配一个任务，可以能会被分配到多个任务。在分配的时候，MapTask尽量考虑数据本地化策略（在分配的时候，哪一个TaskTracker上有要处理的数据就将MapTask分配给哪一个TaskTracker，ReduceTask则会考虑分配给相对空闲的TaskTracker 下载jar包：TaskTracker领取到任务（MapTask或者ReduceTask）之后，会连接对应的节点来下载jar包。这一步体现的是逻辑移动数据固定的思想 执行任务：TaskTracker在下载完jar包之后，会在本节点内部开启一个JVM子进程来执行MapTask或者是ReduceTask。默认情况下，一个JVM子进程只执行一个任务。所以如果一个TaskTracker被分配到了多个任务，那么这个TaskTracker就会多次开启并且关闭JVM子进程 三、uber模式 uber默认是关闭的，即JVM子进程默认只能执行一个任务；当开启uber模式的时候就可以一个jvm子进程的复用，即开启JVM子进程之后，可以执行多个任务之后再关闭。 Shuffle一、Map端的Shuffle MapTask默认对切片进行按行处理,每处理一行，先将数据写到缓冲区（MemoryBuffer）中 每一个MapTask默认自带一个缓冲区，这个缓冲区在内存中，默认占用内存100m的大小 当缓冲区的时候达到指定阈值（默认是0.8，当缓冲区的使用达到80%时就会产生溢写操作），会将缓冲区的数据溢写（spill）到磁盘上，产生一个溢写文件（spillable file） 溢写后的数据会再次向缓冲区写，如果达到阈值，会再次溢写，每一次溢写，会产生一个新的溢写文件 当MapTask将所有的数据处理完成之后会将所有的溢写文件进行合并（merge），合并成一个结果文件（final out），如果maptask处理完之后缓冲区还有数据，会将缓冲区的数据直接合并到fileout中 不一定会产生溢写过程，输入的数据量并不能决定溢写次数，而是由MapTask处理逻辑来决定的；溢写文件的大小并不是完全由缓冲区大小和溢写阈值决定，还需要考虑序列化的影响 如果没产生溢写，结果都在缓冲区中，那么缓冲区的数据直接冲刷产生一个final out 三、Shuffle的调优 扩大缓冲区，实际开发中会将缓冲区设置为250M～400M 尽量增加Combiner 可以将MapTask的结果进行压缩， InputFormat –输入格式 输入格式类是MapReduce中顶级的输入格式的父类 作用： 对文件切片 针对切片提供输入流用于读取切片 InputFormat将HDFS上的数据切片，然后读取切片的内容，将读取的内容传递给Mapper，所以InputFormat读出来的数据是什么格式，mapper接受的就是什么格式 如果没有指定，默认使用的是TextInputformat，所以mapper的接受类型也是&lt;longwritable,Text&gt;，-&gt;TextInputFormat底层实际上依靠了LineRecordReader来读取数据 - 在TextInputFormat中，从第二个切片开始，每一个MapTask处理的数据都是从当前切片的第二行开始到下一个切片的第一行 自定义输入格式：如果MapReduce中提供的输入格式不适合指定场景，那么就需要自己定义输入格式-定义类继承InputFormat，因为实际过程中切片比较复杂所以一般继承FileinputFormat这个类中已经覆盖了切片方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081public class AuthInputFormat extends FileInputFormat&lt;Text, Text&gt; &#123; @Override public RecordReader&lt;Text, Text&gt; createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException &#123; return new AuthReader(); &#125;&#125;class AuthReader extends RecordReader&lt;Text, Text&gt; &#123; private LineReader reader; private static byte[] blank = new Text(\" \").getBytes(); private Text key; private Text value; private Text tmp; //初始化的方法 //AuthReader在创建的时候会自动调用初始化方法 @Override public void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException &#123; //转化为文件切片 FileSplit fsplit = (FileSplit) split; //拿到切片的路径 Path path = fsplit.getPath(); //链接HDFS FileSystem fs = FileSystem.get(URI.create(path.toString()), context.getConfiguration()); //获取输入流 InputStream in = fs.open(path); //输入流是一个字节流，原文件是字符流，三行组成一行数据 //如果使用字节流需要判断什么时候读完三行数据 //将字节流进行包装，最好还能按行读取 reader = new LineReader(in); &#125; //在执行过程中判断是否有下一个间值对需要处理 //如果有，那么就会传递给map中 //如果没有，执行结束 //试着去读取数据，如果读到，那么代表有数据要处理返回true //如果没有读到，那代表没有数据需要处理返回false @Override public boolean nextKeyValue() throws IOException, InterruptedException &#123; key = new Text(); value = new Text(); tmp = new Text(); //会将读取到的一行数据放入传入的参数tmp if (reader.readLine(tmp) == 0) return false; key.set(tmp.toString()); if (reader.readLine(tmp) == 0) return false; value.set(tmp.toString()); value.append(blank, 0, blank.length); if (reader.readLine(tmp) == 0) return false; byte[] bytes = tmp.getBytes(); value.append(bytes, 0, bytes.length); return true; &#125; @Override public Text getCurrentKey() throws IOException, InterruptedException &#123; return key; &#125; @Override public Text getCurrentValue() throws IOException, InterruptedException &#123; return value; &#125;//获取执行进度的方法 @Override public float getProgress() throws IOException, InterruptedException &#123; return 0; &#125; @Override public void close() throws IOException &#123; if (reader != null) &#123; reader.close(); &#125; &#125;&#125; 多源输入：在MapReduce中可以同时指定多个路径来同时处理，这多个路径中的文件格式可以不一样，每一个路径单独制定输入格式以及Mapper类，但是最终使用的一个Reducer 12345678910111213141516171819public class MultiDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; System.setProperty(\"hadoop_user_name\",\"root\"); Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(MultiDriver.class); job.setReducerClass(MultiReducer.class); //设置自定义输入格式 job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); //同时指定多个输入路径 -多元输入 MultipleInputs.addInputPath(job,new Path(\"hdfs://192.168.243.134:9000/txt/score.txt\"), TextInputFormat.class,MultiMapper1.class); MultipleInputs.addInputPath(job,new Path(\"hdfs://192.168.243.134:9000/txt/score3.txt\"), AuthInputFormat.class,MultiMapper2.class); FileOutputFormat.setOutputPath(job,new Path(\"hdfs://192.168.243.134:9000//result/Multi\")); job.waitForCompletion(true); &#125;&#125; 输出格式 OutputFormat作用 校验这个任务的输出路径，例如校验输出路径是否不存在 提供RecorderWriter输出流用于写出数据，这个数据是存储在文件系统中 如果不指定，默认值的是TextOutputFormat OutputFormat是MapReduce中输出格式的顶级父类 数据倾斜 每一个任务处理的数据量不均匀，那么就产生了数据倾斜 数据本身就具有倾斜特性 - 倾斜度越大效率越低 Map端倾斜的产生条件：多源输入，文件大小不均等，文件不可切，这三个条件缺一不可，在实际开发过程中Map端的数据倾斜无法处理。 实际开发过程中，绝大部分的数据倾斜都产生在Reduce端，而直接导致Reduce端数据倾斜的原因是因为数据的分类（分区），本质原因是因为数据本省不均衡，Reduce端的倾斜可以使用二阶段聚合来解决。 join：如果需要处理多个文件，且文件之间有关系，可以把小文件先存根，先处理大文件，然后从根中取出处理 小文件 危害 存储：每一个小文件对应一条元数据，如果HDFS上存储了大量的小文件，那么就会产生大量的元数据。元数据数量过多的时候，导致NameNode的内存被大量占用，还会导致NameNode的查询效率变低。 计算：每一个小文件对应一个切片，那么大量的小文件就会产生大量的切片对应了大量的MapTask，当切片过多导致MapTask（线程）的数量，线程数量过多可能会导致服务器的负载压力变大甚至崩溃 目前为止处理办法，合并和打包 推测执行机制 推测执行机制实际上是Hadoop针对慢任务优化方案，当MapReduce出现慢任务的时候，Hadoop自动将慢任务拷贝一份，谁先处理完就取那个节点的结果，剩下的就会被kill掉 出现慢任务的场景： 任务分配不均 正常情况下不会出现，Namenode默认会找空闲的节点处理 硬件环境不同 正常情况下也不会出现，服务器购买按批买，性能基本一致 数据倾斜 集中在这个场景，推测执行机制无效，所以一般是关闭这个机制 实际开发中，数据倾斜导致的慢任务的场景更多，此时推测执行机制是无效的，所以一般是会关闭推测执行机制。","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://yoursite.com/categories/Hadoop/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"Dubbo","slug":"Dubbo","date":"2020-04-08T06:23:09.000Z","updated":"2020-04-12T13:01:04.351Z","comments":true,"path":"2020/04/08/Dubbo/","link":"","permalink":"http://yoursite.com/2020/04/08/Dubbo/","excerpt":"","text":"一、分布式系统分布式系统是若干独立计算机的集合，这些计算机对于用户来说就像单个相关系统，分布式系统（distributed system）是建立在网络之上的软件系统。垂直应用架构1. 做不到界面+业务逻辑实现分离 2. 应用不可能完全独立，大量的应用之间需要交互分布式应用架构1. 可以做到点后端分离 2. 应用之间的相互调用二、RPC（远程过程调用）什么叫RPCRPC【Remote Procedure Call】是指远程过程调用，是一种进程间通信方式，他是一种技术的思想，而不是规范。它允许程序调用另一个地址空间（通常是共享网络的另一台机器上）的过程或函数，而不用程序员显式编码这个远程调用的细节。即程序员无论是调用本地的还是远程的函数，本质上编写的调用代码基本相同。调用过程 Dubbo中的序列化 Dubbo RPC是Dubbo体系最核心的一种高性能，高吞吐量的远程调用方式，可以称之为多路复用的TCP长连接调用 长连接：避免了每次调用新建TCP连接，提高了调用的响应速度 多路复用：单个TCP连接可以交替传输多个请求和响应的消息，降低了连接的等待闲置时间从而减少了同样并发数下的网络连接数，提高了系统吞吐量 Dubbo RPC 主要用于两个 Dubbo 系统之间的远程调用，特别适合高并发、小数据的互联网场景。而序列化对于远程调用的响应速度、吞吐量、网络带宽消耗等同样也起着至关重要的作用，是我们提升分布式系统性能的最关键因素之一。 Dubbo 中支持的序列化方式： dubbo 序列化：阿里尚未开发成熟的高效 java 序列化实现，阿里不建议在生产环境使用它 hessian2 序列化：hessian 是一种跨语言的高效二进制序列化方式。但这里实际不是原生的hessian2 序列化，而是阿里修改过的 hessian lite，它是 dubbo RPC 默认启用的序列化方式 json 序列化：目前有两种实现，一种是采用的阿里的 fastjson 库，另一种是采用 dubbo 中自己实现的简单 json 库，但其实现都不是特别成熟，而且 json 这种文本序列化性能一般不如上面两种二进制序列化。 java 序列化：主要是采用 JDK 自带的 Java 序列化实现，性能很不理想。 在通常情况下，这四种主要序列化方式的性能从上到下依次递减。对于 dubbo RPC 这种追求高性能的远程调用方式来说，实际上只有 1、2 两种高效序列化方式比较般配，而第 1 个 dubbo 序列化由于还不成熟，所以实际只剩下 2 可用，所以 dubbo RPC 默认采用 hessian2 序列化。 但 hessian 是一个比较老的序列化实现了，而且它是跨语言的，所以不是单独针对 Java 进行优化的。而 dubbo RPC 实际上完全是一种 Java to Java 的远程调用，其实没有必要采用跨语言的序列化方式（当然肯定也不排斥跨语言的序列化）。 最近几年，各种新的高效序列化方式层出不穷，不断刷新序列化性能的上限，最典型的包括： 专门针对 Java 语言的：Kryo，FST 等等 跨语言的：Protostuff，ProtoBuf，Thrift，Avro，MsgPack 等等这些序列化方式的性能多数都显著优于 hessian2（甚至包括尚未成熟的 dubbo 序列化） 有鉴于此，我们为 dubbo 引入 Kryo 和 FST 这两种高效 Java 序列化实现，来逐步取代 hessian2。 其中，Kryo 是一种非常成熟的序列化实现，已经在 Twitter、Groupon、Yahoo 以及多个著名开源项目（如 Hive、Storm）中广泛的使用。而 FST 是一种较新的序列化实现，目前还缺乏足够多的成熟使用案例。 吞吐量：吞吐量是指对网络、设备、端口、虚电路或其他设施，单位时间内成功地传送数据的数量（以比特、字节、分组等测量）。 在面向生产环境的应用中，目前更优先选择 Kryo。 Dubbo + Hystrix 实现服务熔断熔断器简介在微服务架构中，根据业务来拆分成一个个的服务，服务与服务之间可以通过 RPC 相互调用。为了保证其高可用，单个服务通常会集群部署。由于网络原因或者自身的原因，服务并不能保证 100% 可用，如果单个服务出现问题，调用这个服务就会出现线程阻塞，此时若有大量的请求涌入，Servlet 容器的线程资源会被消耗完毕，导致服务瘫痪。服务与服务之间的依赖性，故障会传播，会对整个微服务系统造成灾难性的严重后果，这就是服务故障的 “雪崩” 效应。 为了解决这个问题，业界提出了熔断器模型。Netflix 开源了 Hystrix 组件，实现了熔断器模式，Spring Cloud 对这一组件进行了整合。在微服务架构中，一个请求需要调用多个服务是非常常见的，如下图： 较底层的服务如果出现故障，会导致连锁故障。当对特定的服务的调用的不可用达到一个阀值（Hystrix 是 5 秒 20 次） 熔断器将会被打开。熔断器打开后，为了避免连锁故障，通过 fallback 方法可以直接返回一个固定值。","categories":[{"name":"分布式","slug":"分布式","permalink":"http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"Gateway","slug":"Gateway","date":"2020-04-08T06:23:09.000Z","updated":"2020-04-10T11:53:58.924Z","comments":true,"path":"2020/04/08/Gateway/","link":"","permalink":"http://yoursite.com/2020/04/08/Gateway/","excerpt":"","text":"简介 由于有如此众多的客户端和服务器，在云体系结构中包括一个API网关通常会很有帮助。网关可以负责保护和路由消息，隐藏服务，限制负载以及许多其他有用的事情。Spring Cloud Gateway为您提供对API层的精确控制，集成了Spring Cloud服务发现和客户端负载平衡解决方案，以简化配置和维护。 SpringCloud Gateway 是Spring cloud的一个全新的项目，基于Spring Framework 5，Project Reactor和Spring Boot 2.0等技术开发的网关，它旨在为为微服务架构提供简单有效的统一的API路由管理方式。 SpringCloud Gateway作为Spring cloud 生态系统中的网关，目标是替代zuul，在Springcloud2.0以上的版本中，没有对新版本的zuul2.0一盒收纳柜最新高性能版本进行集成，仍然还是使用的是Zuul1.x非Reactor模式的老版本。而为了提升网关的性能，SpringCloud Gateway 是基于WebFlux框架实现的，而WebFlux框架使用了高性能的Reactor模式通信框架Netty。 Spring Cloud Gateway的目标提供统一的路由方式且基于Filter链的方式提供网关的基本功能，例如：安全，监控/指标，和限流。 Spring Cloud架构亮点 特征Spring Cloud Gateway功能： 基于Spring Framework 5，Project Reactor和Spring Boot 2.0构建 能够匹配任何请求属性上的路由。 谓词和过滤器特定于路由。 Hystrix断路器集成。 Spring Cloud DiscoveryClient集成 易于编写的谓词和过滤器 请求速率限制 路径改写 三大核心概念Route（ 路由）路由是构建网关的基本模块，它由ID，目标URI，一系列的断言和过滤器组成，如果断言为true则匹配该路由。Predicate（断言）开发人员可以匹配HTTP请求中的所有内容（例如请求头或请求参数），如果请求与断言相匹配则进行路由。Filter（过滤）值得是Spring框架中GatewayFilter的实例，使用过滤器，可以在请求被路由前或者之后对请求进行修改。工作流程 Clients make requests to Spring Cloud Gateway. If the Gateway Handler Mapping determines that a request matches a route, it is sent to the Gateway Web Handler. This handler runs the request through a filter chain that is specific to the request. The reason the filters are divided by the dotted line is that filters can run logic both before and after the proxy request is sent. All “pre” filter logic is executed. Then the proxy request is made. After the proxy request is made, the “post” filter logic is run. 客户端向Spring Cloud Gateway发出请求。然后在Gate Handler Mapping中找到与请求相匹配的路由，将其发送到Gateway Web Handler。 Handler再通过指定的过滤器来讲请求发送到实际的服务执行业务逻辑，然后返回。 过滤器之间用虚线分开是因为过滤器可能会在发送代理之前（“pre”）或之后（”post“）执行业务逻辑。 Filter在”pre“类型的过滤器可以做参数校验、权限校验、流量监控、日志输出、协议转换等 在”post“类型的过滤器中可以做响应内容、响应头修改，日志输出，流量监控等有着非常重要的作用。 核心逻辑：路由转发+执行过滤器链","categories":[{"name":"Springcloud","slug":"Springcloud","permalink":"http://yoursite.com/categories/Springcloud/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"HDFS","slug":"HDFS","date":"2020-04-08T06:23:09.000Z","updated":"2020-04-11T08:18:06.026Z","comments":true,"path":"2020/04/08/HDFS/","link":"","permalink":"http://yoursite.com/2020/04/08/HDFS/","excerpt":"","text":"大数据简介一、大数据特征 - 6V 数据体量大，一般从TB级别开始计算 数据种类和来源多 数据的增长速度越来越快 数据的价值密度越来越低，但是这不意味着想要的数据越来越少，相反，想要的数据越来越多的，但是样本总量的增长速度是要高于想要的数据的增长速度的 数据的真实性/质量 数据的连通性 数据的动态性、数据的可视化、合法性 二、概述 Hadoop是Apache提供的一个开源的、可靠的、可扩展的系统架构，可以利用分布式架构来进行海量数据的存储以及计算 Hadoop之父：Doug Cutting（道格·卡丁） 需要注意的是Hadoop处理的是离线数据，即在数据已知以及不要求实时性的场景下使用 Hadoop发行版: a. Apache版本最基本的版本，入门学习较好。原生版本管理相对比较混乱 b. CDH版本是Cloudera提供的商业版本，相对Apache Hadoop更加的稳定和安全 c. Hortonworks文档相对完善友好 三、 版本： Hadoop1.0：包含Common，HDFS，和MapReduce ，停止更新 Hadoop2.0：包含Common，HDFS，和MapReduce，YARN。1.0和2.0完全不兼容，Hadoop2.8之前的不包含ozone Hadoop3.0：包含Common，HDFS，和MapReduce，YARN和ozone。 四、模块 Hadoop Common: 基本模块 Hadoop Distributed File System (HDFS™): 分布式存储 Hadoop YARN:进行任务调度和资源管理 Hadoop MapReduce: 分布式计算 Hadoop Ozone:对象存储 五、安装方式 单机模式：只能启动MapReduce 伪分布式：只能启动HDFS，MapReduce和YARN的大部分功能 完全分布式：能启动Hadoop的所有功能 HDFS一、概述 HDFS是Hadoop提供一套用于进行分布式存储的文件系统 HDFS根据Google的GFS来进行实现的特点： 能够存储超大文件 - 切块 快速的应对和检测故障 - 心跳 能够在相对廉价的机器上来进行动态横向扩展 不支持低延迟响应 不建议存储小文件 - 每一个小文件对应一条元数据，大量的硒筽啊文件就会产生大量的元数据。元数据过多导致内存被大量占用，导致查询效率降低。 简化的一致性模型，一次写入多次读取，不允许修改，但允许追加写入。 不支持强事务或者不支持事务，在实际开发中，在样本量足够大的前提下，允许存在一定的容量误差 二、基本概念 HDFS主要包含2类进程：NameNode和DataNode 在HDFS中，上传的文件自动的进行切块，每一个数据块称之为一个Block HDFS会自动对数据进行备份，每一个备份称之为副本（replication/replicas）默认为3 HDFS仿照Linux设置一套虚拟的文件系统,根路径/ 三、Block Block是HDFS中存储数据的基本单位，即在HDFS中所有数据都是以Block形式存储 Block默认是128M大小，可以通过dfs.blocksize(在hdfs-site.xml，单位是字节)来设置 如果一个文件不到一个Block的大小，那么注意，这个文件本身多大block就是多大 在HDFS中，会给每一个Block自动分配一个全局递增的Block ID 在HDFS中，会给每一个Block分配一个时间戳（Generation Stamp）编号 切块的意义 为了能够存储超大文件 为了能够进行快速备份 四、NameNode NameNode是HDFS中的主节点（Master），作用：管理DataNode和记录与元数据（metadata） 元数据是描述数据的数据 —在HDFS中，元数据对存储的文件进行描述，主要包含 文件的存储路径，例如/txt/a.txt 文件的权限 上传的用户和用户组 文件的大小 Block的大小 文件和BlockID的映射关系 副本数量 BlockID和DataNode的映射关系 元数据是存储在内存以及磁盘中 维系在内存中的目的是为了快速查询 维系在磁盘中的目的是为了崩溃恢复 元数据在磁盘中的存储路径由hadoop.tmp.dir(core-site.xml)属性来决定的 元数据的记录和fsimage以及edits文件相关 edits记录写操作 fsimage：记录元数据。fsimage中的元数据往往是落后于内存中的元数据 当NameNode接受到写请求的时候，NameNode会先将这个写请求记录到edits-iprogress中，如果记录成功则修改内存中的元数据。内存中的元数据修改成功之后会给客户端返回一个成功信号。这个过程中fsimage文件中的元数据没有被修改 当达到指定条件的时候，触发edits_inprogress文件的滚动，edits_inprogress会滚动生成edits文件，会产生一个新的edits_inprogress。在edits_inprogress滚动过程中，会触发fsimage文件的更新，会根据edits_inprogress中的命令去修改fsimage文件中的元数据 edits_inprogress文件的滚动条件 空间：当edits_inprogress文件达到指定大小（64m通过f s.checkpoint.size，单位是字节 - core-site.xml)的时候，会产生滚动 时间：当距离上一次滚动的时间间隔（默认3600s，通过f s.checkpoint.period来调节，默认单位是秒-core site.xml）达到指定大小，会产生滚动 重启：当NameNode重启的时候，自动触发edits——inprogress文件的滚动 强制滚动：可以利用 hadoop dfsadmin -rollEdits来强制滚动 NameNode通过心跳机制来管理DataNode， DataNode定时的通过RPC的方式来给NameNode发送心跳。 默认情况下是每隔3s给NameNode发送心跳，可以通过dfs.heartbeat.interval来修改，单位是秒 —hdfs site.xml NameNode超过10min没有收到DataNode的心跳，就会认为这个DataNode已经Lost（丢失），那么此时NameNode就会讲这个DataNode上的Block备份到其他节点，保证整个集群的副本数量 心跳信息 当前DataNode的状态（预服役、服役、预退役） 当前DataNode存储的Block信息 clusterid：集群编号。当NameNode被格式化的时候，会自动计算产生一个clusterid，每格式化一次就会重新计算产生，当HDFS集群启动的时候，NameNode会将clusterid分发给每一个DataNode的信息之后，会先校验clusterid是否一致；同样，DataNode收到NameNode的指令的时候也会校验clusterid。DataNode只能接受一次clusterid 当NameNode重启的时候，会自动触发edits_inprogress文件的滚动，产生一个新的edits文件和一个新的edits_inprogress文件，同时更新fsimage。当更新完fsimage之后，NameNode就会将fsimage文件中的内容加载到内存中，加载完成后，NameNode等待DataNode的心跳。如果在指定时间内没有收到心跳，则认为节点丢失重新备份。如果收到DataNode的心跳，那么NameNode会校验这个心跳信息，这个过程称之为安全模式。在安全模式中，NameNode如果校验失败，则试图恢复数据，恢复完成之后会重新校验；如果校验成功，则自动退出安全模式。 当NameNode重启的时候，自动进入安全模式，实际开发过程中，如果进入安全模式，需要等待NameNode自动退出安全模式，但是在合理的时间内，NameNode没有退出安全模式，就说明数据产生了不可挽回的丢失，此时需要强制退出安全模式（hadoop afsadmin -safemode leave） 在安全模式中，HDFS不对外提供写服务 正因为有安全模式的存在所以在为分布式中副本数量设置为1； 五、多副本的放置策略 第一个副本 如果集群内部上传，那么谁上传第一个副本就放在谁身上 如果是集群外部上传，NameNode会选择相对较闲的DataNode的节点存储数据 第二个副本 Hadoop2.7之前：第二个副本是放在和第一个副本不同机架的节点上 Hadoop2.7开始：第二个副本是放在和第一个副本相同机架的节点上 第三个副本 Hadoop2.7之前：第三个副本是放在和第二个副本相同机架的节点上 Hadoop2.7开始：第三个副本是放在和第二个副本不同机架的节点上 更多副本：那个几点相对空闲就放在谁身上 六、 机架感知策略 在HDFS中默认没有启用，需要在hadoop-site.xml中配置topology.script.file.name开启机架感知策略 HDFS的机架感知策略是通过指定的脚本来进行配置，这个脚本可以是python和shell，所谓的机架就是一个映射，只需要将DataNode的主机名或者ip映射到值上就对应了机架 机架指的是逻辑机架不是物理机架，允许将一个或几个物理机架上的节点映射到同一个逻辑机架上，但是实际开发中，一般是一个物理机架对应一个逻辑机架 七、DataNode DataNode是HDFS中的从节点，作用：存储数据，数据以block的形式来存储 DataNode会将数据存储在磁盘中，在磁盘上的存储位置由hadoop.tmp.dir属性来决定 DataNode的状态:预服役，服役，预退役，退役 DataNode会主动给NameNode发送心跳来进行注册 八、SecondaryNameNode SecondaryNameNode并不是NameNode备份，而是辅助NameNode进行edits_inprogress文件的滚动和fsimage的更新 在HDFS集群中，如果存在SecondaryNameNode，edits_inprogress文件的滚动和fsimage的更新由SecondaryNameNode来做，如果不存在，上述事情由NameNode自己来做，存活与否不影响集群向外提供服务，只会影响效率 在HDFS集群中，支持的结构一般是有两种 1个NameNode+1个SecondaryNameNode+多个DataNode 2个NameNode+多个DataNode 因为在HDFS中，NameNode是核心节点，所以一般要考虑NameNode的备份，那么此时就需要系用双NameNode机制，一个处于active状态，另一个处于备份状态，当active的NameNode宕机后，备份的立即切换为active，在实际工作中以第二方案为主，这样实现了集群的高可用（HA）。 九、回收站机制 在HDFS中，回收站机制默认不开启，即删除命令会立即生效， 如果要开启回收站机制，要在core-site.xml中配置 十、dfs目录 dfs目录实际上就是HDFS的数据目录，由hadoop.tmp.dir属性来决定存储路径 dfs子目录 a. data 对应datanode数据的存储路径 b. name 对应namenode数据的存储路径 c. namesecondary 对应namesecondary数据的存储路径 实际开发中，三个子目录应该出现在三个不同的节点上 in_use.lock用于标记是否已经启动对应的进程 每一个blk文件对应一个.meta,这个.meta文件可以认为是对blk的校验 HDFS在第一次启动的时候，间隔1min之后出发edits_inprogress文件的滚动，之后就按照指定的条件进行滚动 在HFDS中，每一次写操作看成一次事务，分配一个全局递增的编号，简称txid 文件上传完成之后不能修改 fsimage_XXXX.md5是利用了md5加密算法对fsimage_xxxx来进行校验 version文件中包含的主要内容 clusterID：集群编号 - 本质上是进行校验的 storageType：节点类型 blockpooIID：块池编号 Hadoop流程写流程 -put 客户端发起RPC请求到NameNode要求上传文件 NameNode收到请求之后会进行校验 校验上传的路径是否有写入权限 检验上传的路径下是否有同名文件 如果校验失败，直接报错，如果校验成功，则NameNode会修改edits_inprogress,更新内存中的元数据，最后会给客户端返回一个允许上传的信号。 客户端收到信号之后，会给NameNode发送请求，请求获取第一个Block的存储位置 NameNode收到请求之后，会等待DataNode的心跳，然后选择符合要求的节点（副本放置策略），将节点位置（默认是3个）放入队列中返回给客户端 客户端收到队列之后，会从队列中将所有的位置去除，然后选择一个较近的节点将第一个Block的第一个副本写入，客户端同时会告诉第一个副本所在的节点两个副本的存储位置，第一个副本所在的节点就会通过管道（pipeline，本质就是NIO Channel）将第二个副本写入对应的节点同时告诉第二个副本所在的节点最后一个副本的存储位置，第二个副本所在的节点通过管道将第三个副本写入对应的节点；第三个副本写完之后，会给第二个副本所在的节点返回一个ack（确认字符）信号，第二个副本所在的节点会给第一个副本所在的节点返回一个ack；第一个副本所在的节点会给客户端返回一个ack 客户端收到ack之后，会向namenode发送请求要下一个Block的存储位置，重复4，5，6三个步骤 当写完所有的数据之后，客户端会给NameNode发送信号，请求NameNode关闭文件。文件一旦关闭就不能再修改了。 读流程 -get 客户端发起RPC请求到NameNode要求下载指定文件 NameNode在收到请求之后会查询元数据确定是否有指定的文件，如果没有直接报错，如果有，则给客户端返回一个信号表示允许读取 客户端收到信号之后，会再给NameNode发送请求，请求获取这个文件的第一个block的存储位置 NameNode在收到请求之后，会将第一个Block的存储位置（默认是3个）放到一个队列中返回客户端 客户端收到队列之后，会从队列中将block的位置全部取出来，从中选择一个较近（网络拓扑距离）的DataNode来读取第一个Block 客户端读取完Block之后，会进行checksum（校验和 - 实际上就是利用.meta文件进行校验）验证，如果校验失败，则客户端会给NameNode发送信号，重新选择节点重新读取；如果校验成功，那么客户端会再向NameNode要下一个Block的地址，重复4，5，6三个步骤 当客户端读取完最后一个Block的时候，NameNode就会关闭文件（实际就是关流）删流程 -rmr 客户端发起RPC请求到NameNode要求删除指定的文件 NameNode收到请求后，先记录edits_inprogress文件，然后修改内存中的元数据，然后NameNode就会给客户端返回一个成功信号，但是这个过程中，注意文件并没有从HDFS上移除而是依然存储在DataNode上 NameNode等待DataNode的心跳，收到心跳信息之后，NameNode就会校验这个心跳信息（比对元数据和心跳信息中的BlockID是否一致），然后NameNode就会进行心跳响应，要求DataNode删除对应的Block DataNode收到心跳响应之后才会删除Block，此时文件才真正的从HDFS上移除","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://yoursite.com/categories/Hadoop/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"ZooKeeper","slug":"ZooKeeper","date":"2020-04-07T01:45:00.000Z","updated":"2020-04-28T01:38:10.806Z","comments":true,"path":"2020/04/07/ZooKeeper/","link":"","permalink":"http://yoursite.com/2020/04/07/ZooKeeper/","excerpt":"","text":"简介一、概述 ZooKeeper是由Yahoo（雅虎）开发的，后来提供给Apache管理的一套开源的，用于进行分布式的框架 Zookeeper是根据Google的关于Chubby Lock来设计实现的 Zookeeper是是一个中心化分布式框架的管理框架 zookeeper.out：启动日志，可以查看启动失败原因 二、安装 单机模式：在一台机器上安装，往往只能启动这个框架的一部分的功能 伪分布式：在集群中安装，能够启动这个框架的所有功能 完全分布式：在集群中安装，能启动这个框架的所有功能 基本概念一、基本理论 Zookeeper本身是一个树状结构 - znode树 根结点时/ 每一个节点称之为Znode节点 每一个持久节点都可以挂载字节点，每一个临时节点都没有字节点 每一个节点都需要携带数据，这个数据往往是对节点的描述 不存在相对路径的说法,所有节点都需要从/计算 zookeeper将数据维系在磁盘以及内存中 维系在内存中的目的是为了快速读写 维系在磁盘中的目的是为了防止奔溃 zookeeper在磁盘中存储的位置在dataDir指定 理论上来说，是可以做缓存服务器来使用的 ，但实际开发中不会这么做。因为zookeeper的主要作用作为协调框架来使用的，如果作为缓存服务器使用会占用大量内存,降低zookeeper的协调效率 在节点下不能存在同名节点 在zookeeper中，会讲每一次的写操作看作是一个事务，然后给这个事务分配一个全局递增的编号，称之为事务id，简称Zxid 二、命令 ls / 查看根目录的字节点 create /video ‘manage video servers’ 创建节点 delete / 删除节点下无字节点 rmr /video 递归删除 set /news ‘manage news servers’ 修改节点信息 get /news 获取节点信息 create -e /video ‘’ 创建临时节点 三、节点信息 cZxid 创建事务id ctime 创建时间 mZxid 修改事务id mtime 修改时间 pZxid 字节点个数变化的事务id cversion 字节点个数变化的次数 dataVersion 数据变化的次数 aclVersion 权限变化次数 ephemeralOwner 如果节点为临时节点，那么它的值为这个节点拥有者的session ID；如果该节点不是ephemeral节点, ephemeralOwner值为0 dataLength 节点数据的长度 numChildren 字节点的数目 123456789101112manage news serverscZxid &#x3D; 0x9ctime &#x3D; Fri Apr 03 10:44:15 CST 2020mZxid &#x3D; 0xamtime &#x3D; Fri Apr 03 10:44:42 CST 2020pZxid &#x3D; 0xdcversion &#x3D; 3dataVersion &#x3D; 1aclVersion &#x3D; 0ephemeralOwner &#x3D; 0x0dataLength &#x3D; 19numChildren &#x3D; 3 四、节点类型 顺序节点 create -s /news/n ‘ ‘ s指的是sequence 非顺序节点 create /news/n ‘’ 概述 在配置zookeeper的时候，并没有指定那台服务器节点成为leader，那台服务器是follower，但是zookeeper集群启动之后自动出现了leader和follower，那么说明zookeeper在启动过程中出现了选举过程。 在zookeeper集群刚刚启动的时候，这个时候每一个节点都会推荐自己成为leader，并且将自己的选举信息发送给其他节点。 节点之间会两两比较选举信息，比较成功的会进行下一轮选举，经过多轮选举，最终胜出的节点成为leader。 细节 选举信息： a. 每一台服务器（节点）的最大事务id b. 选举编号 -myid c. 逻辑时钟值 -控制节点的选举轮数 比较原则： a.先比较两个节点的事务id，谁大谁赢 - 事务id越大，说明接受的写操作越频繁 b. 如果节点的事务id越大一样，则比较myid，谁大谁赢 - 在zoo.cfg,要求server.x中的x不一致 c. 一个节点要想成为leader，至少要胜过一半的节点 - 过半性 d. 在zookeeper集群中为了保证稳定性，一旦确定leader，新加入的节点自动成为follower e. 为了避免出现单点故障问题，一旦老leader出现问题，zookeeper集群会自动触发选举机制，会自动选觉一个新的节点成为leader。 f. 因为集群中出现多个leader，这个现象称之为脑裂 g.脑裂产生条件 网络产生了分裂/隔离 分裂之后进行了选举h. 在zookeeper中，当存活的解ID哪个数不足一半的时候，那么存活的节点之间不选觉也不对外提供服务 - 过半性i. 在zookeeper集群中一般把节点个数设置为奇数以满足过半性j.在zookeeper集群中选举出leader会分配一个全局递增的编号，也会通知其他节点，称之为epochid。在zookeeper集群中如果出现多个leader，会保留epochid最大的id为leader，同时将其他的leader节点的状态进行切换，切换成followerk. 集群中节点的状态 voting/looking 选举状态 follower 追随者/跟随者 leader 领导者 observer 观察者l.选觉状态下不对外提供服务 ZAB一、概述 ZAB（Zookeeper Atomic Broadcast）是一套专门为zookeeper设计的用于进行原子广播和崩溃恢复的协议。 ZAB基于2pc算法来进行设计，利用了过半性和PAXOS算法来进行了改善 - 核心思想：一票否决二、原子广播 作用：保证数据的一致性 - 在zookeeper中访问任意一个节点获取到的数据都是相同的 原子广播是基于2pc算法来设计的，然后引入过半性来进行改进 2pc - two phase commit -二阶段提交 -实际过程中要么请求-提交，要么请求-中止 请求阶段：协调者收到任务之后，会将任务发给每一个参与者，等待参与者的反馈 调教阶段：如果协调者收到所有参与者的yes，那么协调者要求所有的参与者执行刚才的任务 中止阶段：如果协调者如果没有接受到所有参与者接受请求的信息，就会拒绝这次任务 原子广播的流程： 记录日志失败的可能性 日志文件被占用 - 此时记录的时候却是以只读模式 - 在计算机中，一个文件一旦被某个进程打开其他进程往往以只读模式来打开 磁盘管道损坏 磁盘存储已满 当follower记录日志失败，但是leader却还要求follower执行这个任务的时候，follower就会向leader发送请求，重新获取任务然后leader会将任务放入到队列中发送给follower，重新记录日志；如果再次记录失败，会将重新给leader发送请求重新记录直到成功为止。 - 如果是第一种可能性导致日志记录失败，只要日志未见被释放那么日志就有可能重新记录成功；但是如果是后两种可能，那么就属于硬件环境问题 三、崩溃恢复 作用：避免单点故障，保证集群的高可用 在zookeeper集群中，当leader节点丢失或宕机时，集群不会因为一个节点而不服务，而是会通过选举机制重新选取leader节点，这个过程叫崩溃恢复。 每一个新上任的leader会把自己的epochid分发给每一个follower，follower在收到epochid会把它存储到acceptepochid中。 当一个节点重启炼乳集群之后，这个节点会先寻找当前节点的最大事务id，将自己的最大事务id发送给leader进行比较，如果发现不一致，leader就会将所缺的操作重新放到队列中发送给follower重新补奇，补齐过程中这个follower暂时不会对外提供服务 在集群中，事务id实际上有64位二进制数字（16个十六进制数字）组成，其中高32位代表epochid，低32位代表的是事务id version-2 中log中记录写操作，snap记录整个zookeeper的树结构 四、观察者 观察者的特点：既不参与投票也不参与选举，但是会监听 投票或者选举结果，根据结果来执行对应的操作 在实际开发中，如果集群庞大，为了提高效率，往往将90%的节点设置成observer。如果在异构网络中，也会将绝大部分的节点设置为观察者 在zookeeper中observer不参与选举但是选举出leader之后，观察者听从leader命令；如果leader和follow二决定执行或者不指定某个操作的时候，observer也需要跟着执行或不执行。 observer没有投票权和选举权，所以observer是否存活并不影响过半。 五、 特性 过半性：过半选举，过半存活，过半执行 原子性：要么所有的节点都执行请求，要么都不执行 数据一致性：访问任意节点所拿到的数据是一致的 顺序性：leader会讲请求发送到队列中发送给follower，所以保证leader和follower的请求的执行顺序是相同的 实时性：实时监控zookeeper的集群变化 可靠性：奔溃恢复 - 不会因为一个节点当即就导致整个集群不服务 AVRO一、概述 AVRO是由Apache提供的一套用于序列化和RPC的机制 AVRO原本是Hadoop的子组件，后来AVRO被独立出来成为了一个顶级项目 二、序列化 序列化实际上是指按照指定的格式将数据转化为其他形式 序列化的作用：为了方便数据的存储和传输 序列化的衡量标准： 序列化花费的时间，占用的资源等 序列化之后产生的数据量的大小 考虑序列化机制本身是否跨平台跨语言 如果数据要在不同的语言之间传输，那么意味着数据要做到数据与语言无关 数字，布尔值，字符/字符串 字符/字符串与编码有关-只要两种计算机语言用同一套码表就可以进行数据的传输 实际开发过程中，绝大部分的序列化机制都考虑转化为字符串-AVRO实际就是将对象转化为jason 三、RPC RPC(Remote Procedure Call,远程过程调用)允许程序员在一个节点（服务器）上去远程调用另一个节点上的方法而不用显示的实现这个方法 特点：简单，高效，通用 RPC的stub（存根）就是限制不同的节点上的方法签名是一致的，在Java中一般用接口","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"JUC","slug":"JUC","date":"2020-03-31T07:54:44.071Z","updated":"2020-05-01T11:33:57.036Z","comments":true,"path":"2020/03/31/JUC/","link":"","permalink":"http://yoursite.com/2020/03/31/JUC/","excerpt":"","text":"原子性操作–Atomic 原子性 原子性操作实际上就是针对属性来提供线程安全的方法，在底层会自动采用CAS来保证线程安全。 volatile是Java提供的轻量级的线程同步机制 阻塞队列 BlockingQueue原则： 遵循FIFO 往往是有界的，容量固定不变 具有阻塞特性：如果队列已满，则试图放入的线程会被阻塞；如果队列为空，就会阻塞尝试获取的线程 不允许元素null 方法： 抛出异常 返回特殊值 产生阻塞 定时阻塞 添加 add - IllegalStateException offer - false put offer 获取 remove - NoSuchElementException poll - null take poll ConcurrentMap -并发映射 ConcurrentHashMap底层基于数组h+链表来存储数据，数组的每一个位置称之为桶 默认初始容量16（桶的数量），加载因子0.75 如果指定容量,底层会进行计算，实际容量一定是2的n次方 每次扩容每次增加一倍的桶数,扩容之后需要rehash操作123456789101112131415161718public ConcurrentHashMap(int initialCapacity) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(); int cap = ((initialCapacity &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(initialCapacity + (initialCapacity &gt;&gt;&gt; 1) + 1)); this.sizeCtl = cap; &#125;//底层算法private static final int tableSizeFor(int c) &#123; int n = c - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; &#125; 分段锁，封桶锁机制，不是把整个映射锁起来 在jdk1.7引入读写锁ConcurrentNavigableMap - 并发导航映射 ConcurrentNavigableMap提供了用于截取子映射的方法 ConcurrentNavigableMap本身是一个接口，所以更多的是使用它的实现类ConcurrentSkipListMap - 并发跳跃表映射 跳跃表 针对有序集合 适合于增删少，查询多的场景 跳跃表可以进行多层提取，最上层的跳跃表的元素个数不能少于2个 跳跃表是典型的“以空间换时间“的产物 在跳跃表中新添元素提取到上层跳跃表，遵循抛硬币原则 跳跃表的时间复杂度是O(logn)ExecutorService概述 本质上是一个线程池线程池：减少服务器的线程的创建和销毁，做到连接的复用 刚开始创建时空的，里面不包含任何线程 每接受一个请求，线程池都会创建一个线程(核心线程）来处理这个请求，在创建线程的时候就需要指定线程的数量 在核心线程达到指定数量之前，来一个请求就会创建一个新的线程来处理这个请求，直到线程满了就不再创建 核心线程处理完这个请求不会销毁，等待下一个请求 如果核心线程全部沾满，新来的请求会暂存在工作队列，工作队列本身是一个阻塞式队列 当核心线程有空的时候，就会从工作队列中取出交给核心线程处理 如果工作队列被全部占用，有接受到新的请求，会将这个请求叫个临时线程来处理 当临时线程处理完请求之后不会立即结束，而是会存活一段时间，如果在这个时间段内接受到新的请求，会处理新的请求，如果没有接收到请求，就会被kill掉 临时线程不会从工作队列中拿出请求处理 当临时线程全部占用，那么新来的请求就会交给执行助手来拒绝处理，在实际开发中，如果需要拒绝请求，可能会产生多部操作，例如记录日志，页面跳转，请求重发 ScheduledExecutorService：定时调度执行器服务。在线程池的基础上加入了定时调度效果。这个线程池本身是很多定时调度机制的底层实现 内存模型栈内存：执行方法- 计算 栈内存是线程独享的 堆内存：存储对象，线程共享 方法区：存储类信息（字节码，静态，常量）。线程共享的 本地方法栈：执行本地方法，线程独享 本地方法是指在Java中用native声明但使用其他语言实现的方法 pc计数器/寄存器：指令计数，线程独享如果需要计算一台服务器的线程承载量，考虑线程独享部分。在jdk1.8中，栈内存最小128kpc计数器一般只占几个字节，可以忽略 红黑树定义a. 红黑树本质上一棵自平衡二叉查找树 b.特征： i. 所有节点的颜色非红即黑 ii. 根节点为黑色 iii. 红节点的子节点一定黑节点，黑节点的子节点可以是红节点也可以是黑节点 iv. 最底层的叶子节点一定是黑色的空节点 v. 从根节点到任意一个叶子节点的路径中经过的黑色节点个数一致，即黑节点高度是一致的 vi. 新添的节点颜色一定是红的c. 修正： i. 涂色：父子节点为红，叔父节点为红，将父节点核叔父节点涂成黑色，然后将祖父节点涂成红色 ii. 左旋：父子节点为红，叔父节点为黑，且子节点是右子叶，那么以子节点为轴进行左旋 iii. 右旋：父子节点为红，叔父节点为黑，且子节点是左子叶，那么以父节点为轴进行右旋d. 红黑树的时间复杂度是O(logn) Callable定义： Callable是Java提供的一种定义线程的方式,在使用的时候通过泛型执行返回值类型 Callable和Runnable的区别 Runnable Callable 返回值 没有返回值 通过泛型来指定返回值的类型 启动方式 1. 通过Thread的start方法启动 2. 通过线程池的submit或者是execute方法执行 通过线程池的submit方法执行 容错机制 不允许抛出异常，所以不能利用全局方式(例如Spring中的异常通知)处理 允许抛出异常，所以可以利用切面或者是全局方式来处理异常 分叉合并池 Fork：分叉。将一个大任务拆分成多个小任务交给多个线程执行 Join：合并。将拆分出来的小任务的执行结果进行汇总 分叉合并的目的是为了提高CPU的利用率 在数据量相对小的时候，循环会比分叉合并快；数据量越大，分叉合并的优势越明显 分叉合并在进行的时候，导致其他程序的执行效率显著降低，所以分叉合并一般是在周末的凌晨来进行 分叉合并在分配子线程的时候，尽量做到每个核上的任务均匀：少的多分，多的少分 在分叉合并中，为了减少”慢任务”带来的效率降低，采取”work-stealing”(工作窃取)策略：当一个核将它的任务队列处理完成之后，这个核并不会闲下来，会随机扫描一个核，然后从被扫描核的任务队列尾端”偷取”一个任务回来执行 Lock - 锁概述 相对于synchronized更加灵活和精细 ReadWriteLock - 读写锁 a. 在使用的时候，需要用这个接口的实现类ReentrantReadWriteLock b. 在加锁的时候，需要先通过对应的方法来获取读锁或者写锁 公平和非公平策略 a. 非公平策略下，线程会直接抢占执行权，在资源有限的前提下，线程之间抢到的次数不一样 b. 公平策略下，线程不是直接抢占执行权，而是去抢占入队顺序。宪曾之间的执行次数是基本一致的 c. 默认情况下，使用的是非公平策略 d. 相对而言，非公平策略的效率会更高一些 其他 CountDownLatch：闭锁/线程递减锁。对线程进行计数，在计数归零之前会让线程陷入阻塞，直到计数归零才会放开阻塞 - 一波线程结束之后开启另一波线程 CyclicBarrier：栅栏。对线程进行计数，在计数归零之前让线程陷入阻塞，直到计数归零为止才会放开阻塞 - 线程到达同一个地点之后再分别执行 Exchanger：交换机。用于交换2个线程之间的信息 Semaphore：信号量。线程只有获取到信号之后才能执行，执行完成之后需要释放信号。如果所有的信号都被占用，那么后来的线程就会被阻塞","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"高并发基础","slug":"高并发基础","date":"2020-03-31T01:11:40.883Z","updated":"2020-04-01T01:08:18.732Z","comments":true,"path":"2020/03/31/高并发基础/","link":"","permalink":"http://yoursite.com/2020/03/31/%E9%AB%98%E5%B9%B6%E5%8F%91%E5%9F%BA%E7%A1%80/","excerpt":"","text":"基本概念a. 同步：一个对象或者一段逻辑在一段时间只允许被一个线程访问b. 异步：一个对象或者一段逻辑在一段时间只允许被多个线程访问c. 阻塞：一个线程只要没有拿到想要的结果就会一直等在这儿d. 非阻塞：一个线程不管有没有拿到结果，都会继续执行下面的程序，不会在那边的等待 分类a. BIO ： 同步式阻塞线程b. NIO ： 同步式非阻塞式线程 JDK1.4c. AIO ： 异步式非阻塞式线程 JDK1.8，目前没有成熟的框架 BIO的缺点 一对一连接：每次有一个客户端发起连接，服务端就要产生一个线程来处理这个连接；如果有大量的客户端发起连接，就会产生大量的线程，可能导致服务端崩溃。 无效连接 如果客户端连接后不做任何操作但是一直保持连接，服务端的处理线程就不会释放，导致服务端 阻塞 效率低NIO三大组件Buffer-缓冲区 作用：存储数据 底层是基于数组来存储数据,针对基本数据类型，提供7个子类：ByteBuffer，ShortBuffer，InteBuffer，LongBuffer，FloatBuffer，DoubleBuffer，CharBuffer 重要位置a.capacity 容量位b.position 操作位c.limit 限制位d. mark 标记位4.重要操作反转缓冲区1234567//byteBuffer.flip(); public final Buffer flip() &#123; limit = position; position = 0; mark = -1; return this; &#125; clear清空缓冲区123456public final Buffer clear() &#123; position = 0; limit = capacity; mark = -1; return this; &#125; reset 重置缓冲区1234567public final Buffer reset() &#123; int m = mark; if (m &lt; 0) throw new InvalidMarkException(); position = m; return this; &#125; rewind 重绕缓冲区123456public final Buffer rewind() &#123; position = 0; mark = -1; return this; &#125; Channel 双向传输网路通信123456789101112131415161718192021222324252627282930313233343536373839404142434445464748//client端public class client &#123; public static void main(String[] args) throws IOException, InterruptedException &#123; //开启通道 SocketChannel sc = SocketChannel.open(); //手动设置非阻塞 sc.configureBlocking(false); //一旦设置为非阻塞，无论是否建立连接，都会往下执行 //发起连接，channel默认是阻塞的 sc.connect(new InetSocketAddress(\"localhost\",8100)); //判断是否建立连接 while (!sc.isConnected())&#123; //试图再次建立连接,这个方法会自动计数，如果多次建立无果，会自动抛出异常 sc.finishConnect(); &#125; //写出数据，都需要将数据以字节 sc.write(ByteBuffer.wrap(\"one world one dream\".getBytes())); ByteBuffer buffer = ByteBuffer.allocate(1024); Thread.sleep(10); sc.read(buffer); System.out.println(new java.lang.String(buffer.array(),0,buffer.position())); sc.close(); &#125;&#125;//sever端public class server &#123; public static void main(String[] args) throws IOException &#123; //开启通道 ServerSocketChannel ssc = ServerSocketChannel.open(); //绑定监听端口 ssc.bind(new InetSocketAddress(8100)); //手动设置非阻塞 ssc.configureBlocking(false); //无论是否建立连接 //接受连接 SocketChannel sc = ssc.accept(); while (sc==null)&#123; sc=ssc.accept(); &#125; //准备容器存储数据 ByteBuffer buffer = ByteBuffer.allocate(1024); sc.read(buffer); System.out.println(new String(buffer.array(),0,buffer.position())); sc.write(ByteBuffer.wrap(\"hello sever\".getBytes())); ssc.close(); &#125;&#125; Selector 多路复用选择器","categories":[{"name":"高并发","slug":"高并发","permalink":"http://yoursite.com/categories/%E9%AB%98%E5%B9%B6%E5%8F%91/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"InterviewDay01","slug":"interview","date":"2020-03-18T15:48:06.390Z","updated":"2020-04-17T13:19:53.586Z","comments":true,"path":"2020/03/18/interview/","link":"","permalink":"http://yoursite.com/2020/03/18/interview/","excerpt":"","text":"1.volatile是什么？volatile是一种轻量级的同步机制volatile有三个特性： 可见性当多线程修改物理内存种的变量值时，会把内存中的变量值拷贝到各自的工作内存中，用volatile关键字修饰的变量，当一个线程在自己的工作内存中修改完成，写入物理内存后会通知其他线程数据已经修改，这就是可见性 不保证原子性在多线程都在实现如num++；可能回导致运算时加塞的情况，导致最后数据的不一致，这类可以用atomicInteger.getAndIncrement()实现原子性 禁止指令重拍为了性能的优化，jvm在不改变正确语义的前提下，会对代码的执行顺序进行优化，底层进行指令的重拍，在多线程的情况下可能导致数据的错误，加上volatile可以禁止指令重拍 Synchronized和Volatile的比较 1）Synchronized保证内存可见性和操作的原子性 2）Volatile只能保证内存可见性 3）Volatile不需要加锁，比Synchronized更轻量级，并不会阻塞线程（volatile不会造成线程的阻塞；synchronized可能会造成线程的阻塞。） 4）volatile标记的变量不会被编译器优化,而synchronized标记的变量可以被编译器优化（如编译器重排序的优化）. 5）volatile是变量修饰符，仅能用于变量，而synchronized是一个方法或块的修饰符。 volatile本质是在告诉JVM当前变量在寄存器中的值是不确定的，使用前，需要先从主存中读取，因此可以实现可见性。而对n=n+1,n++等操作时，volatile关键字将失效，不能起到像synchronized一样的线程同步（原子性）的效果。 单例模式在多线程环境下可能存在安全问题1234567891011121314151617//推荐使用双重检查,解决多线程单例模式的安全问题public class Singletlon &#123; private static Singtlelon instance =null; private Singtlon()&#123; &#125; public static Singtlelon getInstance()&#123; if(instance==null)&#123; synchronized(Singtlon.class)&#123; if(instance==null)&#123; instance=new Singletlon(); &#125; &#125; &#125; return instance; &#125;&#125; 2.CAS是什么？compare andswap 比较且交换在多线程的环境下，对数据惊醒修改，会将线程工作内存的数值与物理内存的期望值相比较如果相同就交换到更新值，返回true，如果不想等，则返回false底层主要是通过unsafe类来实现原子性 3. 底层是通过do while的自旋锁实现的，在多线程高并发的情况下，导致循环时间过长，引起cpu的开销很大只能保证一个变量的原子操作会导致ABA问题，只关心首尾的数值，忽略中间数值已经被修改过，这样会造成数据错乱的问题 如何解决ABA问题用AtomicStampedReference/AtomicMarkableReference原子引用解决ABA问题类似于版本控制，时间戳管控 集合类不安全之并发修改异常arraylist在多线程的情况下造成安全并发的问题，常见的异常ConcurrentModificationException 并发修改异常如何解决arraylist线程不安全的问题 使用vector代替arraylist，因为vector是线程安全的，底部在add方法上加入了synchronized同步锁 使用Collections集合工具类，调用Collections上的synchronizedList()这个方法可以解决arraylist线程不安全的问题 使用JUC下的CopyOnWriteArrayList这个类来解决线程不安全的问题,底层使用来lock锁来完成这一机制123456789101112131415161718192021public E set(int index, E element) &#123; final ReentrantLock lock = this.lock; lock.lock(); try &#123; Object[] elements = getArray(); E oldValue = get(elements, index); if (oldValue != element) &#123; int len = elements.length; Object[] newElements = Arrays.copyOf(elements, len); newElements[index] = element; setArray(newElements); &#125; else &#123; // Not quite a no-op; ensures volatile write semantics setArray(elements); &#125; return oldValue; &#125; finally &#123; lock.unlock(); &#125; &#125; HsahSet底层是HashMap，因为hashset的底层将value写成来一个最终常量，hashset存储数据不关心value值基本数据类型传值（相当于复制了一份） ，对象的引用是地址的传递（相当于指针）12String str =“aaa”//存储在常量池//会在常量池中找相应的值，有就找没有就会新建，注意内存的分析 公平锁和非公平锁公平锁：线程按照指定的顺序来占锁，按顺序执行非公平锁，允许其他线程加塞，上来就抢占锁，占不到按顺序执行。如synchronizedJava ReentrantLock 而言可以通过构造函数来指定是公平锁还是非公平锁，默认是非公平锁，如果参数列表指定true则为公平锁，非公平锁吞吐量比较大，执行效率高。 12345678910111213141516171819//无参构造/** * Creates an instance of &#123;@code ReentrantLock&#125;. * This is equivalent to using &#123;@code ReentrantLock(false)&#125;. */ public ReentrantLock() &#123; sync = new NonfairSync(); &#125;//含参构造/** * Creates an instance of &#123;@code ReentrantLock&#125; with the * given fairness policy. * * @param fair &#123;@code true&#125; if this lock should use a fair ordering policy */ public ReentrantLock(boolean fair) &#123; sync = fair ? new FairSync() : new NonfairSync(); &#125; 可重入锁（ReentrantLock，递归锁）定义：线程可以进入任何一个它已经拥有的锁所同步着的代码块ReentrantLock and synchronized 是可重入锁，默认非公平，防止死锁，相当于用的是同一把锁 自旋锁定义：是指尝试获取锁的线程不会立即阻塞，而是采用循环的方式去尝试获取锁，这样的好处是减少线程上下文切换的消耗，缺点是循环会消耗cpu 独占锁ReentrantLock和synchronized该锁一次只能被一个线程所占有 共享锁多个线程同时读一个资源类没有任何问题，所以为了满足并发量，读取共享资源应该可以同时进行。 读读可以共存 读写不可以共存 写写不可以共存","categories":[{"name":"Interview","slug":"Interview","permalink":"http://yoursite.com/categories/Interview/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"事务","slug":"事务","date":"2020-02-28T16:00:00.000Z","updated":"2020-02-29T03:09:41.839Z","comments":true,"path":"2020/02/29/事务/","link":"","permalink":"http://yoursite.com/2020/02/29/%E4%BA%8B%E5%8A%A1/","excerpt":"","text":"事务逻辑上的一组操作，要么同时成功，要么同时失败 事务的四大特性（ACID） 原子性一个事务是一个不可分割的整体，要么同时成功，要么同时失败 一致性一个事务执行之前和之后数据也应该是完整的 隔离性多个并发的事务应该是独立的，互不影响的。 持久性事务一旦提交，事务对数据库的影响就真实发生了，无论做任何操作，这种影响无法被撤销。隔离性脏读：一个用户读取到另一个用户还未提交的数据，产生脏读不可重复读一个事务可以读取另一个事务已经提交的数据虚读（幻读）一个事务可以读取到另一个事务对整表数据增删改数据库事务的隔离级别read uncommited; - 读未提交 会出现脏读，不可重复读，虚读的问题read commited - 读已提交 可以防止脏读，repeatable red 可重复读 可以防止脏读和不可重复读serializable - 序列化 都可以避免，但效率低下锁机制 共享锁 共享锁和共享锁可以共存 共享锁和排他锁不能共存 排它锁 排他锁和共享锁不能共存 排他锁和排他锁不能共存","categories":[{"name":"Web","slug":"Web","permalink":"http://yoursite.com/categories/Web/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"JavaSE04","slug":"JavaSE04","date":"2020-02-28T14:59:51.201Z","updated":"2020-02-28T14:59:51.201Z","comments":true,"path":"2020/02/28/JavaSE04/","link":"","permalink":"http://yoursite.com/2020/02/28/JavaSE04/","excerpt":"","text":"title: 线程date: 2020-02-28tags: - 分享 - 导航categories: Java基础– 程序为完成特定的任务，用某种语言编写一组指令的集合。一段静态的代码，静态对象 进程正在运行的程序 线程进程可以进一步细化为线程，是一个程序内部的执行路径 -","categories":[],"tags":[]},{"title":"JavaSE03","slug":"JavaSE03","date":"2020-02-28T13:02:22.563Z","updated":"2020-02-28T13:02:22.563Z","comments":true,"path":"2020/02/28/JavaSE03/","link":"","permalink":"http://yoursite.com/2020/02/28/JavaSE03/","excerpt":"","text":"title: Collection(集合)date: 2020-02-28tags: - 分享 - 导航categories: Java基础– 泛型泛型的本质是数据类型的参数化，在编译器阶段处理 -","categories":[],"tags":[]},{"title":"JVM","slug":"JVM","date":"2020-02-27T16:00:00.000Z","updated":"2020-05-06T15:08:05.328Z","comments":true,"path":"2020/02/28/JVM/","link":"","permalink":"http://yoursite.com/2020/02/28/JVM/","excerpt":"","text":"JVM体系结构概览 类装载器（ClassLoader）负责加载class文件，class文件在文件开头有特定的文件标示，将class文件字节码内容加载到内存中，并将这些内容转换成方法区中的运行时数据结构并且ClassLoader只负责class文件的加载，至于他是否可以运行，则由Execution Engine决定虚拟机自带的加载器 启动类加载器/根加载器（Bootstrap）C++ 如果是系统原生的类调用getclass.classloader调用的是原生的bootstrap根加载器 扩展类加载器（Extension）Java 应用程序类加载器（AppClassLoader）Java也叫系统类加载器，加载当前应用的classpath的所有类 如果是自定义的类获得的加载器就是应用程序类加载器 用户自定义加载器Java.lang.ClassLoader的字类，用户可以定制类的加载方式 双亲委派当一个类收到类加载请求，他首先不回自己去尝试这个类，而是把这个请求委派给父类去完成，每一个层次的类加载器都是如此，因此所有的加载请求都应该传送到启动类加载其中，只有当父类加载器反馈自己无法完成这个请求的时候（在它的加载路径下没有找到所需加载的class），字类加载器才会尝试自己去加载 采用双亲委派的一个好处是比如加载位于rt.jar包中的类java.lang.object,不管是哪个加载器加载这个类，最终都委托给最顶层的启动类加载器进行加载，这样就保证了使用不同的类加载器最终得到的都是同样一个object对象 Execution Engine （执行引擎）作用： 负责解释命令，提交操作系统执行 Native Interface（本地接口） 本地接口的作用是融合不同的编程语言为Java所用，他的初衷是融合C/C++程序，Java诞生的时候是C/C++横行的时候，要想立足，必须要调用C/C++程序，于是就在内存中专门开辟了一块区域处理标记为native的代码，他的具体做法是在Native Method Stack 中登记native方法，在Execution Engine 执行时加载native libraries 目前该方法使用的越来越少了，除非是与硬件有关的应用，比如荣国Java程序驱动打印机或者Java系统管理生产设备，在企业级应用中已经比较少见。因为现在的异构领域间的通信很发达，比如可以使用socket通信，也可以使用Web Service 等等。 Native Method Stack（本地方法栈） 他的具体做法是Native Method Stack 中登记native 方法，在Execution Engine 执行时加载本地方法库 PC寄存器 每一个线程都有一个程序计数器，是线程私有的，就是一个指针，指向方法区中的方法字节码（用来存储指向下一条指令的地址，也即将要执行的指令代码),由执行引擎读取下一条指令，是一个非常小的内存空间，几乎可以忽略不计 这一块内存区域很小，它是当前线程所执行的字节码的行号指示器，字节码解释器通过改变这个计数器的值来选取下一条需要执行的字节码指令 如果执行的是一个native方法，那么这个计数器是空的。 用以完成分支、循环、跳转、异常处理、线程恢复等基础功能。不会发生内存溢出（outofmemory=oom）错误 Method Area (方法区)供各线程共享的运行时内存区域。 它存储了每一个类的结构信息,例如运行时常量池（Runtime Constant Pool），字段和方法数据，构造函数和普通方法的字节码内容。 实例变量存在对内存中，和方法区无关 栈管运行，堆管存储 程序=算法+数据结构数据结构 队列（FIFO）先进先出 栈（stack）先进后出（子弹夹） Stack栈栈也叫栈内存，主管Java程序的运行，是在线程创建时创建，它的生命周期是跟随线程的生命周期，线程结束栈内存也就释放， 对于栈来说不存在垃圾回收问题，只要线程已结束该栈就over，生命周期和线程一致，是线程私有的。8种基本类型的变量+对象的引用变量+实例方法都是在函数的栈内存中分配。 栈存储什么？ 本地变量：输入参数和输出参数以及方法内的变量 栈操作：记录出栈，入栈的操作 栈帧数据：包括类文件，方法等等。 栈+堆+方法区的交互关系 HotSpot是使用指针的方式来访问对象 Java堆中会存放访问类元数据的地址 reference存储的就直接是对象的地址 堆（heap） 一个JVM实例只存在一个对内存，堆内存的大小可以调节的。类加载器读取类文件后，需要把类、方法、常变量放到对内存中，保存所有引用类型的真实信息，以方便执行器执行，堆内存分为三个部分： Young Generation Space 新生区 Young/New Tenure generation space 老年代 old/Tenure Permanent Space 永久区 Perm 方法区实际而言，方法区（Method Area）和堆一样，是各个线程共享的内存区域，它用于存储虚拟机加载的：类信息+普通常量+静态常量+编译器编译后的代码等等，虽然JVM规范将方法区描述为堆的一个逻辑部分，但它却还有一个别名叫做Non-HeapZ（非堆），目的就是要和堆分开。 对于HotSpot虚拟机，很多开发者习惯将方法区称之为“永久代”但严格本质上说两者不同，或者说永久代来实现方法区而已，，jdk1.7的版本中，已经将原来放在永久代的字符串常量池移走 元空间在Java8中，永久代已经被移除，被一个称为元空间的区域所取代。元空间的本质和永久代类似。 元空间和永久代之间的区别：永久代使用的JVM的对内存，但是Java8以后的元空间并不在虚拟机中而是使用本机物理内存。因此，默认情况下，元空间的大小仅受本地内存限制。类的元数据放入native memory ，字符串池和类的静态变量放入Java堆中，这样可以加载多少类的元数据就不再由MaxPermSize控制，而由系统的实际可用空间来控制。","categories":[{"name":"面试","slug":"面试","permalink":"http://yoursite.com/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"JavaSE01","slug":"JavaSE01","date":"2020-02-25T16:00:00.000Z","updated":"2020-02-26T13:35:16.531Z","comments":true,"path":"2020/02/26/JavaSE01/","link":"","permalink":"http://yoursite.com/2020/02/26/JavaSE01/","excerpt":"","text":"1. 简单说一下什么是跨平台由于各种系统所支持的指令集不是完全一致，所以在操作系统上加个虚拟机可以来提供统一的接口，屏蔽系统之间的差异。2.Java有几种基本数据类型8种基本数据类型数据类型 字节 默认值 byte 1 0 short 2 0 int 4 0 long 8 0 double 4 0.0d float 8 0.0f char 2 ‘\\u0000’ boolean 1 false 3.面向对象的特征 封装 把描述一个对象的属性和行为封装在一个模块中，也就是封装到一个类中，用变量来定义对象的属性，用方法来定义对象的行为，方法可以直接访问同一对象的属性 继承 发生在父子类中，子类可以继承父类的特征和行为，子类继承父类的非private修饰的方法，子类也可以对父类的方法进行重写，缺点是增加了代码之间的耦合性 多态 不同子类型的对象对同一消息作出不同的响应分为编译时多态和运行时多态编译时多态：常见的方法的重载，也就是一个类中存在多个方法名相同，而参数列表不同的方法运行时多态 方法的重写实现的是运行时多态，重写指的是子类重写父类的方法，重写的原则是，子类和父类的方法签名相同，也就是子类方法的返回值类型，方法的参数列表，以及方法名要与父类方法相同，子类抛出的异常小于等于父类抛出的异常，子类方法的访问修饰权限要&gt;=父类的访问修饰符权限，向上造型：用父类型引用子类型对象，这样调用同样的方法就会根据子类对象的不同而表现出不一样的行为。 抽象抽象是将一类对象的共同特征总结出来构造类的过程，包括数据抽象和行为抽象两方面。抽象只关注对象有那些属性和行为，并不关注这些行为的细节是什么？ 4.为什么要有包装类型让基本数据类型也具有对象的特征。jdk1.5的新的性自动装箱：Integer i = 1 ; 把基本数据类型转化为包装类，底层用的Integer.valueof(1)自动拆箱：int i =new Integer(6) 把包装类型转化为基本数据类型 i.intValue(); 二者的区别： 声明方式不同：基本类型不使用new关键字，而包装类型需要使用new关键字在堆中分配内存 存储方式及位置不同：基本类型是直接将变量存储在栈中，而包装类型是将对象放在堆中，然后通过引用来使用； 初始值不同：基本类型的初始值为0，boolean为false，而包装类型的初始值为null 使用方式不同：基本类型直接赋值直接使用就好，而包装类型在集合如Collection、Map时会使用到。","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"Interface(接口）","slug":"JavaSE02","date":"2020-02-25T16:00:00.000Z","updated":"2020-02-28T12:27:41.878Z","comments":true,"path":"2020/02/26/JavaSE02/","link":"","permalink":"http://yoursite.com/2020/02/26/JavaSE02/","excerpt":"","text":"抽象类和抽象方法抽象类：如果一个类中有一个或多个抽象方法，那么这个类必须是抽象类。 抽象类不能创建对象，因为抽象类创建对象没有意义,但可以创建抽象类的子类对象 子类继承抽象类需要重写抽象类中的抽象方法，如果不重写这个类必须是抽象类，否则会产生编译时异常 抽象类中可以没有抽象方法, 可以向上造型，引用父类类型创建子类对象，调用子类方法 abstract和static不能同时使用###抽象方法：没有方法体的方法，用abstract关键字修饰接口 interface关键字修饰，产生一个完全的抽象类，接口中的方法都是抽象方法，接口也不能被实例化 在接口中定义的方法必须被public修饰，而抽象类中的方法可以被其他访问权限修饰符修饰，如果不写java默认会用public修饰 接口中的变量默认被static和final关键字修饰，可以直接被接口名调用，不能被修改","categories":[{"name":"Java基础","slug":"Java基础","permalink":"http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"LinuxDay03","slug":"LinuxDay03","date":"2020-02-25T10:45:09.000Z","updated":"2020-02-25T13:53:45.484Z","comments":true,"path":"2020/02/25/LinuxDay03/","link":"","permalink":"http://yoursite.com/2020/02/25/LinuxDay03/","excerpt":"","text":"网络地址配置网络的时候需要配置下面的信息 IP地址：PC在网络中的通信地址 子网掩码: 子网掩码用于划分网络，将一个IP地址中的网络位和主机位进行划分。是一个32位的地址。 网关： 网络的关口，用于数据转发，通常理解为路由器的地址，大部分硬件厂家默认地址，192.168.0.1 |192.168.1.1 DNS: 用于解析域名的作用，Domain Name System 域名解析系统DNS静态优点：可以是我们PC/服务器有一个更快的解析速度。维护方式手动配置hosts文件，配置ip和域名的映射缺点：hosts一般都是为本机系统所有，维护上万台的服务器的集群很困难动态优点：只需要给服务器指明DNS服务器地址即可，无需手动配置hosts文件缺点：有一定的响应时间（延迟），若DNS服务器党机，那么此域名就没有办法访问桥接模式和NAT模式的优缺点桥接模式：优点：同一个局域网中任意一台物理机想要访问虚拟机时，只要拥有账户和密码，就可以直接进行通信缺点：如果宿主主机没有连接网络，那么虚拟机也就不存在与该真实网络的环境中，换句话，虚拟机使用桥接模式的时候，它的网络依赖于宿主的网络的环境。NAT优点：可以无视物理机网络环境，即便时物理机没有网络，也不影响本机和虚拟机进行通信。因为虚拟机真正通信网卡是VMNet8提供（网络环境）缺点：其他物理机想要访问NAT模式下的虚拟机时，比较麻烦Linux的网络知识通过域名查看IP host www.baidu.com远程拷贝本机与Linux之间的文件传输，在本机的控制台上,本机的文件的路径为绝对路径例如：本机传送文件到Linux中 命令：scp /Users/yuxiangrui/Desktop/1.jpg root@192.168.144.134:/home/ 本机传送文件夹，命令：scp -r /Users/yuxiangrui/Desktop/1.jpg root@192.168.143.134 linux中传送文件到本机 scp root@92.168.143.134：/home/1.jpg /Users/yuxiangrui/Desktop/ linux中传送文件夹到本机 scp -r root@92.168.143.134：/home/1.jpg /Users/yuxiangrui/Desktop/登陆远程服务器 ssh root@192.168.143.134SSH免密登陆Linux免密登陆使用的RSA算法 RSA本身是一种非对称加密算法，会生成公钥和私钥生成密钥 ssh-keygen这样.ssh目录下就会有公钥和私钥的文件 将公钥注册到其他服务器上ssh-copy-id {UserName}@IPwget wget http://www.baidu.com查看进程ps -aux查询特定的进程ps -aux｜grep sshd","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"LinuxDay02","slug":"LinuxDay02","date":"2020-02-24T11:07:09.000Z","updated":"2020-02-24T14:06:52.707Z","comments":true,"path":"2020/02/24/LinuxDay02/","link":"","permalink":"http://yoursite.com/2020/02/24/LinuxDay02/","excerpt":"","text":"用户、用户组概念Linux系统是一个多用户多任务的分时操作系统，任何一个要使用系统资源的用户都必须首先向系统管理员申请账号，然后以这个账号的身份进入系统。用户在登陆时建入正确的用户名和口令后，就能够进入系统和自己的主目录。用户管理&gt; &gt;&gt; 用法ls -l &gt; a.txt 把显示的内容覆盖写入a.txt，如果没有这个文件则创建此文件 追加，例如ls -l &gt;&gt; a.txt把当前显示的内容追加到a.txt的末尾。 echo输出打印 echo “hello” head显示文件的前10行head -n 10 /etc/peofile tailtail -n 5 /etc/profile 查看文件的后5行tail -f 实时监控文件是否变化 chmod 修改用户的权限4:可读 2:可写 1:可执行0:没有任何权限 设定用户权限可读可写可执行，用户组全前线可读可执行 其他没有权限 chmod 750 文件or文件夹 usermod 将用户加入用户组usermod -g A zhangsan 将张三加入A组中 chown 修改文件目录属主chown root：big A 将A的属主改成big","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"YARN","slug":"YARN","date":"2020-02-20T16:00:00.000Z","updated":"2020-05-15T15:08:19.549Z","comments":true,"path":"2020/02/21/YARN/","link":"","permalink":"http://yoursite.com/2020/02/21/YARN/","excerpt":"","text":"概述 YARN（Yet Another Resource Negotiator，迄今另一个资源调度器）是Hadoop提供的一套用于进行资源管理和任务调度的框架 YARN是Hadoop2.0中最重要的特性之一 YARN产生的原因： 内因：在Hadoop1.0中，JobTracker既要接收任务还得分配任务并且还需要监控这些任务是否执行完成，这就使得JobTracker成为了MapReduce的性能瓶颈。在官方文档中给定，Hadoop1.0中，TaskTracker的个数不能超过4000个 外果：随着数据的发展，产生了越来越多的计算框架(Pig、Storm、Spark等)，在Hadoop1.0中，这些计算框架都是直接抢占Hadoop集群的资源从而容易导致资源冲突，所以在Hadoop2.0中提供了一个统一的资源调度器 YARN的结构： ResourceManager：主节点。作用是资源管理 ResourceScheduler：资源分配 ApplicationsManager：任务调度以及管理ApplicationMaster NodeManager：从节点。作用是执行任务 YARN的Job执行流程 客户端将job任务提交给ResourceManager 当ResourceManager收到任务之后，会先将这个任务临时缓存，等待NodeManager的心跳 当ResourceManager收到NodeManager心跳之后，ResourceManager会进行心跳响应，会将这个job任务分配给这个NodeManager，同时在心跳响应中要求NodeManager来开启ApplicationMaster NodeManager收到心跳响应后，会在本节点内部开启ApplicationMaster，同时将job任务交给ApplicationMaster来进行处理 ApplicationMaster收到ApplicationMaster的请求之后，会将这个请求所需要的资源封装成Container对象（封装的是CPU核和内存描述）返回给ApplicationsManager，然后ApplicationsManager在将Container返回给ApplicationMaster ApplicationMaster收到资源之后，会对资源进行二次分配，将资源分配给子任务，然后将子任务分配到不同的NodeManager上执行(MapTask在分配的时候要尽量满足数据本地化策略，ReduceTask在分配的时候是分配到相对空闲的节点上)，同时ApplicationMaster会监控这些子任务的完成情况 注意问题： a. 一个job对应一个ApplicationMaster b. YARN的管理结构：ResourceManager中的ApplicationsManager管理ApplicationMaster，ApplicationMaster管理子任务 c. 默认情况下一份资源包含1个CPU核+1G内存 d. ApplicationMaster在申请资源的时候会多要，但是ResourceManager不会多给。例如一个job拆分出来4个MapTask和1个ReduceTask，那么默认情况下，考虑数据本地化策略，ApplicationMaster申请4*3+1=13份资源，但是注意ResourceManager只返回5份资源。”要的多”的目的是为了减少阻塞提高job的执行效率，”给的少”的目的为了避免资源浪费 ResourceScheduler ResourceScheduler是一个纯粹的资源调度器，并不参与具体的任务 ResourceScheduler本身被设计成一个可拔插的组件，可以根据不同的场景来设计不同的资源调度策略 FCFS（First Come First Serve） - 先来先得 - ResourceScheduler默认采用也是这种策略 优先级/权重策略 - 给每一个请求来设置优先级，优先级越高，那么越早分配资源 端任务有限策略 - 如果在能够估计任务的执行时间的情况下，且任务之间的时间差别特别的时候，可以将资源有限分配给短任务 实际开发中，资源分配调度不再是单一使用而是结合使用，如果任务执行时间相差较大，考虑短任务优先；如果任务执行时间相差不大，那么考虑优先级；如果执行时间相差不大且优先级又一致，那么考虑FCFS","categories":[{"name":"Hadoop","slug":"Hadoop","permalink":"http://yoursite.com/categories/Hadoop/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"Web总结","slug":"Web","date":"2020-02-20T16:00:00.000Z","updated":"2020-02-23T06:42:58.702Z","comments":true,"path":"2020/02/21/Web/","link":"","permalink":"http://yoursite.com/2020/02/21/Web/","excerpt":"","text":"AJAX 背景 web开发是通过HTTP协议实现的 Http协议的特点： a. 基于请求相应模型 b. 一次请求对应一次响应 c. 请求只能由客户端发出，服务器端只能被动等待请求作出响应基于这样的特点，网站只能实现同步请求，全部刷新的效果，但实际生活中需要在不影响正在展示的内容的情况下还要保证服务器不停的向客户端发送最新的数据，这只靠HTTP协议无法完成，所以AJAX的技术应运而生。 特点 AJAX技术实现：异步请求，局部刷新。AJAX不是一项新的技术，而是多个已有的技术整合实现的，其原理就是在页面的js中通过代码在不影响当前页面的情况下向服务器发送请求，得到服务器传递的最新数据后再通过js更新当前页面的局部内容。 AJAX仍然是基于HTTP协议，没有违背HTTP协议的原理 jquery方式实现AJAX a. 通用方式12345678910function getTime() &#123; $.ajax(&#123; type:&quot;post&quot;, url:&quot;&#x2F;ExecDemo01_war_exploded&#x2F;TimeServlet&quot;, data:&quot;&quot;, success: function (msg) &#123; $(&quot;#d01&quot;).text(msg) &#125; &#125;) &#125; b. get方式1234567&#x2F;*get方式*&#x2F; function getTime1()&#123; $.get(&quot;&#x2F;ExecDemo01_war_exploded&#x2F;TimeServlet&quot;,function (msg) &#123; $(&quot;#d01&quot;).text(msg) &#125;) &#125; c. post方式1234567&#x2F;*post方式*&#x2F; function getTime2()&#123; $.post(&quot;&#x2F;ExecDemo01_war_exploded&#x2F;TimeServlet&quot;,function (msg) &#123; $(&quot;#d01&quot;).text(msg) &#125;) &#125; d. getJSON方式123456&#x2F;*getJSON方式*&#x2F; function getTime3()&#123; $.getJSON(&quot;&#x2F;ExecDemo01_war_exploded&#x2F;TimeServlet&quot;,function (obj) &#123; $(&quot;#d01&quot;).text(obj.time) &#125;) &#125; HTTP协议 概述 基于TCP协议的应用层协议。 是客户端的和服务器通信的常用协议。 规则 a. 基本规则 基于请求响应模型 一次请求对应一次响应 请求只能由客户端发出，服务器端被动接受请求作出响应 请求： 一个请求行 GET /index.html HTTP/1.1 请求方式 请求方式（8种），最常见的是GET和POST请求 8种请求方式：opions get post put head delete trace connect GET请求：请求参数在地址栏上拼接，相对安全性较差传输的数据量有限 POST请求：请求参数在实体内容中传输，相对安全性较高，传输的数据量理论上没有限制。请求的资源地址 使用的协议版本 1.0 每次请求都要重新创建连接，完成请求响应断开连接，效率低 1.1 在一次请求响应结束后，可以选择保持底层连接，如果后续请求到来可以复用底层连接，效率高请求头： Accept：浏览器可以接受的MIME类型 Accept-Charset：浏览器可接受的字符集 Accept-Encoding：浏览器能够进行解码的数据编码方式，比如说gzip，Servlet能够向支持的gzip的浏览器返回经gzip编码的HTML页面。许多情形下这可以减少5到10被的下&gt;载的时间Accept-Language:浏览器可以接受的语言种类 Connection：表示是否需要持久连接，HTTP默认KeepAlive Content-Length：表示请求消息正文的长度 Cookie：存放缓存信息 Host：初始URL中的主机和端口 If-Modified—Since：只有当所请求的内容在指定的日期之后又经过修改才返回他&gt;，否则返回304NotModified Pragma：指定“no-cache”值表示服务器必须返回一个刷新后的文档，即使它是代理服务器而且已经有了页面的本地拷贝 Referer：包含一个URL，用户从该URL代表的页面出发访问当前请求的页面。 User-Agent：浏览器类型，如果Servlet返回的内容与浏览器类型有关则该值非常有用； 一个空白行 存在的目的是将请求头和实体内容分割开实体内容 要向服务器传递数据 GET提交时，实体内容为空，post提交时，请求参数在实体内容中传输HTTP协议只支持ISO8859-1（Latin1）编码，此编码集只有英文数字常见的符号，所以必须要通过URL解决乱码问题 c.响应 一个状态行 HTTP/1.1200 ok 使用的协议 状态码 1XX 临时响应 2XX 成功 200成功 3XX 重定向 302重定向304/307使用缓存 4XX 请求错误 找不到资源 5XX 服务器错误 500服务器错误","categories":[{"name":"Web","slug":"Web","permalink":"http://yoursite.com/categories/Web/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"sql优化","slug":"sql-optimize","date":"2020-02-20T16:00:00.000Z","updated":"2020-04-24T14:49:50.582Z","comments":true,"path":"2020/02/21/sql-optimize/","link":"","permalink":"http://yoursite.com/2020/02/21/sql-optimize/","excerpt":"","text":"mysql的数据引擎主流数据引擎（MyISAM、InnoDB）的对比 性能下降SQL慢分析 导致执行时间过长 导致等待时间过长原因 sql语句写的不合理 没有建索引或者索引已经失效 出现大量的关联查询和子查询，也就是很多join字段 服务器调优及各个参数设置(缓冲\\线程数等) sql执行顺序 索引索引（Index）是帮助MySQL高效获取数据排好序的数据结构，本质就是：索引是数据结构，目的在于提高查询效率，类比于字典。本质：排好序的快速查找数据结构，BTree结构 一般来说索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储在磁盘中，我们平常说的索引，如果没有指明一般都是指B树（多路搜索树，并不一定是二叉的）结构组织的索引。其中聚集索引，次要索引，覆盖索引，唯一索引默认都是使用B+树索引。当然，除了B+树这种类型的索引之外，还有hash索引（hash index）等 优势 提高数据检索的效率，降低数据库的IO成本 通过索引列对数据进行了排序，降低数据排序的成本，降低了cpu的消耗 劣势 实际也是一张表，保存了主键与索引的字段，也会占据磁盘的空间 降低更新表的速度，执行update的操作的时候，也需要更新索引 索引的分类 单指索引：即一个索引只包含单个列，一个表可以有多个单列索引 唯一索引：索引列的值必须唯一，但允许有空值 复合索引：即一个索引包含了多个列 基本语法 索引的选择 主键自动建立唯一索引 频繁作为查询条件的字段应该创建索引 查询中与其他表关联的字段，处理关系建立索引 频繁更新的字段不适合创建索引 因为每次更新不单单是更新了记录还会更新索引 where条件里用不到的字段不创建索引 单键/复合索引的选择问题？在高并发的下倾向于创建复合索引 查询中排序的字段，排序字段若通过索引去访问将大大提高排序的速度 查询中统计或者分组字段 explain 分析sql语句字段 id - id如果相同，可以认为是一组，从上往下顺序执行，在所有组中id值越大，优先级越高，越先执行。决定表的读取顺序和加载顺序 select_type table 显示这一行的数据是关于哪张表的 type 显示查询使用了何种类型，从最好到最差依次是system&gt;const&gt;eq_ref&gt;ref&gt;range&gt;index&gt;ALL possible_keys 显示可能应用在这张表中的索引,一个或多个。查询涉及的字段上若存在索引，则该索引将被列出，但不一定被查询实际使用 key 实际使用的索引。如果为null则没有使用索引 ;查询中若使用了覆盖索引，则索引和查询的select字段重叠 key_len 表示索引中使用的字节数，可通过该列计算查询中使用的索引的长度。在不损失精确性的情况下，长度越短越好 ref 显示索引那一列被使用了，如果可能的话，是一个常数。那些列或常量被用于查找索引列上的值 rows 根据表统计信息及索引选用情况，大致估算出找到所需的记录所需要读取的行数 Extra 包含不适合在其他列中显示但十分重要的额外信息 1.Using filesort 说明mysql会对数据使用一个外部的索引排序，而不是按照表内的索引顺序进行读取。MySQL中无法利用索引完成排序操作成为“文件排序 2.Using temporary 使用了临时表保存中间结果，MySQL在对查询结果排序时使用临时表。常见于排序order by 和分组查询 group by 3.USING index 表示相应的select操作中使用了覆盖索引（Coveing Index）,避免访问了表的数据行，效率不错！如果同时出现using where，表明索引被用来执行索引键值的查找；如果没有同时出现using where，表面索引用来读取数据而非执行查找动作。 ** 左连接加有表，右连接加左表**","categories":[{"name":"Mysql","slug":"Mysql","permalink":"http://yoursite.com/categories/Mysql/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"LinuxDay01","slug":"LinuxDay01","date":"2020-02-20T14:08:06.604Z","updated":"2020-02-20T14:08:06.604Z","comments":true,"path":"2020/02/20/LinuxDay01/","link":"","permalink":"http://yoursite.com/2020/02/20/LinuxDay01/","excerpt":"","text":"Linux简介Linux是一款操作系统，它是由unix演变而来的，创始人是芬兰当时的大学生肯.汤普森写的一款操作系统，Linux具有开源免费的特点，现在广泛运用于服务器端。 Linux的优势相较于Windows而言Linux更加的安全，更加的高效,对网络有良好的支持性，稳定，低成本，可以很好的解决多并发的问题。 2.创建虚拟机（空间）#### 配置网络连接时有三种形式： 1.桥连接：Linux可以和任意系统进行通信，但是可能造成ip冲突的问题 2.NAT：网络地址的转化方式，linux可以访问外网但外网不可以直接访问Linux 3.主机模式：Linux是独立的主机不可以访问外网 3.Linux的目录结构Linux的文件是采用级层式的树状目录，在此结构中的最上层是根目录“/”,然后在&gt;此目录下再创建其他的目录 $\\color{#FF0000}{在linux的世界里，一切皆文件}$常见的命令","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"MyBatisDay01","slug":"MyBatisDay01","date":"2020-02-06T11:29:59.000Z","updated":"2020-02-19T13:49:44.974Z","comments":true,"path":"2020/02/06/MyBatisDay01/","link":"","permalink":"http://yoursite.com/2020/02/06/MyBatisDay01/","excerpt":"","text":"mybatis的概述mybatis是一个数据访问层的框架，用java编写的。 它封装了jdbc操作的很多细节，使开发者只需要关注sql语句本身，而无需关注注册驱动，创建连接等复杂过程，是一种半自动对象表的Dao层框架 它使用了ORM思想实现了结果集的封装。优点可以手写sql灵活实现数据访问，自动封装数据，减少代码的冗余ORM思想：Object Relational Mapping （对象关系映射）","categories":[{"name":"框架","slug":"框架","permalink":"http://yoursite.com/categories/%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"工厂模式","slug":"工厂模式","date":"2020-02-01T12:30:09.000Z","updated":"2020-02-03T08:39:22.722Z","comments":true,"path":"2020/02/01/工厂模式/","link":"","permalink":"http://yoursite.com/2020/02/01/%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"简单工厂模式定义：定义了一个创建对象的类，由这个类来封装实例化对象的行为（代码）简单工厂模式是由一个工厂对象决定创建出哪一种产品类的实例","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"Design Patterns","slug":"Design-Patterns","date":"2020-01-31T04:53:50.000Z","updated":"2020-02-01T10:13:04.969Z","comments":true,"path":"2020/01/31/Design-Patterns/","link":"","permalink":"http://yoursite.com/2020/01/31/Design-Patterns/","excerpt":"","text":"设计模式的类型设计模式分为三种类型，共23种 创建型模式：单例模式，抽象工厂模式，工厂模式，原型模式，建造者模式。 结构型模式：适配器模式，装饰者模式，桥接模式，组合模式，外观模式，享元模式，代理模式 行为型模式：模版方法模式，命令模式，访问者模式，迭代器模式，观察者模式，备忘录模式，解释器模式（interpreter模式），状态模式，策略模式，职责链模式（责任链）模式。设计模式常用的七大原则1.单一职责原则 降低类的复杂度，一个类只负责一项职责 便于后期的代码的维护，提高代码的刻度性 降低后期因变更而引起的风险 通常情况下我们应该遵循单一职责原则，但在一个类中方法较少的时候，我们可以在方法的级别保持单一职责原则即可 2. 接口隔离原则 一个类通过接口依赖另一个类，该接口应该是最小接口，也就是通过接口的隔离保证类中不需要接口中多余的方法，如果接口中有用不到的方法，就把这个接口中拆分成多个小接口，避免造成浪费。 3. 依赖倒转原则（Dependence Inversion Principle）案例12345678910111213141516171819202122232425262728293031323334public class DependcyIversion &#123; public static void main(String[] args) &#123; Person person = new Person(); person.receive(new Email()); person.receive(new Weixin()); &#125;&#125;interface Ireceiver &#123; String getInfo();&#125;class Email implements Ireceiver &#123; @Override public String getInfo() &#123; return \"hello world\"; &#125;&#125;class Weixin implements Ireceiver &#123; @Override public String getInfo() &#123; return \"one world,one dream\"; &#125;&#125;class Person &#123; public void receive(Ireceiver ireceiver) &#123; System.out.println(ireceiver.getInfo()); &#125;&#125;//总结：通过类与接口发生依赖，与子实现类没有关系，这样便于后期的修改，这就是面向接口编程的思想 低层模块最好是抽象类或者接口，这样程序会更稳定 变量声明类型尽量是抽象类或接口，这样我们的变量引用和实际对象间，就存在一个缓冲层，利用程序扩展和优化 继承时遵循里氏替换原则 4. 里氏替换原则 继承增加了程序的耦合性 子类尽量不要重写父类的方法，如果需要重写，就通过依赖，组合，聚合的方法，提升一个基类，让原有的父类和子类继承这个基类案例1234567891011121314151617181920212223242526272829303132333435363738394041424344public class Liskov &#123; public static void main(String[] args) &#123; A a = new A(); System.out.println(\"11-3=\" + a.fun1(11, 3)); System.out.println(\"1-8=\" + a.fun1(1, 8)); System.out.println(\"-----------\"); B b = new B(); System.out.println(\"11+3=\" + b.fun1(11, 3)); System.out.println(\"1+8=\" + b.fun1(1, 8)); System.out.println(\"11+3+9=\" + b.fun2(11, 3)); System.out.println(\"11-3=\" + b.fun3(11, 3)); &#125;&#125;class Base &#123;&#125;class A extends Base &#123; public int fun1(int num1, int num2) &#123; return num1 - num2; &#125;&#125;//B类继承了A//增加了一个新功能：完成两个数相加，然后和9求和class B extends Base &#123; //通过依赖的方式与类A发生关系 private A a = new A(); public int fun1(int a, int b) &#123; return a + b; &#125; public int fun2(int a, int b) &#123; return fun1(a, b) + 9; &#125; public int fun3(int a, int b) &#123; return this.a.fun1(a, b); &#125;&#125; 5. 开闭原则 ocp(open closed principle) 是编程中最基础，最重要的设计原则6. 迪米特法则（最少知道原则） 降低类之间的耦合，对自己依赖的类知道的越少越好 降低类间（对象间）耦合关系，并不是要求完全没有依赖关系 直接朋友，出现在成员变量，方法参数，方法返回值，如果出现在局部变量中则是陌生朋友7. 合成复用原则 原则是尽量使用合成/聚合的方式，而不是使用继承123456789101112131415161718192021222324252627282930313233343536373839404142public class Demo1 &#123; @Test public void test()&#123; Dog d = new Dog(); d.setAnimal(); &#125;&#125;class Animal &#123; void run()&#123; System.out.println(\"正在跑\"); &#125; void sleep()&#123; System.out.println(\"正在睡\"); &#125; void eat()&#123; System.out.println(\"正在吃\"); &#125;&#125;//方式一依赖/*class Dog &#123; public void action(Animal animal)&#123; animal.run(); animal.eat(); &#125;&#125;*///方式二聚合/*class Dog &#123; private Animal animal; public void setAnimal(Animal animal)&#123; animal.eat(); &#125;&#125;*///方式三：组合class Dog &#123; private Animal animal = new Animal(); public void setAnimal() &#123; animal.eat(); &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"SpringMVCDay01","slug":"MyBatisday02","date":"2020-01-29T04:22:20.000Z","updated":"2020-02-17T13:52:02.540Z","comments":true,"path":"2020/01/29/MyBatisday02/","link":"","permalink":"http://yoursite.com/2020/01/29/MyBatisday02/","excerpt":"","text":"1.SpringMVC概述SpringMVC是一个WEB层，控制层框架，主要用来负责与客户端交互，业务逻辑的调用。SpringMVC是Spring家族的一大组件，Spring整合SpringMVC可以做到无缝集成，特点：简单易用性能佳2.SpringMVC相对与Servlet的优势a. Servlet的开发配置相对麻烦，servlet特别多的时候web.xml文件将会非常的臃肿b. 每个Servlet都只能处理一个功能，如果需要多个功能就需要开发多个servlet，项目中存在大量的servlet显得臃肿。c. 获取请求参数进行类型转换封装数据到bean的过程比较繁琐。d. 其他开发中不方便的地方，例如乱码问题，数据格式处理，表单检索 spring MVC详解 SpringMVC的组件a. 前端控制器（DispatcherServlet） 本质上是一个servlet，相当于一个中转站，所有的访问都会走到这个servlet中，再根据配置进行中转到相对应的handler中进行处理，获取数据和视图b. 处理器映射器（HandlerMapping） 本质上就是一段映射关系，将将访问路径和对应的Handler存储为映射关系，在需要时供前端控制器查阅。c. 处理器适配器（HandlerAdapter） 本质上就是一个适配器，可以根据要求找到对应的handler来运行。前端控制器找到处理器映射器找到对应的handler信息之后，将请求响应和对应的handler信息交由处理器适配器来处理，处理器适配器找到真正的handler执行后，将结果也就是model and view返回给前端控制器。d. 视图解析器（ViewResolver） 本质上是一种映射关系，可以将视图名称映射到真正的视图地址。前端控制器调用处理器适配完成后的到model和view，将view信息传给视图解析器得到真正的view。e. 视图（view） 本质上就是将handler处理器中返回的model数据嵌入到视图解析后得到的jsp页面中，向客户端作出响应生成SpringMvc的核心配置文件1234567891011121314&lt;!--配置前端控制器--&gt; &lt;servlet&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;!--手动配置核心文件的位置--&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:/springmvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;url-pattern&gt;*.action&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; 1234567&lt;!--配置处理器映射器中的路径和处理器的映射关系--&gt; &lt;bean name=\"/hello.action\" class=\"cn.tedu.web.Hello\"&gt;&lt;/bean&gt; &lt;!--配置视图解析器--&gt; &lt;bean class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\"&gt; &lt;property name=\"prefix\" value=\"/WEB-INF/jsp/\"&gt;&lt;/property&gt; &lt;property name=\"suffix\" value=\".jsp\"&gt;&lt;/property&gt; &lt;/bean&gt; 123456789101112131415//创建类实现controller接口public class Hello implements Controller &#123; @Override public ModelAndView handleRequest(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse) throws Exception &#123; //创建modelAndView ModelAndView modelAndView = new ModelAndView(); //封装数据 modelAndView.addObject(\"msg1\",\"hello world\"); modelAndView.addObject(\"msg2\",\"hello springMVC\"); //封装视图 modelAndView.setViewName(\"hello\"); //返回modelAndView return modelAndView; &#125;&#125; SpringMVC注解方式配置 SpringMVC支持使用注解方式配置，比配置文件更加的灵活易用，是目前的主流配置方式1234567&lt;context:component-scan base-package=\"cn.tedu\"&gt;&lt;/context:component-scan&gt; &lt;mvc:annotation-driven&gt;&lt;/mvc:annotation-driven&gt; &lt;!--配置视图解析器中视图名称和真正页面的映射关系--&gt; &lt;bean class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\"&gt; &lt;property name=\"prefix\" value=\"/WEB-INF/jsp/\"&gt;&lt;/property&gt; &lt;property name=\"suffix\" value=\".jsp\"&gt;&lt;/property&gt; &lt;/bean&gt; 1234567891011121314&lt;!--配置前端控制器--&gt; &lt;servlet&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;!--手动配置核心文件的位置--&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:springmvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;url-pattern&gt;*.action&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; 123456789@RequestMapping(\"/my01\")@Controllerpublic class Controller01 &#123; @RequestMapping(\"/test01.action\") public String test01(Model model)&#123; model.addAttribute(\"msg\",\"one word one dream\"); return \"test01\"; &#125;&#125; springMVC注解工作原理 当服务器启动时，会先加载web.xml文件，之后通过引入核心配置文件来加载SpringMVC.xml 当解析到包扫描时，扫描指定的包，并将含有@Conteoller注解的类解析为处理器 如果配置过mvc:annotation-driven就会解析Spring-MVC注解 解析@requestMapping（value=“/test01.action”），将指定的地址和当前方法的映射关系保存 当客户端发出请求访问时，SpringMVC寻找改地址的映射关系，找到就执行方法，找不到抛出404 SpringMVC获取请求参数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106@Controllerpublic class MyController01 &#123; /** * 获取请求参数：日期数据处理 * 通过注册自定义类型编辑器使SpringMVC支持自定义格式请求参数的处理 * 此处，SpringMVC为Date类型已经提供了编辑器类，所以直接使用，不用自己写 * http://localhost:8080/SpringMVCDay01_04_Params_war_exploded/my01test09.jsp */ @InitBinder public void myInitBinder(ServletRequestDataBinder dataBinder)&#123; dataBinder.registerCustomEditor(Date.class,new CustomDateEditor(new SimpleDateFormat(\"yyyy-MM-dd\"),true)); &#125; @RequestMapping(\"/test09.action\") public void test09(String name, int age, Date birthday) &#123; System.out.println(name); System.out.println(age); SimpleDateFormat format = new SimpleDateFormat(\"yyyy-MM-dd\"); String format1 = format.format(birthday); System.out.println(format1); &#125; /** * 获取请求参数：中文乱码解决 * 如果服务器配置的编码是utf-8,且项目采用的也是utf-8则默认请求参数无乱码 * tomcat8默认编码为utf-8(可以更改) * tomcat7及更老的版本默认编码为iso8859-1(可以更改) * 如果遇到服务器编码和项目编码不一致时会产生乱码 * 对于POST提交 可以通过request.setCharacterEncoding(\"utf-8\")解决乱码 * 但此行代码对GET无效，GET提交的请求参数乱码，只能手工编解码来解决 * 手工编解码解决乱码的方式对POST提交也有效 * springmvc提供了过滤器CharacterEncodingFilter来帮助我们解决乱码 * 但此过滤器本质上就是request.setCharacterEncoing()所以也是只对POST有效 * ,即使配置了GET提交乱码也要手动解决 * */ @RequestMapping(\"/test08.action\") public void test08(HttpServletRequest request) throws UnsupportedEncodingException &#123; //POST提交 //request.setCharacterEncoding(\"utf-8\"); //String uname = request.getParameter(\"uname\"); //System.out.println(uname); //GET提交 //String uname = request.getParameter(\"uname\"); //byte [] data = uname.getBytes(\"iso8859-1\"); //uname = new String(data,\"utf-8\"); //System.out.println(uname); &#125; // 获取请求参数：多个同名请求参数的处理 // http://localhost:8080/SpringMVCDay01_04_Params_war_exploded/my01/test07.action?name=tt&amp;love=bb&amp;love=pp&amp;love=zz @RequestMapping(\"/test07.action\") public void test07(String name,String [] love) &#123; //封装到数组里 System.out.println(name); System.out.println(Arrays.asList(love)); &#125; //获取请求参数：自动封装请求参数到bean 对复杂类型的处理 // http://localhost:8080/SpringMVCDay01_04_Params_war_exploded/my01/test06.action?id=35&amp;name=jj&amp;age=18&amp;addr=cd&amp;dog.name=wd&amp;dog.age=1&amp;dog.cat.name=xx&amp;dog.cat.age=3 @RequestMapping(\"/test06.action\") public void test06(User user) &#123; System.out.println(user); &#125; //获取请求参数：自动封装请求参数到bean 对复杂类型的处理 // http://localhost:8080/SpringMVCDay01_04_Params_war_exploded/my01/test05.action?id=35&amp;name=jj&amp;age=18&amp;addr=cd&amp;dog.name=wd&amp;dog.age=1 @RequestMapping(\"/test05.action\") public void test05(User user) &#123; System.out.println(user); &#125; //获取请求参数：自动封装请求参数到bean // http://localhost:8080/SpringMVCDay01_04_Params_war_exploded/my01/test04.action?id=35&amp;name=jj&amp;age=18&amp;addr=cd @RequestMapping(\"/test04.action\") public void test04(User user) &#123; System.out.println(user); &#125; //获取请求参数: 通过@RequestParam指定参数赋值,解决请求参数名和方法参数不一致时赋值问题 // http://localhost:8080/SpringMVCDay01_04_Params_war_exploded/my01/test03.action?name=yasuo&amp;uage=19 @RequestMapping(\"/test03.action\") public void test03(@RequestParam(\"name\") String username, @RequestParam(\"uage\") int age) &#123; System.out.println(username + \"~~~~\" + age); &#125; //获取请求参数: 直接获取 // http://localhost:8080/SpringMVCDay01_04_Params_war_exploded/my01/test02.action?username=yasuo&amp;age=19 @RequestMapping(\"/test02.action\") public void test02(String username, int age) &#123; System.out.println(username + \"~~~~\" + age); &#125; //获取请求参数：传统方式获取 // http://localhost:8080/SpringMVCDay01_04_Params_war_exploded/my01/test01.action?username=yasuo&amp;age=19 @RequestMapping(\"/test01.action\") public void test01(HttpServletRequest request) &#123; String username = request.getParameter(\"username\"); int age = Integer.parseInt(request.getParameter(\"age\")); System.out.println(username + \"~~~~\" + age); &#125;","categories":[{"name":"框架","slug":"框架","permalink":"http://yoursite.com/categories/%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"Shell","slug":"Shell","date":"2020-01-29T04:22:20.000Z","updated":"2020-04-07T01:42:33.317Z","comments":true,"path":"2020/01/29/Shell/","link":"","permalink":"http://yoursite.com/2020/01/29/Shell/","excerpt":"","text":"Shell传递参数值123456#!&#x2F;bin&#x2F;bashecho &quot;Shell 传递参数实例&quot;echo &quot;执行的文件名：$0&quot;echo &quot;第一个参数为：$1&quot;echo &quot;第二个参数为：$2&quot;echo &quot;第三个参数为：$3&quot; Shell $*和$@的演示123456789#!&#x2F;bin&#x2F;bashecho &quot;--\\$*演示---&quot;for i in &quot;$*&quot; ;do echo $idoneecho &quot;--\\$@演示---&quot;for i in &quot;$@&quot; ;do echo $idone Shell的基础命令（find）查找文件的位置 find 位置 选项 参数 例如：按文件名进行查询 find / -name java 查找当前系统中所有的.log后缀名的文件 find / -name “*.log” 查找系统中/home目录下的非普通文件 find /home ! -type f 查找/home目录下权限为700的文件 find /home -perm 700 查找/dev目录下的块设备文件 find /dev -type b","categories":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/categories/Linux/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"Spring-Review","slug":"Spring-Review","date":"2020-01-29T04:22:20.000Z","updated":"2020-04-28T12:41:50.294Z","comments":true,"path":"2020/01/29/Spring-Review/","link":"","permalink":"http://yoursite.com/2020/01/29/Spring-Review/","excerpt":"","text":"概念 Spring是一个用于service（控制层）的框架，可以整合其他框架一起来进行工作，提高了开发效率 Spring的两大核心 IOC/DI（控制反转）/依赖注入 AOP面向切面编程 IOC/DI（控制反转）/依赖注入概念 IOC 即将创建的对象的权利和对象的生命周期的管理交给Spring容器，开发者不需要关心对象的创建和生命周期的管理，这一过程称之为控制反转 DI在Spring创建对象的过程中会根据配置对对象的属性进行设置，这一过程称之为依赖注入 IOC的实现原理在初始化一个Spring容器时，Spring会去解析xml文件，如果遇到bean标签会根据bean标签的全路径名反射创建对象，把创建的对象放入内置的map中，map中的key就是bean标签的id，value就是bean对象，之后通过getbean方法来从容器中获取对象时，会根据指定的id在内置map中寻找对应的key value，如果有就返回bean对象，如果没有就报错 结论 默认情况下，多次获取同一个id的bean，得到的将是同一个对象 不可以配置多个id相同的bean 可以配置多个id不同，class相同的bean IOC获取对象的方式 通过id获取，bean标签中id是唯一的，不存在获取bean对象抛出异常的问题，建议使用 通过class方式获取，由于不同的bean可能配置成同一个类可能导致获取时不知道要获取那个bean而抛出异常","categories":[{"name":"面试","slug":"面试","permalink":"http://yoursite.com/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"SpringCloud","slug":"SpringCloud","date":"2020-01-29T04:22:20.000Z","updated":"2020-04-07T01:43:51.737Z","comments":true,"path":"2020/01/29/SpringCloud/","link":"","permalink":"http://yoursite.com/2020/01/29/SpringCloud/","excerpt":"","text":"1.微服务1.1 单体项目：所有的功能都集中在一个web项目中。容易造成功能的强耦合1.2 由于单体项目本身具有强耦合的问题，所以需要对项目进行拆分 a.横向拆分目的：仅仅解决了一个项目的分布式，并行开发的问题，必能解决单体项目的各种问题b.纵向拆分目的：将一个运行的系统，变成多个独立运行的系统，可以解觉单体项目的问题？按照业务功能，领域不同，进行的划分，每一个划分出来的系统都可以独立运行Springcloud组件功能 springcloud概括是spring家族的一员，本身也是一批不同组件的集合框架。在微服务发展过程中，不同的公司，团队各自开发了不同的小的功能。spring创建框架时，采用一种整合的方式，将一些成熟的技术，兼容到框架来，行程一个庞大的集体 1.1组件 eureka:服务治理(动态微服务信息维护组件） ribbon：负载均衡客户端（负载均衡调用微服务组件） zuul:网关组件（每一个微服务不能提供对外暴露的接口）上述这3个组件就能开发出一个基本的微服务框架的项目 config：分布式配置中心 feign:负载均衡客户端 eureka服务治理组件 2.1eureka概括 作为服务治理组件，是每个微服务框架必备的组件。实现动态管理和维护所有微服 务信息。springcloud只能使用eureka作为服务治理组件，通过http协议的REST风 格实现通信过程（已经定义好了所有访问这个组件的接口地址，url：/hello/ /abc/cs）。","categories":[{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"SpringDay01","slug":"SpringDay01","date":"2020-01-29T04:22:20.000Z","updated":"2020-02-11T09:18:51.390Z","comments":true,"path":"2020/01/29/SpringDay01/","link":"","permalink":"http://yoursite.com/2020/01/29/SpringDay01/","excerpt":"","text":"框架（半成品软件）高度抽取可重用代码的一种设计，高度的通用性; 框架：抽取成一种高度可重用的；事务控制，强大的servlet，项目中的一些工具。 框架：多个可重用模块的集合，形成一个某个领域的整体解决方案；常见的框架SSH（老三大框架）Struts2 Spring HibernateSSM（新三大框架）SpringMVC(WEB) Spring（Service） Mybatis（Dao） Spring 框架Spring是一个service层的框架，可以整合其他框架 容器（可以管理所有的组件（类））框架；核心关注：IOC和AOPIOC：- 控制反转 AOP：- 面向切面编程案例一第一步:先导入jar包 第二步:配置Spring容器约束文件 1234567&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;bean id=\"person01\" class=\"cn.tedu.domain.Person01\"&gt;&lt;/bean&gt; &lt;bean id=\"person02\" class=\"cn.tedu.domain.Person01\"&gt;&lt;/bean&gt;&lt;/beans&gt; 案例一1234567891011121314151617181920212223242526272829303132333435363738394041/* spring 容器底层默认通过读取配置文件的类的全路径名，通过反射来创建对象* 并保存到Spring容器的Map内存中并且通过键值对的形式储存，降低了程序的耦合性* 通过getbean方法容器中找到匹配的键值对，来创建实例，多次获取同一个id的实例是同一个实例*/public class Test01 &#123; @Test public void test01()&#123; //初始化容器,ApplicationContext是接口创建实现类对象 ApplicationContext context = new ClassPathXmlApplicationContext(\"applicationContext\"); //获取bean，获得对象 Person01 p = (Person01) context.getBean(\"person01\"); //调用p的方法 p.eat(); p.sleep(); System.out.println(p); //关闭容器 ((ClassPathXmlApplicationContext) context).close(); &#125; //如果配置文件中包含两个id则会抛出异常 BeanDefinitionParsingException, // 这是因为Spring容器在存储对象时默认以键值对的形式存储的不能出现相同的key值 @Test public void test02()&#123; ApplicationContext context = new ClassPathXmlApplicationContext(\"applicationContext.xml\"); Person01 p = (Person01) context.getBean(\"person01\"); System.out.println(p); &#125; /* * Spring容器中可以包含相同的class属性，对应不同的id,且获取的是不同的实例*/ @Test public void test03()&#123; ApplicationContext context = new ClassPathXmlApplicationContext(\"applicationContext.xml\"); Person01 p1 = (Person01) context.getBean(\"person01\"); Person01 p2 = (Person01) context.getBean(\"person02\"); System.out.println(p1);//cn.tedu.domain.Person01@6e0e048a System.out.println(p2);//cn.tedu.domain.Person01@5bc79255 //关闭资源 ((ClassPathXmlApplicationContext) context).close(); &#125;&#125; 案例二：创建bean的方式123456789101112131415161718192021222324252627282930313233//创建bean /* * Spring中默认是调用类的无参构造构造,通过反射来创建bean的 * 如果没有无参构造构造就不能直接配置类的全路径名获得bean*/ //Spring创建bean，没有无参构造构造，默认创建失败public class Test04 &#123; //java.lang.NoSuchMethodException:没有默认的无参构造 @Test public void test01()&#123; ApplicationContext context = new ClassPathXmlApplicationContext(\"applicationContext.xml\"); Person01 p = (Person01) context.getBean(\"person01\"); System.out.println(p); &#125; //通过反射创建对象 //java.lang.InstantiationException: cn.tedu.domain.Person01 //直接用clz.newstance,默认调用无参构造但只有含参数构造会抛出异常 // Caused by: java.lang.NoSuchMethodException: cn.tedu.domain.Person01.&lt;init&gt;() @Test public void test02() throws Exception &#123; Class&lt;?&gt; clz = Class.forName(\"cn.tedu.domain.Person01\"); Constructor&lt;?&gt; constructor = clz.getConstructor(String.class); Person01 p = (Person01) constructor.newInstance(\"reason\"); System.out.println(p);//cn.tedu.domain.Person01@1edf1c96 &#125; //普通方法创建对象 @Test public void test03()&#123; Person01 person01 = new Person01(\"xx\"); System.out.println(person01); &#125;&#125; 案例三：利用工厂获取bean1. 静态工厂1234567&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;!--配置静态工厂--&gt; &lt;bean id=\"person01\" class=\"cn.tedu.factory.Person01StaticFactory\" factory-method=\"getInstance\"&gt;&lt;/bean&gt;&lt;/beans&gt; 123456789101112131415161718//创建静态工厂public class Person01StaticFactory &#123; //私有化构造方法，不让外界随意的创建对象 private Person01StaticFactory()&#123; &#125; //对外界提供公共的静态的getInstance的方法 public static Person01 getInstance()&#123; return new Person01(\"reason\"); &#125;&#125;//测试静态工厂 @Test public void test05()&#123; ApplicationContext context = new ClassPathXmlApplicationContext(\"applicationContext.xml\"); Person01 person01 = (Person01) context.getBean(\"person01\"); System.out.println(person01);//cn.tedu.domain.Person01@3b084709 &#125; 2. 实例工厂12345678&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;!--配置实例工厂--&gt; &lt;bean id=\"person01InstanceFactory\" class=\"cn.tedu.factory.Person01InstanceFactory\" &gt;&lt;/bean&gt; &lt;bean id=\"person01\" factory-bean=\"person01InstanceFactory\" factory-method=\"getInstance\"&gt;&lt;/bean&gt;&lt;/beans&gt; 12345678910111213//创建实例工厂public class Person01InstanceFactory &#123; public Person01 getInstance()&#123; return new Person01(\"xx\"); &#125;&#125;//测试实例工厂@Test public void test06()&#123; ApplicationContext context = new ClassPathXmlApplicationContext(\"applicationContext1.xml\"); Person01 person01 = (Person01) context.getBean(\"person01\"); System.out.println(person01);//cn.tedu.domain.Person01@3b084709 &#125; 3. Spring工厂1234567&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt;&lt;!--配置Spring工厂--&gt; &lt;bean id=\"person01\" class=\"cn.tedu.factory.Person01SpringFactory\"&gt;&lt;/bean&gt;&lt;/beans&gt; 123456789101112131415161718192021222324252627//创建SpringFactory需要实现FactoryBean接口指定泛型为获取实例的泛型public class Person01SpringFactory implements FactoryBean&lt;Person01&gt; &#123; @Override public Person01 getObject() throws Exception &#123; //返回person01实例 return new Person01(\"xx\"); &#125; @Override public Class&lt;?&gt; getObjectType() &#123; //返回实例的class对象 return Person01.class; &#125; @Override public boolean isSingleton() &#123; //是否是单例，Spring默认全局只有一个实例 return true; &#125;&#125;//测试Spring工厂 @Test public void test07()&#123; ApplicationContext context = new ClassPathXmlApplicationContext(\"applicationContext2.xml\"); Person01 person01 = (Person01) context.getBean(\"person01\"); System.out.println(person01);//cn.tedu.domain.Person01@23e028a9 &#125;","categories":[{"name":"框架","slug":"框架","permalink":"http://yoursite.com/categories/%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"SpringDay02","slug":"SpringDay02","date":"2020-01-29T04:22:20.000Z","updated":"2020-02-11T09:19:23.414Z","comments":true,"path":"2020/01/29/SpringDay02/","link":"","permalink":"http://yoursite.com/2020/01/29/SpringDay02/","excerpt":"","text":"单例和多例Spring容器管理的bean在默认情况下是单例，也就是一个bean创建一个对象，存在内置Map集合中，之后无论获取多少次该bean，都返回的同一个对象。bean在单例模式的生命周期：bean在单例模式下，Spring容器启动时解析xml发现该bean标签后，直接创建该bean的对象存入内部map中保存，此后无论调用多少次getBean（）都是从map中获取该对象返回，一直是一个对象，次对象一直被Spring容器持有，知道容器退出时，随着容器的退出对象被销毁案例-单例123456789101112&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;!--配置约束条件可以设置从bean中获取对象是单例还是多例 scope=\"prototype\"是多例 scope=\"singleton\"是单例 Spring容器默认配置是单例 所以不配置是单例 --&gt; &lt;bean id=\"person\" class=\"cn.tedu.domain.Person\" scope=\"singelton\"&gt;&lt;/bean&gt;&lt;/beans&gt; 12345678910111213//测试单例 @Test public void test01()&#123; //初始化Spring容器 ApplicationContext context = new ClassPathXmlApplicationContext(\"applicationContext.xml\"); //获取bean对象 Person person1 = (Person) context.getBean(\"person\"); Person person2 = (Person) context.getBean(\"person\"); System.out.println(person1);//cn.tedu.domain.Person@2df3b89c System.out.println(person2);//cn.tedu.domain.Person@2df3b89c //关闭容器 ((ClassPathXmlApplicationContext) context).close(); &#125; bean在多例模式下的生命周期：bean在多例模式下，Spring容器启动时解析xml时发现bean标签后，只是将该bean进行管理，并不会创建对象，此后每次使用getBean（）获取该bean时，Spring都会重新创建该对象返回，每次都是一个新的对象。这个对象Spring容器不会持有，什么时候销毁取绝于使用对象自己销毁。案例-多例1234567891011&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;!--配置约束条件可以设置从bean中获取对象是单例还是多例 scope=\"prototype\"是多例 scope=\"singleton\"是单例 Spring容器默认配置是单例 所以不配置是单例 --&gt; &lt;bean id=\"person\" class=\"cn.tedu.domain.Person\" scope=\"prototype\"&gt;&lt;/bean&gt; 懒加载机制Spring默认在容器初始化的过程中，解析xml，并且创建单例的bean保存到map中，这样的机制在bean较少时问题不大，但一旦bean非常多时，Spring需要在启动的过程中花费大量的时间来创建bean花费大量的空间来存储bean，但这些bean可能很就都用不上，这种在启动时在时间和空间上的浪费很不值得。所以spring提供了懒加载机制。所谓的懒加载机制就是可以规定指定的bean不在启动时立刻创建，而是后续在第一次使用到才创建，从而减轻在启动过程中对时间和内存的消耗 懒加载机制只对单例bean有作用，对于多例bean设置懒加载没有意义 懒加载只是延后了对象的创建的时机，对象仍然是单例 懒加载的配置方式为指定bean配置懒加载1234567891011&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\" &gt;&lt;!--设置指定bean的懒加载default-lazy-init=\"true\"--&gt;&lt;bean id=\"person\" class=\"cn.tedu.domain.Person\" lazy-init=\"true\"&gt;&lt;/bean&gt;&lt;/beans&gt; 为全局配置懒加载1234567891011&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\" default-lazy-init=\"true\"&gt;&lt;!--设置默认全局配置的懒加载default-lazy-init=\"true\"--&gt;&lt;bean id=\"person\" class=\"cn.tedu.domain.Person\"&gt;&lt;/bean&gt;&lt;/beans&gt; 配置初始化和销毁的方法在Spring中如果某个bean在初始化之后or销毁之前要做一些额外操作可以为该bean配置厨师化和销毁的方法，在这些方法中完成功能Spring约束文件1234567&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;!--配置初始化和摧毁的方法--&gt;&lt;bean id=\"jdbcUtils\" class=\"cn.tedu.domain.JDBCUtils\" init-method=\"myInit\" destroy-method=\"myDestroy\"&gt;&lt;/bean&gt;&lt;/beans&gt; Spring中关键方法的执行顺序：!!!! 在Spring创建bean对象时，先创建对象（通过无参构造or工厂），之后立即调用init方式来执行初始化操作，之后此bean就可以哪来调用其它普通方法，而在对象销毁之前，Spring容器调用其destroy方法来执行销毁操作 Spring DIIOC（DI）-控制反转（依赖注入）所谓的IOC称之为控制反转，简单来说就是将对象的创建的权利及对象的生命周期的管理过程交由Spring框架来处理，从此在开发过程中不再需要关注对象的创建和生命周期的管理，而是在需要时由Spring框架提供，这个由Spring框架管理对象创建和生命周期的机制称之为控制反转。 而在创建对象的过程中Spring可以配置对对象的属性进行设置，这个过程称之为依赖注入也即DIset方法注入通常的javabean属性都会私有化，而对外暴露setXxx（）getXxx方法，此时Spring可以通过这样的setXxx（）方法将属性的值注入对象123456789101112131415161718192021222324252627282930313233343536&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;bean id=\"hero\" class=\"cn.tedu.domain.Hero\"&gt; &lt;property name=\"name\" value=\"亚索\"&gt;&lt;/property&gt; &lt;property name=\"age\" value=\"18\"&gt;&lt;/property&gt; &lt;property name=\"job\"&gt; &lt;list&gt; &lt;value&gt;程序员&lt;/value&gt; &lt;value&gt;架构师&lt;/value&gt; &lt;value&gt;大数据工程师&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;property name=\"set\"&gt; &lt;set&gt; &lt;value&gt;10000&lt;/value&gt; &lt;value&gt;40000&lt;/value&gt; &lt;value&gt;30000&lt;/value&gt; &lt;/set&gt; &lt;/property&gt; &lt;property name=\"map\"&gt; &lt;map&gt; &lt;entry key=\"亚索\" value=\"德玛西亚\"&gt;&lt;/entry&gt; &lt;entry key=\"刀妹\" value=\"艾欧尼亚\"&gt;&lt;/entry&gt; &lt;/map&gt; &lt;/property&gt; &lt;property name=\"props\"&gt; &lt;props&gt; &lt;prop key=\"1\"&gt;亚索&lt;/prop&gt; &lt;prop key=\"2\"&gt;刀妹&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 自定义bean的注入123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;bean id=\"hero\" class=\"cn.tedu.domain.Hero\"&gt; &lt;property name=\"name\" value=\"亚索\"&gt;&lt;/property&gt; &lt;property name=\"age\" value=\"18\"&gt;&lt;/property&gt; &lt;property name=\"job\"&gt; &lt;list&gt; &lt;value&gt;程序员&lt;/value&gt; &lt;value&gt;架构师&lt;/value&gt; &lt;value&gt;大数据工程师&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;property name=\"set\"&gt; &lt;set&gt; &lt;value&gt;10000&lt;/value&gt; &lt;value&gt;40000&lt;/value&gt; &lt;value&gt;30000&lt;/value&gt; &lt;/set&gt; &lt;/property&gt; &lt;property name=\"map\"&gt; &lt;map&gt; &lt;entry key=\"亚索\" value=\"德玛西亚\"&gt;&lt;/entry&gt; &lt;entry key=\"刀妹\" value=\"艾欧尼亚\"&gt;&lt;/entry&gt; &lt;/map&gt; &lt;/property&gt; &lt;property name=\"props\"&gt; &lt;props&gt; &lt;prop key=\"1\"&gt;亚索&lt;/prop&gt; &lt;prop key=\"2\"&gt;刀妹&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;property name=\"dog\" ref=\"dog\" &gt;&lt;/property&gt; &lt;property name=\"cat\" ref=\"cat\"&gt;&lt;/property&gt; &lt;property name=\"snake\" ref=\"snake\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean id=\"dog\" class=\"cn.tedu.domain.Dog\" &gt; &lt;property name=\"name\" value=\"jobs\"&gt;&lt;/property&gt; &lt;property name=\"age\" value=\"11\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean id=\"cat\" class=\"cn.tedu.domain.Cat\"&gt; &lt;property name=\"name\" value=\"tom\"&gt;&lt;/property&gt; &lt;property name=\"age\" value=\"4\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean id=\"snake\" class=\"cn.tedu.domain.Snake\"&gt; &lt;property name=\"name\" value=\"小白\"&gt;&lt;/property&gt; &lt;property name=\"age\" value=\"1000\"&gt;&lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 自动装配在Spring的set方式实现注入的过程中，支持自动装配机制，所谓的自动装配机制，会根据要设置的javabean属性的名字or类型到Spring中自动寻找对应的idor类型的进行设置，从而省去依次配置的过程，简化了配置123456789101112131415161718192021222324&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;!-- autowire=\"byName\" 根据javabean中需要注入属性的名字，在Spring容器中找到对应类型的id进行自动装配 autowire=\"byType\" 根据javabean中需要注入属性的类型，在Spring容器中找到对应的class类型进行配置 --&gt; &lt;bean id=\"hero\" class=\"cn.tedu.domain.Hero\" autowire=\"byName\"&gt; &lt;/bean&gt; &lt;bean id=\"dog\" class=\"cn.tedu.domain.Dog\" &gt; &lt;property name=\"name\" value=\"jobs\"&gt;&lt;/property&gt; &lt;property name=\"age\" value=\"11\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean id=\"cat\" class=\"cn.tedu.domain.Cat\"&gt; &lt;property name=\"name\" value=\"tom\"&gt;&lt;/property&gt; &lt;property name=\"age\" value=\"4\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean id=\"snake\" class=\"cn.tedu.domain.Snake\"&gt; &lt;property name=\"name\" value=\"小白\"&gt;&lt;/property&gt; &lt;property name=\"age\" value=\"1000\"&gt;&lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; 基于构造方法的注入对象属性设置的另一种方式是在对象创建的过程中通过构造方法传入参数并且设置对象的属性。123456789101112131415&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;bean id=\"hero\" class=\"cn.tedu.domain.Hero\"&gt; &lt;constructor-arg index=\"0\" value=\"1\"&gt;&lt;/constructor-arg&gt; &lt;constructor-arg index=\"1\" value=\"旺财\"&gt;&lt;/constructor-arg&gt; &lt;constructor-arg index=\"2\" ref=\"dog\"&gt;&lt;/constructor-arg&gt; &lt;/bean&gt; &lt;bean id=\"dog\" class=\"cn.tedu.domain.Dog\"&gt; &lt;property name=\"name\" value=\"小白\"&gt;&lt;/property&gt; &lt;property name=\"age\" value=\"4\"&gt;&lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt;","categories":[{"name":"框架","slug":"框架","permalink":"http://yoursite.com/categories/%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"SpringDay04","slug":"SpringDay04","date":"2020-01-29T04:22:20.000Z","updated":"2020-02-11T11:54:50.871Z","comments":true,"path":"2020/01/29/SpringDay04/","link":"","permalink":"http://yoursite.com/2020/01/29/SpringDay04/","excerpt":"","text":"SpringAOP1. Spring aop的基本概念连接点 连接点（Joinpoint）：在程序执行过程中某个特定的点，比如某方法调用的时候或者处理异常的时候。在Spring AOP中，一个连接点总是表示一个方法的执行。 通俗讲： 层与层之间方法调用的过程称之为连接点 切入点（Pointcut）：匹配连接点的断言。通知和一个切入点表达式关联，并在满足这个切入点的连接点上运行（例如，当执行某个特定名称的方法时）。切入点表达式如何和连接点匹配是AOP的核心。核心：Spring缺省使用AspectJ切入点语法。 通俗讲 在连接点的基础上 增加上切入规则 选择出需要进行增强的连接点 这些基于切入规则选出来的连接点 就称之为切入点。 切面 切面（Aspect）：一个关注点的模块化，这个关注点可能会横切多个对象。事务管理是J2EE应用中一个关于横切关注点的很好的例子。在Spring AOP中，切面可以使用基于模式）或者基于@Aspect注解的方式来实现。 通知 通知（Advice）：在切面的某个特定的连接点上执行的动作。其中包括了 “around”、“before”和“after”等不同类型的通知（通知的类型将在后面部分进行讨论）。许多AOP框架（包括Spring）都是以拦截器做通知模型，并维护一个以连接点为中心的拦截器链。 通俗讲： 在spring底层的代理拦截下切入点后，将切入点交给切面类，切面类中就要有处理这些切入点的方法，这些方法就称之为通知（也叫增强 增强方法）。针对于切入点执行的过程，通知还分为不同的类型，分别关注切入点在执行过程中的不同的时机。目标对象 目标对象（Target Object）： 被一个或者多个切面所通知的对象。也被称做被通知（advised）对象。 既然Spring AOP是通过运行时代理实现的，这个对象永远是一个被代理（proxied）对象 通俗讲：就是真正希望被访问到的对象。spring底层的动态代理对他进行了代理，具体能不能真的访问到目标对象，或在目标对象真正执行之前和之后是否做一些额外的操作，取决于切面。案例1234567891011121314151617181920212223&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:aop=\"http://www.springframework.org/schema/aop\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd\"&gt; &lt;context:component-scan base-package=\"cn.tedu\"&gt;&lt;/context:component-scan&gt; &lt;context:annotation-config&gt;&lt;/context:annotation-config&gt; &lt;!--配置切面--&gt; &lt;aop:config&gt; &lt;aop:aspect ref=\"firstAspect\"&gt; &lt;!--配置通知方法--&gt; &lt;aop:before method=\"myBefore\" pointcut=\"within(cn.tedu.service.UserServiceImpl)\"&gt;&lt;/aop:before&gt; &lt;/aop:aspect&gt; &lt;/aop:config&gt;&lt;/beans&gt; 123456@Componentpublic class FirstAspect &#123; public void myBefore()&#123; System.out.println(\"记录日志\"); &#125;&#125;","categories":[{"name":"框架","slug":"框架","permalink":"http://yoursite.com/categories/%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"SpringDay03","slug":"SpringDay03","date":"2020-01-29T04:22:20.000Z","updated":"2020-02-11T09:19:58.546Z","comments":true,"path":"2020/01/29/SpringDay03/","link":"","permalink":"http://yoursite.com/2020/01/29/SpringDay03/","excerpt":"","text":"Spring注解方式实现IOC和DI1.Spring注解Spring除了默认的使用xml配置&lt;bean&gt;标签的方式实现配置外，也可以通过注解的方式来实现配置，这种方式效率更高，配置信息清晰，代码在哪对应的配置也在哪儿。所谓注解就是程序看的提示信息，很多时候都用来作为轻量级配置方式 Spring注解方式实现IOCa. 导入开发jar包b. 编写配置文件，并导入context约束 123456789101112&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\"&gt; &lt;context:component-scan base-package=\"cn.tedu\"&gt;&lt;/context:component-scan&gt; &lt;context:annotation-config&gt;&lt;/context:annotation-config&gt;&lt;/beans&gt; c. 开启包扫描 1&lt;context:component-scan base-package=\"cn.tedu\"&gt;&lt;/context:component-scan&gt; d. 使用注解注册bean 在配置的包中的额类上使用@Component注解，这个类会自动被注册为bean，使用当前类的class为的class，默认情况下判断类名的第二字母，如果是大写，首字母不变作为id，如果第二个字母是小写，首字母就是小写，默认作为id，也可以自己指定id e. 可以使bean类实现BeanNameAware接口，并实现其中setBeanName 方法， spring容器会在初始化bean时，调用此方法告知当前bean的id。通过这个方式可以获取 bean的id信息。 1234567@Component(\"person\")//指定标签id不指定类名的首字母小写public class Person implements BeanNameAware &#123; @Override public void setBeanName(String name) &#123; System.out.println(\"===\"+this.getClass().getName()+\"===\"+name); &#125;&#125; f. 注解方式实现工厂注册bean 1234567@Componentpublic class DogFactory &#123; @Bean//可以自己指定id，也可以默认getInstance public Dog getInstance()&#123; return new Dog(\"小黑\"); &#125;&#125; Spring注解方式实现DI123456789101112131415161718192021222324252627282930313233&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:util=\"http://www.springframework.org/schema/util\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util-3.2.xsd\"&gt; &lt;context:component-scan base-package=\"cn.tedu.domain\"&gt;&lt;/context:component-scan&gt; &lt;!--开启注解方式的DI--&gt; &lt;context:annotation-config&gt;&lt;/context:annotation-config&gt; &lt;context:property-placeholder location=\"my.properties\"&gt;&lt;/context:property-placeholder&gt; &lt;util:list id=\"list\"&gt; &lt;value&gt;aaa&lt;/value&gt; &lt;value&gt;bbb&lt;/value&gt; &lt;value&gt;ccc&lt;/value&gt; &lt;/util:list&gt; &lt;util:set id=\"set\"&gt; &lt;value&gt;111&lt;/value&gt; &lt;value&gt;222&lt;/value&gt; &lt;value&gt;333&lt;/value&gt; &lt;/util:set&gt; &lt;util:map id=\"map\"&gt; &lt;entry key=\"k1\" value=\"v1\"&gt;&lt;/entry&gt; &lt;/util:map&gt; &lt;util:properties id=\"properties\"&gt; &lt;prop key=\"1\"&gt;小&lt;/prop&gt; &lt;/util:properties&gt;&lt;/beans&gt; 在类的属性中通过@Value注入赋值1234567891011121314151617181920212223242526272829@Componentpublic class Hero &#123; //@Value(\"亚索\") @Value(\"$&#123;name&#125;\") private String name; //@Value(\"13\") @Value(\"$&#123;age&#125;\") private int age; @Value(\"#&#123;@list&#125;\") private List&lt;String&gt; list; @Value(\"#&#123;@set&#125;\") private Set&lt;String&gt; set; @Value(\"#&#123;@map&#125;\") private Map&lt;String,String&gt; map; @Value(\"#&#123;@properties&#125;\") private Properties props; @Override public String toString() &#123; return \"Hero&#123;\" + \"name='\" + name + '\\'' + \", age=\" + age + \", list=\" + list + \", set=\" + set + \", map=\" + map + \", props=\" + props + '&#125;'; &#125;&#125; f.使用注解注入自定义bean类型数据在bean中的属性上通过@Autowired实现自定义bean类型的属性注入代码1234567891011121314151617181920@Componentpublic class Hero &#123; @Value(\"亚索\") private String name; @Value(\"10\") private int age; @Autowired @Qualifier(\"JMDog\")//一旦配置了@Qualifier(\"JMDog\")会按照指定id注入，找到注入，找不到就抛出异常 //@Resource(name = \"JMDog\") Java提供的不是Spring提供的，基本不用 private Dog dog; @Override public String toString() &#123; return \"Hero&#123;\" + \"name='\" + name + '\\'' + \", age=\" + age + \", dog=\" + dog + '&#125;'; &#125;&#125; 当Spring容器解析到@Component注解时，创建当前类的bean在Spring容器中进行管理，在创建bean的过程中发现了@Autowired注解，会根据当前bean的类型，寻找Spring中是否存在该类型的bean，如果唯一的bean，直接注入，如果有多个根据id判断，如果有注入，如果没有抛出异常 其次，可以额外配置@Qualifier（“”）注解强制要求按照id进行寻找，找到就注入，找不到就抛出异常，注意@Autowired&@Qualifier（“”）需要配合使用 其他注解 @Scope(value=”prototype”) 配置修饰类是单例还是多例，Spring默认是单例12345//配置bean为多例@Component@Scope(\"prototype\")public class Person &#123;&#125; @Lazy 配置修饰的类的bean采用懒加载机制 Spring默认在初始化时就通过反射创建对象，并且以键值对的形式存入Spring容器中的Map集合中，在类上配置懒加载的注释，可以在对象使用时再创建，避免内存的浪费1234567@Component@Lazy//懒加载public class Person2 &#123; public Person2() &#123; System.out.println(\"person2...init\"); &#125;&#125; @PostConstruct 在bean对应的类中 修饰某个方法 将该方法声明为初始化方法，对象创建之后立即执 行。1234@PostConstruct public void initConn()&#123; System.out.println(\"初始化数据库连接\"); &#125; @PreDestroy 在bean对应的类中 修饰某个方法 将该方法声明为销毁的方法，对象销毁之前调用的方 法。1234@PreDestroy public void destroyConn()&#123; System.out.println(\"销毁数据库连接\"); &#125; @Controller @Service @Repository @Component 这四个注解的功能是完全相同的，都是用来修饰类，将雷声名为Spring管理的bean的 @Component 一般认为是通用的注解 @Controller 用在软件分层的控制层，一般用在web层上 @Service用在业务访问层，service层 @Repository用在数据访问层（Dao层） 利用Spring IOC DI 实现软件分层解耦 软件分层思想回顾 在软件领域有MVC软件设计思想，指导着软件开发过程。在javaee开发领域，javaee 的经典三层架构MVC设计思想的经典应用。而在软件设计思想中，追求的是”高内聚 低 耦合”的目标，利用Spring的IOC 和 DI 可以非常方便的实现这个需求。 12345678910111213141516171819202122232425262728293031323334353637383940//创建不同类的数据的类型实现userDao接口@Repository(\"userDao\")public class OracleUserDao implements UserDao&#123; @Override public void addUser() &#123; System.out.println(this+\"正在向数据库添加用户\"); &#125;&#125;@Repositorypublic class MySqlUserDao implements UserDao &#123; @Override public void addUser() &#123; System.out.println(this+\"正在添加用户\"); &#125;&#125;public interface UserDao &#123; public void addUser();&#125;//service层@Servicepublic class UserServiceImpl implements UserService &#123; @Autowired UserDao userDao = null; @Override public void regist() &#123; userDao.addUser(); &#125;&#125;public interface UserService &#123; public void regist();&#125;//web层@Controllerpublic class RegistServlet &#123; @Autowired private UserService service = null; public void regist() &#123; service.regist(); &#125;&#125;","categories":[{"name":"框架","slug":"框架","permalink":"http://yoursite.com/categories/%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"SpringDay05","slug":"SpringDay05","date":"2020-01-29T04:22:20.000Z","updated":"2020-02-12T14:55:17.430Z","comments":true,"path":"2020/01/29/SpringDay05/","link":"","permalink":"http://yoursite.com/2020/01/29/SpringDay05/","excerpt":"","text":"切入点表达式within表达式通过类名进行匹配，是一种粗粒度的切入点表达式，不可以指定某个方法进行增强，以类作为基本的单位在Spring的约束文件中配置切入点和切面（within切入点表达式）1234567891011121314151617 &lt;!--添加Spring AOP相关的约束 xmlns:aop=\"http://www.springframework.org/schema/aop\" http://www.springframework.org/schema/aop/spring-aop.xsd --&gt; &lt;!--配置包扫描--&gt; &lt;context:component-scan base-package=\"cn.tedu\"&gt;&lt;/context:component-scan&gt; &lt;!--注释配置--&gt; &lt;context:annotation-config&gt;&lt;/context:annotation-config&gt; &lt;!--配置AOP--&gt; &lt;aop:config&gt; &lt;!--配置切面类--&gt; &lt;aop:aspect ref=\"firstAspect\"&gt; &lt;!--指定切入点规则,通知方法--&gt; &lt;aop:before method=\"before\" pointcut=\"within(cn.tedu.service.UserServiceImpl)\"&gt;&lt;/aop:before&gt; &lt;/aop:aspect&gt; &lt;/aop:config&gt;&lt;/beans&gt; execution表达式通过方法进行匹配，是一种细粒度的切入点表达式，以方法作为基本的单元123456&lt;aop:config&gt; &lt;aop:pointcut id=\"pc01\" expression=\"execution(* cn.tedu.service..*(..))\"/&gt; &lt;aop:aspect ref=\"firstAspect\"&gt; &lt;aop:before method=\"before\" pointcut-ref=\"pc01\"&gt;&lt;/aop:before&gt; &lt;/aop:aspect&gt; &lt;/aop:config&gt; Spring的五大通知类型前置通知 在目标方法执行之前执行的通知环绕通知 在目标方法执行之前，执行之后都会执行的通知后置通知 在目标方法执行之后的通知异常通知 在目标方法抛出异常后执行的通知最终通知 无论如何都会在目标方法调用后执行 多个通知执行顺序未出现异常前置通知 环绕通知前 目标方法（添加用户cn.tedu.domain.User@710f4dc7） 最终通知 环绕通知后 后置通知出现异常（）前置通知 环绕通知前 目标方法（添加用户cn.tedu.domain.User@710f4dc7） 最终通知 异常通知出现多个切面类时，采用了责任链设计模式，切面的配置顺序决定了通知的执行顺序。以两个切面为例：具体执行过程如下：web层调用service层目标方法时，会经过第一个切面，执行前置通知和环绕前通知，环绕通知调用proceed（）时进入下一个切面，执行第二个切面的前置通知，和环绕前通知，进入第二个切面的proceed的方法，放行调用目标方法，下面进入第二个切面的环绕后通知，和后置通知，最终通知，在进入第一个切面层，环绕后通知，和后置通知，最终通知，响应给web层代码执行结果1234567891011前置通知环绕通知前second前置通知second环绕通知前添加用户cn.tedu.domain.User@2de23121second最终通知second环绕通知后second后置通知最终通知环绕通知后后置通知","categories":[{"name":"框架","slug":"框架","permalink":"http://yoursite.com/categories/%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"SpringDay06","slug":"SpringDay06","date":"2020-01-29T04:22:20.000Z","updated":"2020-02-14T14:50:47.212Z","comments":true,"path":"2020/01/29/SpringDay06/","link":"","permalink":"http://yoursite.com/2020/01/29/SpringDay06/","excerpt":"","text":"1.通过aop进行权限的控制 通过自定义注解声明业务方法是否需要权限控制 通过权限注解上的属性声明需要什么样的权限 通过切面拦截业务的方法，根据是否需要权限，是否具有权限，控制目标方法的执行关键代码123456789101112131415161718192021222324252627//切面代码@Component@Aspectpublic class PrivAspect &#123; @Around(\"execution(* cn.tedu.servive..*(..))\") public Object around(ProceedingJoinPoint pjp) throws Throwable &#123; //1.获取目标对象 Object target = pjp.getTarget(); //2.获取目标方法 MethodSignature signature = (MethodSignature) pjp.getSignature(); Method method = signature.getMethod();//接口上的方法 //3.获取实现类上的目标方法 Method methodInstance = target.getClass().getMethod(method.getName(), method.getParameterTypes()); //4.判断是否有注解 if (methodInstance.isAnnotationPresent(PrivAnno.class))&#123; PrivAnno annotation = methodInstance.getAnnotation(PrivAnno.class); PrivEnum[] privEnums = annotation.value(); if (Arrays.asList(privEnums).contains(Test01.priv)) &#123; return pjp.proceed(); &#125;else &#123; throw new RuntimeException(\"权限不够\"); &#125; &#125;else &#123; return pjp.proceed(); &#125; &#125;&#125; 1234//枚举类public enum PrivEnum &#123; ADMIN,SUPERADMIN,VISITOR&#125; 123456//开发注释，来进行权限控制@Target(ElementType.METHOD)@Retention(RetentionPolicy.RUNTIME)public @interface PrivAnno &#123; PrivEnum [] value() ;&#125; 实现事务的控制 通过AOP实现事务的控制 开发事务注解，通过业务方法上是否有注解来表示方法是否需要事务 在切面中判断目标方法是否具有事务注解决定是否执行事务 通过事务管理器管理事务防止耦合1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253//jdbc连接工具类public class JDBCUtils &#123; private JDBCUtils() &#123; &#125; static &#123; try &#123; Class.forName(\"com.mysql.jdbc.Driver\"); &#125; catch (Exception e) &#123; e.printStackTrace(); throw new RuntimeException(e); &#125; &#125; public static Connection getConn() &#123; try &#123; return DriverManager.getConnection(\"jdbc:mysql:///mydb2\",\"root\",\"root1234\"); &#125; catch (SQLException e) &#123; e.printStackTrace(); throw new RuntimeException(e); &#125; &#125; public static void release(Connection conn, Statement stat, ResultSet rs) &#123; if (rs != null) &#123; try &#123; rs.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; finally &#123; rs = null; &#125; &#125; if (stat != null) &#123; try &#123; stat.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; finally &#123; stat = null; &#125; &#125; if (conn != null) &#123; try &#123; conn.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; finally &#123; conn = null; &#125; &#125; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162//事务管理public class TransactionManager &#123; private static Connection conn = JDBCUtils.getConn(); private TransactionManager()&#123; &#125; /** * 开启事务 */ public static void startTran()&#123; try &#123; conn.setAutoCommit(false); &#125; catch (SQLException e) &#123; e.printStackTrace(); throw new RuntimeException(e); &#125; &#125; /** * 获取连接 */ public static Connection getConn()&#123; return conn; &#125; /** * 提交事务 */ public static void commitTran()&#123; try &#123; conn.commit(); &#125; catch (SQLException e) &#123; e.printStackTrace(); throw new RuntimeException(e); &#125; &#125; /** * 回滚事务 */ public static void rollbackTran()&#123; try &#123; conn.rollback(); &#125; catch (SQLException e) &#123; e.printStackTrace(); throw new RuntimeException(e); &#125; &#125; /** * 释放资源 */ public static void release()&#123; try &#123; conn.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); throw new RuntimeException(e); &#125; &#125;&#125; 12345678910111213141516171819//切面类@Component@Aspectpublic class TransAspect &#123; @Around(\"execution(* cn.tedu.service..*(..)) &amp;&amp; @annotation(ax)\") public Object myAround(ProceedingJoinPoint pjp, Trans ax) throws Throwable &#123; try &#123; TransactionManager.startTran(); Object retObj = pjp.proceed(); TransactionManager.commitTran(); return retObj; &#125; catch (Throwable e) &#123; TransactionManager.rollbackTran(); throw e; &#125; finally &#123; TransactionManager.release(); &#125; &#125; spring整合JDBC-声名式事务处理1234567&lt;!--配置数据源--&gt;&lt;bean id=\"dataSource\" class=\"com.mchange.v2.c3p0.ComboPooledDataSource\"&gt; &lt;property name=\"driverClass\" value=\"com.mysql.cj.jdbc.Driver\"&gt;&lt;/property&gt; &lt;property name=\"jdbcUrl\" value=\"jdbc:mysql:///mydb2\"&gt;&lt;/property&gt; &lt;property name=\"user\" value=\"root\"&gt;&lt;/property&gt; &lt;property name=\"password\" value=\"root1234\"&gt;&lt;/property&gt; &lt;/bean&gt; 12345678910&lt;!--配置jdbc模版--&gt;&lt;bean id=\"dataSource\" class=\"com.mchange.v2.c3p0.ComboPooledDataSource\"&gt; &lt;property name=\"driverClass\" value=\"com.mysql.jdbc.Driver\"&gt;&lt;/property&gt; &lt;property name=\"jdbcUrl\" value=\"jdbc:mysql:///mydb2\"&gt;&lt;/property&gt; &lt;property name=\"user\" value=\"root\"&gt;&lt;/property&gt; &lt;property name=\"password\" value=\"root1234\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean id=\"jdbcTemplate\" class=\"org.springframework.jdbc.core.JdbcTemplate\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"&gt;&lt;/property&gt; &lt;/bean&gt; Spring中的事务管理（配置文件）123456789101112131415161718&lt;!--配置事务管理--&gt; &lt;bean id=\"transactionManager\" class=\"org.springframework.jdbc.datasource.DataSourceTransactionManager\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!--配置事务切面--&gt; &lt;aop:config&gt; &lt;aop:pointcut id=\"pc01\" expression=\"execution(* cn.tedu.service..* (..))\"/&gt; &lt;aop:advisor advice-ref=\"txAdvice\" pointcut-ref=\"pc01\"&gt;&lt;/aop:advisor&gt; &lt;/aop:config&gt; &lt;!--配置事务通知--&gt; &lt;tx:advice id=\"txAdvice\" transaction-manager=\"transactionManager\" &gt; &lt;tx:attributes&gt; &lt;!--rollback-for=\"java.lang.Throwable 对异常强制回滚，进行事务管理--&gt; &lt;tx:method name=\"addUser\" propagation=\"REQUIRED\" rollback-for=\"java.lang.Throwable\"/&gt; &lt;tx:method name=\"delUser\" propagation=\"REQUIRED\"/&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; Spring中的事务管理（注解方式）12345678910111213141516171819&lt;context:component-scan base-package=\"cn.tedu\"&gt;&lt;/context:component-scan&gt; &lt;context:annotation-config&gt;&lt;/context:annotation-config&gt; &lt;!--配置数据源--&gt; &lt;bean id=\"dataSource\" class=\"com.mchange.v2.c3p0.ComboPooledDataSource\"&gt; &lt;property name=\"driverClass\" value=\"com.mysql.jdbc.Driver\"&gt;&lt;/property&gt; &lt;property name=\"jdbcUrl\" value=\"jdbc:mysql:///mydb2\"&gt;&lt;/property&gt; &lt;property name=\"user\" value=\"root\"&gt;&lt;/property&gt; &lt;property name=\"password\" value=\"root1234\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!--配置jdbc模版类--&gt; &lt;bean id=\"jdbcTemplate\" class=\"org.springframework.jdbc.core.JdbcTemplate\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!--配置事务管理器--&gt; &lt;bean id=\"transactionManager\" class=\"org.springframework.jdbc.datasource.DataSourceTransactionManager\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!--开启注解方式事务管理--&gt; &lt;tx:annotation-driven/&gt; 指定开启事务的方法12345678@Transactional(rollbackFor = IOException.class) @Override public void addUser(User user) throws IOException &#123; userDao.addUser(user); //int i = 1/0; throw new IOException(\"io 异常\"); &#125;","categories":[{"name":"框架","slug":"框架","permalink":"http://yoursite.com/categories/%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"SpringMVCDay01","slug":"SpringMVCDay01","date":"2020-01-29T04:22:20.000Z","updated":"2020-04-28T14:58:16.072Z","comments":true,"path":"2020/01/29/SpringMVCDay01/","link":"","permalink":"http://yoursite.com/2020/01/29/SpringMVCDay01/","excerpt":"","text":"1.SpringMVC概述SpringMVC是一个WEB层，控制层框架，主要用来负责与客户端交互，业务逻辑的调用。SpringMVC是Spring家族的一大组件，Spring整合SpringMVC可以做到无缝集成，特点：简单易用性能佳2.SpringMVC相对与Servlet的优势a. Servlet的开发配置相对麻烦，servlet特别多的时候web.xml文件将会非常的臃肿b. 每个Servlet都只能处理一个功能，如果需要多个功能就需要开发多个servlet，项目中存在大量的servlet显得臃肿。c. 获取请求参数进行类型转换封装数据到bean的过程比较繁琐。d. 其他开发中不方便的地方，例如乱码问题，数据格式处理，表单检索 spring MVC详解 SpringMVC的组件a. 前端控制器（DispatcherServlet） 本质上是一个servlet，相当于一个中转站，所有的访问都会走到这个servlet中，再根据配置进行中转到相对应的handler中进行处理，获取数据和视图b. 处理器映射器（HandlerMapping） 本质上就是一段映射关系，将将访问路径和对应的Handler存储为映射关系，在需要时供前端控制器查阅。c. 处理器适配器（HandlerAdapter） 本质上就是一个适配器，可以根据要求找到对应的handler来运行。前端控制器找到处理器映射器找到对应的handler信息之后，将请求响应和对应的handler信息交由处理器适配器来处理，处理器适配器找到真正的handler执行后，将结果也就是model and view返回给前端控制器。d. 视图解析器（ViewResolver） 本质上是一种映射关系，可以将视图名称映射到真正的视图地址。前端控制器调用处理器适配完成后的到model和view，将view信息传给视图解析器得到真正的view。e. 视图（view） 本质上就是将handler处理器中返回的model数据嵌入到视图解析后得到的jsp页面中，向客户端作出响应 图解： 配置前端控制器1.前端控制器本身就是一个servlet ，首先要在WEB-INF下的web.xml中配置该web项目的servlet123456789101112131415&lt;!--配置前端控制器--&gt; &lt;servlet&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;!--手动配置核心文件的位置--&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:/springmvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;/servlet&gt; &lt;!--配置servlet的映射--&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;url-pattern&gt;*.action&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; 生成SpringMVC核心配置文件1234567&lt;!--配置处理器映射器中的路径和处理器的映射关系--&gt; &lt;bean name=\"/hello.action\" class=\"cn.tedu.web.Hello\"&gt;&lt;/bean&gt; &lt;!--配置视图解析器中视图名称和真正页面的映射关系，拼接上前缀后缀--&gt; &lt;bean class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\"&gt; &lt;property name=\"prefix\" value=\"/WEB-INF/jsp/\"&gt;&lt;/property&gt; &lt;property name=\"suffix\" value=\".jsp\"&gt;&lt;/property&gt; &lt;/bean&gt; 123456789101112131415//创建类实现controller接口public class Hello implements Controller &#123; @Override public ModelAndView handleRequest(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse) throws Exception &#123; //创建modelAndView ModelAndView modelAndView = new ModelAndView(); //封装数据 modelAndView.addObject(\"msg1\",\"hello world\"); modelAndView.addObject(\"msg2\",\"hello springMVC\"); //封装视图 modelAndView.setViewName(\"hello\"); //返回modelAndView return modelAndView; &#125;&#125; SpringMVC注解方式配置SpringMVC支持使用注解方式配置，比配置文件更加的灵活易用，是目前的主流配置方式springMVC注解工作原理 当服务器启动时，会先加载web.xml文件，之后通过引入核心配置文件来加载SpringMVC.xml 当解析到包扫描时，扫描指定的包，并将含有@Conteoller注解的类解析为处理器 如果配置过mvc:annotation-driven就会解析Spring-MVC注解 解析@requestMapping（value=“/test01.action”），将指定的地址和当前方法的映射关系保存 当客户端发出请求访问时，SpringMVC寻找改地址的映射关系，找到就执行方法，找不到抛出404 123456789&lt;!--配置包扫描--&gt;&lt;context:component-scan base-package=\"cn.tedu\"&gt;&lt;/context:component-scan&gt;&lt;!--配置mvc注解方式的DI--&gt; &lt;mvc:annotation-driven&gt;&lt;/mvc:annotation-driven&gt; &lt;!--配置视图解析器中视图名称和真正页面的映射关系--&gt; &lt;bean class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\"&gt; &lt;property name=\"prefix\" value=\"/WEB-INF/jsp/\"&gt;&lt;/property&gt; &lt;property name=\"suffix\" value=\".jsp\"&gt;&lt;/property&gt; &lt;/bean&gt; 1234567891011121314&lt;!--配置前端控制器--&gt; &lt;servlet&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;!--手动配置核心文件的位置--&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:springmvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;url-pattern&gt;*.action&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; test01.jsp123456789&lt;%@ page contentType=\"text/html;charset=UTF-8\" language=\"java\" %&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;this is test01.jsp&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;p&gt;$&#123;msg&#125;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 123456789101112131415161718//测试//在类上加上注解指的是在访问的地址上加上前缀@RequestMapping(\"/my01\")@Controllerpublic class Controller01 &#123; @RequestMapping(\"/test01.action\") public String test01(Model model)&#123; model.addAttribute(\"msg\",\"one word one dream\"); return \"test01\"; &#125; //通过value属性指定当前的映射到那些路径,可以映射到多个路径 @RequestMapping(&#123;\"/test04.action\",\"/test02.action\",\"/test03.action\"&#125;) public String test02(Model model)&#123; System.out.println(\"just do it\"); model.addAttribute(\"msg\",\"one word one dream\"); return \"test02\"; &#125;&#125; SpringMVC获取请求参数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106@Controllerpublic class MyController01 &#123; /** * 获取请求参数：日期数据处理 * 通过注册自定义类型编辑器使SpringMVC支持自定义格式请求参数的处理 * 此处，SpringMVC为Date类型已经提供了编辑器类，所以直接使用，不用自己写 * http://localhost:8080/SpringMVCDay01_04_Params_war_exploded/my01test09.jsp */ @InitBinder public void myInitBinder(ServletRequestDataBinder dataBinder)&#123; dataBinder.registerCustomEditor(Date.class,new CustomDateEditor(new SimpleDateFormat(\"yyyy-MM-dd\"),true)); &#125; @RequestMapping(\"/test09.action\") public void test09(String name, int age, Date birthday) &#123; System.out.println(name); System.out.println(age); SimpleDateFormat format = new SimpleDateFormat(\"yyyy-MM-dd\"); String format1 = format.format(birthday); System.out.println(format1); &#125; /** * 获取请求参数：中文乱码解决 * 如果服务器配置的编码是utf-8,且项目采用的也是utf-8则默认请求参数无乱码 * tomcat8默认编码为utf-8(可以更改) * tomcat7及更老的版本默认编码为iso8859-1(可以更改) * 如果遇到服务器编码和项目编码不一致时会产生乱码 * 对于POST提交 可以通过request.setCharacterEncoding(\"utf-8\")解决乱码 * 但此行代码对GET无效，GET提交的请求参数乱码，只能手工编解码来解决 * 手工编解码解决乱码的方式对POST提交也有效 * springmvc提供了过滤器CharacterEncodingFilter来帮助我们解决乱码 * 但此过滤器本质上就是request.setCharacterEncoing()所以也是只对POST有效 * ,即使配置了GET提交乱码也要手动解决 * */ @RequestMapping(\"/test08.action\") public void test08(HttpServletRequest request) throws UnsupportedEncodingException &#123; //POST提交 //request.setCharacterEncoding(\"utf-8\"); //String uname = request.getParameter(\"uname\"); //System.out.println(uname); //GET提交 //String uname = request.getParameter(\"uname\"); //byte [] data = uname.getBytes(\"iso8859-1\"); //uname = new String(data,\"utf-8\"); //System.out.println(uname); &#125; // 获取请求参数：多个同名请求参数的处理 // http://localhost:8080/SpringMVCDay01_04_Params_war_exploded/my01/test07.action?name=tt&amp;love=bb&amp;love=pp&amp;love=zz @RequestMapping(\"/test07.action\") public void test07(String name,String [] love) &#123; //封装到数组里 System.out.println(name); System.out.println(Arrays.asList(love)); &#125; //获取请求参数：自动封装请求参数到bean 对复杂类型的处理 // http://localhost:8080/SpringMVCDay01_04_Params_war_exploded/my01/test06.action?id=35&amp;name=jj&amp;age=18&amp;addr=cd&amp;dog.name=wd&amp;dog.age=1&amp;dog.cat.name=xx&amp;dog.cat.age=3 @RequestMapping(\"/test06.action\") public void test06(User user) &#123; System.out.println(user); &#125; //获取请求参数：自动封装请求参数到bean 对复杂类型的处理 // http://localhost:8080/SpringMVCDay01_04_Params_war_exploded/my01/test05.action?id=35&amp;name=jj&amp;age=18&amp;addr=cd&amp;dog.name=wd&amp;dog.age=1 @RequestMapping(\"/test05.action\") public void test05(User user) &#123; System.out.println(user); &#125; //获取请求参数：自动封装请求参数到bean // http://localhost:8080/SpringMVCDay01_04_Params_war_exploded/my01/test04.action?id=35&amp;name=jj&amp;age=18&amp;addr=cd @RequestMapping(\"/test04.action\") public void test04(User user) &#123; System.out.println(user); &#125; //获取请求参数: 通过@RequestParam指定参数赋值,解决请求参数名和方法参数不一致时赋值问题 // http://localhost:8080/SpringMVCDay01_04_Params_war_exploded/my01/test03.action?name=yasuo&amp;uage=19 @RequestMapping(\"/test03.action\") public void test03(@RequestParam(\"name\") String username, @RequestParam(\"uage\") int age) &#123; System.out.println(username + \"~~~~\" + age); &#125; //获取请求参数: 直接获取 // http://localhost:8080/SpringMVCDay01_04_Params_war_exploded/my01/test02.action?username=yasuo&amp;age=19 @RequestMapping(\"/test02.action\") public void test02(String username, int age) &#123; System.out.println(username + \"~~~~\" + age); &#125; //获取请求参数：传统方式获取 // http://localhost:8080/SpringMVCDay01_04_Params_war_exploded/my01/test01.action?username=yasuo&amp;age=19 @RequestMapping(\"/test01.action\") public void test01(HttpServletRequest request) &#123; String username = request.getParameter(\"username\"); int age = Integer.parseInt(request.getParameter(\"age\")); System.out.println(username + \"~~~~\" + age); &#125;","categories":[{"name":"框架","slug":"框架","permalink":"http://yoursite.com/categories/%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"redis-cluster","slug":"redis-cluster","date":"2020-01-29T04:22:20.000Z","updated":"2020-03-18T06:23:14.782Z","comments":true,"path":"2020/01/29/redis-cluster/","link":"","permalink":"http://yoursite.com/2020/01/29/redis-cluster/","excerpt":"","text":"特点： 两两互联，任意节点和其他节点通信，底层使用二进制协议优化传输速度 引入新的数据分片计算方法（hash槽）引入hash取模（散列算法CRC16后对16384取余槽道","categories":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/categories/redis/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"单例模式","slug":"单例模式","date":"2020-01-28T08:37:29.000Z","updated":"2020-02-01T11:53:20.471Z","comments":true,"path":"2020/01/28/单例模式/","link":"","permalink":"http://yoursite.com/2020/01/28/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"单例设计模式（饿汉式静态变量）1234567891011121314151617181920212223242526public class SingleTest1 &#123; @Test public void test()&#123; //通过调用静态方法来创建实例 Singleton instance = Singleton.getInstance(); Singleton instance1 = Singleton.getInstance(); //证明不管多少次调用Singleton类同一个实例 System.out.println(instance==instance1);//true &#125;&#125;//饿汉式（静态变量）//优点：这种写法比较简单，在类装载的时候就完成了实例化，避免了线程同步的问题//缺点：没有实现lazy loading的效果，实例创建可能不会被使用，导致内存的浪费class Singleton&#123; //私有化构造方法，目的：不让外界随便创建实例 private Singleton()&#123; &#125; //本类部自己创建实例 private static Singleton instance = new Singleton(); //对外界提供公共的静态方法，返回实例的对象 public static Singleton getInstance()&#123; return instance; &#125;&#125; 饿汉式（静态代码块）1234567891011121314151617181920212223242526272829303132333435public class SingleTest2 &#123; @Test public void test() &#123; //通过调用静态方法来创建实例 Singleton2 instance = Singleton2.getInstance(); Singleton2 instance1 = Singleton2.getInstance(); //证明不管多少次调用Singleton类同一个实例 System.out.println(instance == instance1);//true &#125;&#125;//饿汉式（静态代码块）//优点：这种写法比较简单，在类装载的时候就完成了实例化，避免了线程同步的问题//缺点：没有实现lazy loading的效果，实例创建可能不会被使用，导致内存的浪费class Singleton2 &#123; //私有化构造方法，目的：不让外界随便创建实例 private Singleton2() &#123; &#125; //在本类中声明实例 private static Singleton2 instance; static &#123; //在静态代码块中实例化对象 instance = new Singleton2(); &#125; //对外界提供公共的静态方法，返回实例的对象 public static Singleton2 getInstance() &#123; return instance; &#125;&#125; 单例设计模式（懒汉式线程不安全）123456789101112131415161718192021222324252627public class SingleDemo2 &#123; @Test public void test() &#123; //通过调用静态方法来创建实例 Singleton instance = Singleton.getInstance(); Singleton instance1 = Singleton.getInstance(); //证明不管多少次调用Singleton类同一个实例 System.out.println(instance == instance1);//true &#125;&#125;//懒汉式（多线程不安全）class Singleton &#123; private static Singleton instance; //私有化构造方法 private Singleton() &#123; &#125; //需要时再创建对象 public static Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125;&#125; 123456789101112131415161718192021222324252627282930public class SingleDemo2 &#123; @Test public void test() &#123; //通过调用静态方法来创建实例 Singleton instance = Singleton.getInstance(); Singleton instance1 = Singleton.getInstance(); //证明不管多少次调用Singleton类同一个实例 System.out.println(instance == instance1);//true &#125;&#125;//懒汉式（线程安全）//优点：解决了线程安全的问题//缺点：效率太低,不推荐使用class Singleton &#123; private static Singleton instance; //私有化构造方法 private Singleton() &#123; &#125; //需要时再创建对象,提供一个静态的公有方法，加入同步处理的代码，解决线程安全问题 public static synchronized Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125;&#125; 双重检查解决了多线程安全问题12345678910111213141516171819202122232425262728293031323334353637public class SingleDemo2 &#123; @Test public void test() &#123; //通过调用静态方法来创建实例 Singleton instance = Singleton.getInstance(); Singleton instance1 = Singleton.getInstance(); //证明不管多少次调用Singleton类同一个实例 System.out.println(instance == instance1);//true &#125;&#125;// 双重检查 class Singleton &#123;//volatile防止指令重排推荐使用 private static volatile Singleton instance; //私有化构造方法 private Singleton() &#123; &#125; //需要时再创建对象 public static synchronized Singleton getInstance() &#123; //第一次检查 if (instance == null) &#123; synchronized (Singleton.class) &#123; //第二次检查 if (instance == null) &#123; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125;&#125; 静态内部类利用jvm的装载机制，当类加载时静态内部类不会立即被加载，保证了线程安全以及效率的问题1234567891011121314151617181920212223242526272829303132public class SingleDemo2 &#123; @Test public void test() &#123; //通过调用静态方法来创建实例 Singleton instance = Singleton.getInstance(); Singleton instance1 = Singleton.getInstance(); //证明不管多少次调用Singleton类同一个实例 System.out.println(instance == instance1);//true &#125;&#125;//静态内部类class Singleton &#123; private static volatile Singleton instance; //私有化构造方法 private Singleton() &#123; &#125; private static class SingleInstance &#123; private static final Singleton instance = new Singleton(); &#125; //需要时再创建对象,提供一个静态的公有方法，加入同步处理的代码，解决线程安全问题 public static synchronized Singleton getInstance() &#123; return SingleInstance.instance; &#125;&#125; 枚举使用枚举，可以实现单例，推荐使用，还能防止反序列化创建对象123456789101112131415161718public class SingletonTest8 &#123; public static void main(String[] args) &#123; Singleton instance = Singleton.INSTANCE; Singleton instance1 = Singleton.INSTANCE; System.out.println(instance == instance1);//true System.out.println(instance.hashCode()); System.out.println(instance1.hashCode()); &#125;&#125;enum Singleton &#123; INSTANCE;//属性 public void sayOk() &#123; &#125;&#125;","categories":[{"name":"设计模式","slug":"设计模式","permalink":"http://yoursite.com/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]},{"title":"My-first-blog","slug":"My-first-blog","date":"2020-01-28T04:21:00.000Z","updated":"2020-01-28T04:37:19.618Z","comments":true,"path":"2020/01/28/My-first-blog/","link":"","permalink":"http://yoursite.com/2020/01/28/My-first-blog/","excerpt":"","text":"Mac下 hexo博客的搭建安装Node.js这里可以去Nodejs的官网去下载安装 [hyperlink]（https://nodejs.org/en/） 点击downloads 选择合适的版本下载 打开电脑的终端切换到我们的sudo用户，这里是以管理员身份打开 执行代码：sudo su 显示Password：“输入密码” 查看node的版本，npm的版本 执行代码: node -v 执行代码: npm -v 安装hexo的框架这里使用npm包安装管理器来安装一个淘宝的cnpm的镜像源包安装管理器来安装 执行代码：npm install -g cnpm --registry=https://registry.npm.taobao.org 安装完成后可以用代码cnpm 查看版本 用cnpm全局安装hexo博客 执行代码：cnpm install -g hexo-cli 安装完成后用hexo -v查看版本，确认安装完成。 查看当前路径:pwd 新建一个blog的文件夹 ：mkdir blog 注意！！！下面的所有操作都是在blog文件下操作，如果后面发生错误实在找不到删除blog文件夹重新操作即可。 进入blog文件夹，执行命令：cd blog hexo初始化 执行命令sudo hexo init 最后启动hexo blog 执行命令：hexo s 默认在http://localhost:4000/可以打开","categories":[{"name":"前端","slug":"前端","permalink":"http://yoursite.com/categories/%E5%89%8D%E7%AB%AF/"}],"tags":[{"name":"分享","slug":"分享","permalink":"http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"},{"name":"导航","slug":"导航","permalink":"http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"}]}]}