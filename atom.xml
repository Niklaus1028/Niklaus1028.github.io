<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-04-27T09:40:40.222Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Niklaus Yu</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>HBase</title>
    <link href="http://yoursite.com/2020/04/25/HBase/"/>
    <id>http://yoursite.com/2020/04/25/HBase/</id>
    <published>2020-04-25T06:23:09.000Z</published>
    <updated>2020-04-27T09:40:40.222Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><ol><li>HBase是有Apache提供的基于Hadoop的分布式，可扩展的非关系型数据库</li><li>HBase可以管理很大的数据的表 -  billions of rows X millions of columns</li><li>HBase是Doug Cutting根据Google的BigTable来实现，所以HBase和BigTable的原理一模一样，只是BigTable是用C语言实现的，HBase是Java语言实现的</li><li>本身是非关系型数据库，底层是利用键值对来存储</li><li>在HBase中也不支持多表关联</li><li>HBase中数据类型只支持字符串和数字</li><li>适合存储稀疏数据 - 结构化数据和半结构化数据</li><li>在put的时候表名列族以及行键都一样，再一次put是更新的效果</li><li>hbase作为一个数据库提供了完整的增删改查的功能，但是hbase是基于hdfs来进行存储的，hdfs上的数据是允许一次写入多次读取不允许修改允许追加的，hbase如何实现“改”的功能 - 当执行“改”操作的时候，HBase并不是修改原来的数据，而是在HDFS中存储的文件的尾端来追加数据，每一条数据都会自动添加时间戳，这个时间戳就是版本</li><li>通过时间戳，每一次默认返回最新的数据</li><li>HBase中的表默认只保留一个版本的数据，也只返回一个版本的数据，如果保留版本，需要建表时需要手动指定</li></ol><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><ol><li><p>Rowkey - 行键</p><ul><li>在HBase中没有主键的概念，而是采用行键</li><li>行键不属于任意列族</li><li>建表的时候不需要指定行键，行键是在添加数据的时候手动指定</li><li>默人字典排序</li></ul></li><li><p>Column Family - 列族/列族</p><ul><li>在HBase中，一个表中至少包含一个列族，可以多个列族</li><li>理论上来说，列族的数量不限定，但一个表一般不超过三个。</li><li>一个列族中可以包含0到多个列 - HBase，列是可以动态增删的，建表的时候不需要指定列。</li></ul></li><li><p>Cell - 单元</p><ul><li>在HBase中，通过行键+列族+列+版本可以确定唯一数据，这个结构成为cell单元</li></ul></li><li><p>namespace - 名称空间</p><ul><li>类似于mysql中的database</li><li>默认自带两个名称空间default和hbase</li><li>如果没有指定默认使用default</li></ul></li></ol><h3 id="基本指令"><a href="#基本指令" class="headerlink" title="基本指令"></a>基本指令</h3><table><thead><tr><th>命令</th><th>解释</th></tr></thead><tbody><tr><td>status</td><td>查看HBase状态</td></tr><tr><td>version</td><td>查看hbase的版本</td></tr><tr><td>whoami</td><td>查看当前用户</td></tr><tr><td>create ‘person’ ,{NAME =&gt;’basic’},{NAME =&gt; ‘info’},{NAME =&gt; ‘other’}</td><td>建表方式</td></tr><tr><td>List</td><td>查看已经建立的表</td></tr><tr><td>put ‘person’ ,’p1’ , ‘basic:name’ ,’Reason’</td><td>表示向person表中的basic列族name列添加行键p1的数据Reason</td></tr><tr><td>get ‘person’, ‘p1’, ‘basic:name’</td><td>获取basic 列族name列的值</td></tr><tr><td>get ‘person’, ‘p1’, ‘basic’</td><td>获取basic 列族所有值</td></tr><tr><td>get ‘person’ , ‘p1’</td><td>获取行键p1所有的值</td></tr><tr><td>scan ‘person’ ,{COLUMNS =&gt; ‘basic:name’}</td><td>获取basic所有name列的值</td></tr><tr><td>scan ‘person’ ,{COLUMNS =&gt; ‘basic’}</td><td>获取basic列族所有列的值</td></tr><tr><td>scan ‘person’</td><td>查看person表中的所有数据</td></tr><tr><td>delete ‘person’ ,’p2’ ,’other:phone’</td><td>删除行键p2 other列族的phone字段</td></tr><tr><td>deleteall ‘person’ ,’p2’</td><td>删除行键p2的所有数据</td></tr><tr><td>create ‘student’ , {NAME =&gt; ‘basic’ ,VERSIONS =&gt;3} ,{NAME =&gt; ‘info’ ,VERSIONS =&gt;5}</td><td>创建一个student表有basic 和info两个列族，basic保留三个版本，info保留5个版本</td></tr><tr><td>get ‘student’,’s1’ ,{COLUMN =&gt;’basic:age’,VERSIONS =&gt; 4}</td><td>查询student表列族为basic age字段的4个版本的值，但建表的时候指定三个版本，只能拿到最新的三个版本的值</td></tr><tr><td>disable ‘student’</td><td>先禁用表</td></tr><tr><td>drop ‘student’</td><td>再删除表</td></tr><tr><td>describe ‘person’ or  desc ‘person’</td><td>描述person表的信息</td></tr><tr><td>enable ‘person’</td><td>启用person表</td></tr><tr><td>exits ‘person’</td><td>判断person表是否存在</td></tr><tr><td>create ‘demo:person’ ,’basic’ ,’expand’</td><td>在指定工作空间demo下创建person</td></tr><tr><td>list_namespace_tables ‘demo’</td><td>查看指定名称空间的表</td></tr><tr><td>list_namespace</td><td>查看所有的名称空间</td></tr><tr><td>create namespace ‘demo’</td><td>创建名称空间demo</td></tr><tr><td>disable_all ‘demo:.*’</td><td>禁用demo空间下所有的表</td></tr><tr><td>drop_all ‘demo:.*’</td><td>删除demo中所有的表</td></tr><tr><td>drop_namespace ‘demo’</td><td>删除工作空间demo</td></tr></tbody></table><h2 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h2><h3 id="HRegion"><a href="#HRegion" class="headerlink" title="HRegion"></a>HRegion</h3><ol><li>在HBase，会从行键方向上将一个表拆分成一个或者多个HRegion</li><li>每一个HRegion都会交会交由某一个HRegionServer来进行管理</li><li>由于行键是字典排序，所以HRegion之间的范围是不交叉的，也因此客户端在请求的时候会根据行键去访问不同的HRegionServer</li><li>HRegion实际上是行键排序（默认是字典排序）后的按规则分割的连续的存储空间</li></ol><p><img src="/" class="lazyload" data-src="/Users/yuxiangrui/blog/source/picture/%E6%88%AA%E5%B1%8F2020-04-27%E4%B8%8B%E5%8D%885.35.27.png"  alt=""></p><ol start="5"><li>一张Hbase表，可能有多个HRegion，每个HRegion达到一定大小（默认是10GB）时，进行分裂。<br><img src="/" class="lazyload" data-src="/Users/yuxiangrui/blog/source/picture/%E6%88%AA%E5%B1%8F2020-04-27%E4%B8%8B%E5%8D%885.37.21.png"  alt=""></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;h3 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h3&gt;&lt;ol&gt;
&lt;
      
    
    </summary>
    
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="导航" scheme="http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"/>
    
      <category term="分享" scheme="http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"/>
    
  </entry>
  
  <entry>
    <title>Hive</title>
    <link href="http://yoursite.com/2020/04/21/Hive/"/>
    <id>http://yoursite.com/2020/04/21/Hive/</id>
    <published>2020-04-21T14:11:43.660Z</published>
    <updated>2020-04-25T08:34:55.612Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><h3 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h3><ol><li>Apache Hive™数据仓库软件有助于使用SQL读取，写入和管理驻留在分布式存储中的大型数据集。可以将结构投影到已经存储的数据上。提供了命令行工具和JDBC驱动程序以将用户连接到Hive。</li><li>提供了类SQL（HQL）语言来管理HDFS上的数据，底层会将sql转化为MapReduce执行，Hive适用于离线分析</li><li>在Hive中，每一个database在HDFS上对应一个目录</li><li>在Hive中没有主键的概念</li><li>一个表一旦建立之后，这个表中字段之间的间隔符就不能修改了</li></ol><h3 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h3><ol><li>create table p2 like person;  建立一个p2表和person表一样结构</li><li>insert overwrite local directory ‘/home/hivedemo’ row format delimited fields terminated by ‘ ‘  select * from person;  将查询数据传给本地目录中，以空格间隔，要求目录不存在，否则会被覆盖</li></ol><h3 id="表结构"><a href="#表结构" class="headerlink" title="表结构"></a>表结构</h3><h4 id="内部表和外部表"><a href="#内部表和外部表" class="headerlink" title="内部表和外部表"></a>内部表和外部表</h4><h5 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h5><ol><li>内部表:先在hive里建一张表，然后向这个表插入数据(用insert可以插入数据，也可以通过加载外部文件方式来插入数据)，这样的表称之为hive的内部表</li><li>外部表:HDFS里已经有数据了，然后，通过hive创建一张表来管理这个文件数据。则这样表称之为外部表。需要注意的是，hive外部表管理的是HDFS里的某一个目录下的文件数据外部表创建命令<br>进入hive，执行:create external table stu (id int,name string) row format delimited fields terminated by ‘ ‘ location<h5 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h5></li><li>对于内部表，在删除该表的时候，HDFS对应的目录节点会被删除 2. 对于外部表，在删除该表的时候，HDFS对应的目录节点不会删除</li></ol><h3 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h3><ol><li>分区表的作用往往是对数据进行分类</li><li>基本操作<h4 id="建表"><a href="#建表" class="headerlink" title="建表"></a>建表</h4>create table cities(id int, name string) partitioned by(province string) row format delimited fields terminated by ‘ ‘;<h4 id="加载hebei分区数据"><a href="#加载hebei分区数据" class="headerlink" title="加载hebei分区数据"></a>加载hebei分区数据</h4>load data local inpath ‘/home/hivedata/hebei.txt’ into table cities partition(province = ‘hebei’);<h4 id="加载guangdong分区数据"><a href="#加载guangdong分区数据" class="headerlink" title="加载guangdong分区数据"></a>加载guangdong分区数据</h4>load data local inpath ‘/home/hivedata/guangdong.txt’ into table cities partition(province = ‘guangdong’);<h4 id="加载jiangsu分区数据"><a href="#加载jiangsu分区数据" class="headerlink" title="加载jiangsu分区数据"></a>加载jiangsu分区数据</h4>load data local inpath ‘/home/hivedata/jiangsu.txt’ into table cities partition(province = ‘jiangsu’);</li><li>每一个分区对应一个目录</li><li>如果在HDFS上新建了目录作为分区，那么需要在Hive中来手动添加分区<br>alter table cities add partition(province=’shandong’) location ‘/user/hive/warehouse/hivedemo.db/cities/province=shandong’;<h4 id="修复表，但是注意这句SQL执行不稳定，有时候会执行失败"><a href="#修复表，但是注意这句SQL执行不稳定，有时候会执行失败" class="headerlink" title="修复表，但是注意这句SQL执行不稳定，有时候会执行失败"></a>修复表，但是注意这句SQL执行不稳定，有时候会执行失败</h4>msck repair table cities;</li><li>在Hive的分区表中，如果根据分区字段进行查询，那么只查询分区对应的目录从而提高查询效率；如果跨分区查询，查询效率反而会降低</li><li>在分区表中，分区字段在原始数据中是不存在的，是需要在加载数据的时候手动指定的</li><li>如果分区字段在原始文件中已经存在，那么需要进行动态分区</li><li>先建立一张表管理原始数据<br>create table c_tmp(cid int, cname string, cpro string) row format delimited fields terminated by ‘ ‘;</li><li>加载数据<br>load data local inpath ‘/home/hivedata/cities.txt’ into table c_tmp;</li><li>开启动态分区机制 - 从一张未分区表中查询数据放到一张已分区的表中，这个过程称之为动态分区<br>set hive.exec.dynamic.partition.mode=nonstrict;</li><li>指定分区字段进行动态分区<br>insert into table cities partition(province) select cid, cname, cpro from c_tmp distribute by cpro;</li><li>删除分区：alter table cities drop partition(province = ‘liaoning’);</li><li>在Hive中，支持多字段分区，前边的字段会包含后边的字段。指定了几个字段，就会形成一个几级的目录。当需要对数据进行多级分类的时候，那么这个时候可以多字段分区 - 实际开发中，往往是需要对数据进行多级分类<h4 id="建表语句"><a href="#建表语句" class="headerlink" title="建表语句"></a>建表语句</h4>create table product(id int, name string) partitioned by(kind string, sub string, subkind string) row format delimited fields terminated by ‘ ‘;<h4 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h4>load data local inpath ‘/home/hivedata/shirt.txt’ into table product partition(kind = ‘clothes’,  sub=’coat’, subkind=’Tshirt’);<br>load data local inpath ‘/home/hivedata/sports.txt’ into table product partition(kind = ‘clothes’, sub = ‘shoes’, subkind = ‘sports’);<h4 id="分桶表"><a href="#分桶表" class="headerlink" title="分桶表"></a>分桶表</h4></li><li>分桶表的作用是对数据进行抽样</li><li>在Hive中，分桶表默认是不开启的，所以需要手动开启分桶机制</li><li>案例<h4 id="开启分桶机制"><a href="#开启分桶机制" class="headerlink" title="开启分桶机制"></a>开启分桶机制</h4>set hive.enforce.bucketing=true;<h4 id="建立分桶表-表示根据name字段进行分桶-在分桶的时候会计算name字段的哈希码，然后利用哈希码进行二次计算确定放在哪个桶中"><a href="#建立分桶表-表示根据name字段进行分桶-在分桶的时候会计算name字段的哈希码，然后利用哈希码进行二次计算确定放在哪个桶中" class="headerlink" title="建立分桶表 - 表示根据name字段进行分桶 - 在分桶的时候会计算name字段的哈希码，然后利用哈希码进行二次计算确定放在哪个桶中"></a>建立分桶表 - 表示根据name字段进行分桶 - 在分桶的时候会计算name字段的哈希码，然后利用哈希码进行二次计算确定放在哪个桶中</h4>create table c_bucket(id int, name string) clustered by(name) into 6 buckets row format delimited fields terminated by ‘ ‘;<h4 id="向分桶表中添加数据"><a href="#向分桶表中添加数据" class="headerlink" title="向分桶表中添加数据"></a>向分桶表中添加数据</h4>insert overwrite table c_bucket select id, name from cities;<h4 id="抽样"><a href="#抽样" class="headerlink" title="抽样"></a>抽样</h4>select * from c_bucket tablesample (bucket 1 out of 3 on name);</li><li>bucket x out of y：x表示起始桶编号，从1开始计算，y表示的是步长。bucket 1 out of 3表示从第1个桶开始抽取数据，每隔3个桶抽取一次。其中x&lt;=y</li></ol><h3 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h3><ol><li>Hive分为基本类型和复杂类型</li><li>复杂类型</li><li>array：数组类型，对应了Java中的数组或者集合<h4 id="建表-1"><a href="#建表-1" class="headerlink" title="建表"></a>建表</h4><figure class="highlight plain"><figcaption><span>table arr(nums1 array<int>, nums2 array<int>) row format delimited fields terminated by ' ' collection items terminated by ',';```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">#### 加载数据</span><br><span class="line">load data local inpath &#39;&#x2F;home&#x2F;hivedata&#x2F;nums&#39; into table arr;</span><br><span class="line">####  非空查询</span><br><span class="line">select nums1[5] from arr where nums1[5] is not null;</span><br><span class="line">1. map：映射类型。对应了Java中的映射Map</span><br><span class="line">#### 建表</span><br><span class="line">create table infos (id int, info map&lt;string,int&gt;) row format delimited fields terminated by &#39; &#39; map keys terminated by &#39;,&#39;;</span><br><span class="line">#### 加载数据</span><br><span class="line">load data local inpath &#39;&#x2F;home&#x2F;hivedata&#x2F;map.txt&#39; into table infos;</span><br><span class="line">#### 查询tom对应的值</span><br><span class="line">select info[&#39;tom&#39;] from infos where info[&#39;tom&#39;] is not null;</span><br><span class="line">1. struct：结构体类型。对应了Java中的对象</span><br><span class="line">#### 看作一个对象的四个属性来建表</span><br><span class="line">create external table scores(s struct&lt;name:string, chinese:int, math:int, english:int&gt;) row format delimited collection items terminated by &#39; &#39; location &#39;&#x2F;score&#39;;</span><br><span class="line">#### 查询每一个人的语文成绩</span><br><span class="line">select s.chinese from scores;</span><br><span class="line"></span><br><span class="line">### explode</span><br><span class="line">1. explode会将数组中的每一个元素取出来作为单独的一行处理</span><br><span class="line">2. 案例：单词统计</span><br><span class="line">1. 方式一</span><br><span class="line">#### 建表管理HDFS存在的words.txt文件</span><br><span class="line">&#96;&#96;&#96;create external table words(word array&lt;string&gt;) row format delimited collection items terminated by &#39; &#39; location &#39;&#x2F;words&#39;;</span><br></pre></td></tr></table></figure><h4 id="建表存储单词"><a href="#建表存储单词" class="headerlink" title="建表存储单词"></a>建表存储单词</h4>create table ws(w string);<h4 id="将words表中拆分出来的单词查询出来放到ws表中"><a href="#将words表中拆分出来的单词查询出来放到ws表中" class="headerlink" title="将words表中拆分出来的单词查询出来放到ws表中"></a>将words表中拆分出来的单词查询出来放到ws表中</h4>insert into table ws select explode(*) from words;<h4 id="统计每一个单词出现的次数"><a href="#统计每一个单词出现的次数" class="headerlink" title="统计每一个单词出现的次数"></a>统计每一个单词出现的次数</h4>select w, count(w) from ws group by w;</li><li>方式二<h4 id="建表管理HDFS存在的words-txt文件"><a href="#建表管理HDFS存在的words-txt文件" class="headerlink" title="建表管理HDFS存在的words.txt文件"></a>建表管理HDFS存在的words.txt文件</h4><figure class="highlight plain"><figcaption><span>external table words(word array<string>) row format delimited collection items terminated by ' ' location '/words';```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">#### 统计每一个单词出现的次数</span><br><span class="line">select w, count(w) from (select explode(*) w from words)ws group by w;</span><br><span class="line"></span><br><span class="line">### UDF</span><br><span class="line">1. UDF(User-Defined Function)是Hive提供一套用于自定义函数的机制</span><br><span class="line">#### 2. 案例</span><br><span class="line">1. 写一个类继承UDF类，覆盖其中的evaluate方法</span><br><span class="line">2. 打成jar包放到Linux上</span><br><span class="line">3. 在Hive中添加jar包：add jar &#x2F;home&#x2F;hivedata&#x2F;hive-1.0-SNAPSHOT.jar;</span><br><span class="line">4. 定义一个临时函数，并且给函数绑定类：create temporary function repeatstring as &#39;cn.tedu.udf.RepeatString&#39;;</span><br><span class="line"></span><br><span class="line">### join</span><br><span class="line">1. 如果在join的时候没有指定，那么默认就是inner join</span><br><span class="line">2. 案例</span><br><span class="line">#### 先建立orders表</span><br><span class="line">create external table orders(orderid int, orderdate string, productid string, num int) row format delimited fields terminated by &#39; &#39; location &#39;&#x2F;orders&#39;;</span><br><span class="line">#### 建立products表</span><br><span class="line">create external table products(productid int, name string, price double) row format delimited fields terminated by &#39; &#39; location &#39;&#x2F;products&#39;;</span><br><span class="line">#### 统计每一天的售出的商品的总价</span><br><span class="line">select o.orderdate, sum(o.num * p.price) from orders o join products p on o.productid &#x3D; p.productid group by orderdate;</span><br><span class="line">1. Hive中，支持inner&#x2F;left&#x2F;right&#x2F;full outer join，还支持left semi join。如果a left semi join b，那么表示查询a表中哪些数据在b表中出现过：select * from products p left semi join orders o on p.productid &#x3D; o.productid;</span><br><span class="line"></span><br><span class="line">### Serde</span><br><span class="line">1. Serde是Hive提供的一套针对不规则数据进行处理的机制</span><br><span class="line">2. 在Serde中，利用正则的捕获组来对应字段，正则表达式中的每一个捕获组对应了Hive表中的一个字段</span><br><span class="line">3. 案例</span><br><span class="line">#### 建表</span><br><span class="line">&#96;&#96;&#96;create table logs(ip string, time string, timezone string, request_way string, resource string, protocol string, stateid int) row format serde &#39;org.apache.hadoop.hive.serde2.RegexSerDe&#39; with serdeproperties ( &quot;input.regex&quot; &#x3D; &quot;(.*) \-\- \\[(.*) (.*)\\] \&quot;([A-Z]*) (.*) (.*)\&quot; ([0-9]*) \-&quot; ) stored as textfile;</span><br></pre></td></tr></table></figure><h4 id="加载数据-1"><a href="#加载数据-1" class="headerlink" title="加载数据"></a>加载数据</h4>load data local inpath ‘/home/hivedata/server.log’ into table logs;</li></ol><h3 id="beeline"><a href="#beeline" class="headerlink" title="beeline"></a>beeline</h3><ol><li>在实际开发过程中，应该是远程连接公司的Hive服务器，那么此时就需要去进行远程访问 - beeline<h4 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h4> 1.在公司服务器的开启Hive的后台服务进程 <figure class="highlight sh"><figcaption><span>hive --service hiveserver2 &```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">如果开启成功应该出现RunJar进程</span><br><span class="line">1.远程连接公司的Hive服务器</span><br><span class="line">```sh beeline -u jdbc:hive2://10.9.162.133:10000/hivedemo -n root</span><br></pre></td></tr></table></figure></li></ol><h3 id="视图"><a href="#视图" class="headerlink" title="视图"></a>视图</h3><ol><li>一个表中可以包含很多字段，但是每一个字段的使用频率并不是相同的，那么可以考虑将其中比较常用的字段抽取出来形成子表，这个子表可以用视图来表示</li><li>如果将抽取出来的视图存到磁盘上，那么此时称之为是物化视图；如果将抽取出来的视图存到内存中，那么此时称之为是虚拟视图</li><li>数据库中一般支持两种视图，但是Hive中只支持虚拟视图</li><li>建立视图：create view o_view as select orderid, num from orders;这句话表示将orders表中的orderid和num两个字段进行抽取，封装到o_view视图中</li><li>建立视图的时候封装的select语句并没有执行，而是需要在第一次使用视图的时候才会触发这句select</li></ol><h3 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h3><ol><li>数据库中，因为存在主键，所以会自动针对主键建立索引，数据库中采用的索引是B+tree机制。在Hive中，因为不存在主键，所以也不会自动建立索引</li><li>在Hive中可以手动建立索引，并且可以针对任意字段来建立索引</li><li>案例<h4 id="建立索引表"><a href="#建立索引表" class="headerlink" title="建立索引表"></a>建立索引表</h4>create index o_index on table orders(orderid) as ‘org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler’ with deferred rebuild in table order_index;<h4 id="建立索引，索引建立完成之后不会自动更新-即orders表中产生变化的时候索引表不会变"><a href="#建立索引，索引建立完成之后不会自动更新-即orders表中产生变化的时候索引表不会变" class="headerlink" title="建立索引，索引建立完成之后不会自动更新 - 即orders表中产生变化的时候索引表不会变"></a>建立索引，索引建立完成之后不会自动更新 - 即orders表中产生变化的时候索引表不会变</h4>alter index o_index on orders rebuild;<h4 id="查询索引数据"><a href="#查询索引数据" class="headerlink" title="查询索引数据"></a>查询索引数据</h4>select * from order_index;<h4 id="删除索引"><a href="#删除索引" class="headerlink" title="删除索引"></a>删除索引</h4>drop index o_index on orders;</li><li>如果一个表比较常用，那么可以针对这个表中的常用的字段来建立索引而并不是针对所有字段/表建立索引</li></ol><h3 id="元数据"><a href="#元数据" class="headerlink" title="元数据"></a>元数据</h3><ol><li>用于描述数据的数据就是元数据</li><li>Hive中的元数据包括database名、table名、字段名、分区、分桶信息等都属于元数据<br>Hive中的元数据默认是存储在关系型数据中的，目前支持的元数据的数据库是Derby和MySQL，如果不指定默认使用的是Derby。因为Derby是单连接数据库，所以需要将Hive的元数据库更换为MySQL</li></ol><h3 id="调优"><a href="#调优" class="headerlink" title="调优"></a>调优</h3><h4 id="map-side-join："><a href="#map-side-join：" class="headerlink" title="map side join："></a>map side join：</h4><ol><li>如果一个小表和一个大表进行join查询，那么可以考虑将小表放在缓存中然后处理大表，如果需要用到小表的数据可以从内存中查询</li><li>需要执行set hive.auto.convert.join=true;才会开启小表的缓存机制</li><li>默认要求小表要放在join的左边，即小表 join 大表</li><li>默认只要小表不超过25M，那么就会放入内存中，这个大小可以通过hive.mapjoin.smalltable.filesize来设置<h4 id="如果在join的时候附带了查询条件，那么考虑先用子查询执行where来降低数据量，然后再进行join"><a href="#如果在join的时候附带了查询条件，那么考虑先用子查询执行where来降低数据量，然后再进行join" class="headerlink" title="如果在join的时候附带了查询条件，那么考虑先用子查询执行where来降低数据量，然后再进行join"></a>如果在join的时候附带了查询条件，那么考虑先用子查询执行where来降低数据量，然后再进行join</h4></li><li>group by在有多个ReduceTask的情况下可能会产生Reduce端的数据倾斜，可以通过二阶段聚合解决这个问题。在Hive中可以通过set hive.groupby.skewindata=true;来自动开启二阶段聚合</li><li>distinct和聚合函数的优化，聚合函数最终只能利用一个ReduceTask来进行计算，所以如果distinct和聚合函数同时使用，那么考虑先用子查询来进行去重，最后再进行聚合</li><li>调整切片数：如果数据结构相对比较复杂或者处理逻辑相对比较复杂，那么可以考虑将切片调小来增多MapTask的数量；如果数据结构相对比较简单或者处理逻辑相对比较简单，那么可以考虑将切片调大来减少MapTask的数量 - 任务复杂就增多线程，任务简单就减少线程</li></ol><h3 id="数据处理流程"><a href="#数据处理流程" class="headerlink" title="数据处理流程"></a>数据处理流程</h3><h3 id="数据仓库和数据库"><a href="#数据仓库和数据库" class="headerlink" title="数据仓库和数据库"></a>数据仓库和数据库</h3><table><thead><tr><th></th><th>数据库</th><th>数据仓库</th></tr></thead><tbody><tr><td>数据量</td><td>&lt;=GB</td><td>&gt;=TB</td></tr><tr><td>来源</td><td>相对比较单一，往往来源于单一的web应用</td><td>来源相对丰富，可以是日志，爬虫，网页埋点等</td></tr><tr><td>种类</td><td>数据结构相对单一 - 结构化数据</td><td>数据结构相对丰富 - 结构化数据，半结构化数据，非结构化数据</td></tr><tr><td>操作</td><td>提供了完整的增删改查的能力</td><td>往往只提供增和查的操作而不是提供删和改的操作</td></tr><tr><td>事务</td><td>强调事务，存在ACID四个特性</td><td>不强调事务，往往是若事务甚至不支持事务</td></tr><tr><td>冗余</td><td>精简，尽量避免冗余</td><td>人为制造冗余 - 副本</td></tr><tr><td>数据场景</td><td>往往是用于线上实时捕获数据</td><td>离线历史数据和线上实时处理</td></tr><tr><td>系统</td><td>OLTP - Online Transaction Processor 联机事务处理 - 强调事务的完整性</td><td>OLAP - Online Analysis Processor - 联机分析处理 - 强调数据分析过程</td></tr><tr><td>面向对象</td><td>程序员，DBA，运维等技术人员</td><td>面向领导，市场，客户，销售等非技术人员，辅助决策</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;h3 id=&quot;一、概述&quot;&gt;&lt;a href=&quot;#一、概述&quot; class=&quot;headerlink&quot; title=&quot;一、概述&quot;&gt;&lt;/a&gt;一、概述&lt;/h
      
    
    </summary>
    
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="导航" scheme="http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"/>
    
      <category term="分享" scheme="http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"/>
    
  </entry>
  
  <entry>
    <title>Flume</title>
    <link href="http://yoursite.com/2020/04/18/Flume/"/>
    <id>http://yoursite.com/2020/04/18/Flume/</id>
    <published>2020-04-17T16:00:00.000Z</published>
    <updated>2020-04-22T01:55:18.847Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Flume"><a href="#Flume" class="headerlink" title="Flume"></a>Flume</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><ol><li>Flume是一种分布式，可靠且可用的服务，用于有效地<strong>收集，聚合和移动大量日志数据</strong>。它具有基于流数据流的简单灵活的体系结构。它具有可调整的可靠性机制以及许多故障转移和恢复机制，具有强大的功能和容错能力。它使用一个简单的可扩展数据模型，允许在线分析应用程序。</li><li>Flume最主要的作用就是，实时读取服务器本地磁盘的数据，将数据写到HDFS中 </li><li>Flume的版本更新相对稳定：</li></ol><ul><li>Flume1.0:Flume-ng Flume-ng和Flume-og不兼容。现在开发过程中一般使用的是Flume-ng</li><li>Flume0.9:Flume-og。Flume-og是单线程执行任务</li></ul><h3 id="数据流模型"><a href="#数据流模型" class="headerlink" title="数据流模型"></a>数据流模型</h3><h4 id="单级流动"><a href="#单级流动" class="headerlink" title="单级流动"></a>单级流动</h4><p><img src="/" class="lazyload" data-src="http://flume.apache.org/_images/DevGuide_image00.png"  alt=""></p><h4 id="多剂流"><a href="#多剂流" class="headerlink" title="多剂流"></a>多剂流</h4><p><img src="/" class="lazyload" data-src="http://flume.apache.org/_images/UserGuide_image03.png"  alt=""></p><h4 id="合并"><a href="#合并" class="headerlink" title="合并"></a>合并</h4><p><img src="/" class="lazyload" data-src="http://flume.apache.org/_images/UserGuide_image02.png"  alt=""></p><h4 id="复流"><a href="#复流" class="headerlink" title="复流"></a>复流</h4><p><img src="/" class="lazyload" data-src="http://flume.apache.org/_images/UserGuide_image01.png"  alt=""></p><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><ol><li>Event：</li></ol><ul><li>传输单元，Flume传输数据的基本单元，以Event的形式将数据从源头送至目的地。Event由Header和Body两部分组成，Header用来存放该evevt的一些属性，为K-V结构，Body用来存放该条数据，形式为字节数组。</li><li>一个Event就是一条日志，即在Flume中会将收集到的每一条日志封装成一个Event</li><li>Event本质上是一个json串，即Flume将收集到的日志封装成json，这个Event固定包含header和body两个部分</li></ul><ol start="2"><li>Agent：是Flume中构成流动模型的基本单位</li></ol><ul><li>Source： 从数据源采集数据</li><li>Channel ：临时存储数据</li><li>Sink： 将数据写到目的地</li></ul><h3 id="Source"><a href="#Source" class="headerlink" title="Source"></a>Source</h3><ol><li>AVRO Source：接收被AVRO序列化之后的数据，可以结合AVRO Sink可以实现多级、扇入、扇出流动</li><li>Exec Source：将命令的执行结果作为日志进行收集</li><li>NetCat Source：监听指定的端口接收TCP请求，将TCP请求的数据作为日志进行收集</li><li>Sequence Generator Source：不断产生递增的数字</li><li>Spooling Directory Source：监听指定的目录，如果监听的目录中产生了新的文件，那么会自动收集新文件中的内容</li><li>HTTP Source：监听HTTP请求，只能监听GET和POST请求，但是实际使用过程中，对GET请求的监听不稳定，所以一般只用这个Source来监听POST请求</li><li>扩展：自定义Source：如果实际生产中遇到的场景，Flume提供的Source无法解决，就需要自定义Source。如果需要自定义Source，需要写一个类实现EventDrivenSource/PollableSource</li></ol><ul><li>EventDrivenSource：事件驱动Source，被动型Source。需要自己定义线程获取数据发送数据</li><li>PollableSource：拉取Source，主动型Source。已经提供了预定义的线程来获取和发送数据</li></ul><h3 id="Channel"><a href="#Channel" class="headerlink" title="Channel"></a>Channel</h3><ol><li>Memory Channel:内存通道。将Source收集到的数据临时存储在内存中。Memory Channel的读写速度相对较快，但并不可靠，适合于高并发不要求可靠性的场景</li><li>File Channel ： 文件通道, 将Source收集到的数据临时存储在磁盘上。File Channel的读写速度相对比较慢，但是可靠，适用于要求可靠性但是传输速度相对较低的场景</li></ol><h3 id="Sink"><a href="#Sink" class="headerlink" title="Sink"></a>Sink</h3><ol><li>Logger Sink：将收集到的日志以指定的日志级别来进行记录或者打印。在记录的时候为了防止内容过多将屏幕占满，所以一旦数据超过16个字节那么就会自动省略后边的内容</li><li>File_roll Sink：将收集到的数据存放到指定的目录中。如果不指定，那么会在指定的目录下，默认每隔30s生成一个小文件，致使生成大量的小文件，所以实际开发中会将这个属性来进行设置</li><li>HDFS Sink：将收集到的数据写到HDFS上。如果不指定，会在HDFS上每隔30s生成一个小文件</li><li>AVRO Sink：将数据用AVRO机制进行序列化之后传输到下一个节点上。AVRO Sink结合AVRO Source实现多级、扇入、扇出流动效果</li><li>扩展：自定义Sink。如果实际生产过程中，需要按照指定格式将数据输出，这个时候就需要使用自定义Sink。如果需要自定义Sink，那么需要写一个类实现Sink、Configurable接口</li></ol><h3 id="组件"><a href="#组件" class="headerlink" title="组件"></a>组件</h3><ol><li>Selector - 选择器</li></ol><ul><li>Selector是Source的子组件</li><li>Selector存在2种模式：replicating和multiplexing<ul><li>replicating：复制模式。一个节点在收到数据之后，会将数据进行复制然后分发给每一个节点，从而导致每一个节点收到的数据都是相同的</li><li>multiplexing：路由模式/多路复用模式。监听headers中指定的字段的值，根据值来确定发往哪个节点，所以每一个节点收到的数据是不同的</li><li>如果不配置，默认是复制模式</li></ul></li></ul><ol start="2"><li>Interceptor - 拦截器</li></ol><ul><li>Interceptor是Source的子组件</li><li>Interceptor可以有多个，构成拦截器链</li></ul><ol start="3"><li>常用的Interceptor：</li></ol><ul><li>Timestamp：在数据的headers中添加一个时间戳字段，记录数据收集的时间。结合HDFS Sink实现日志的按天收集和存放</li><li>Host：在数据的headers中添加一个host字段，记录数据来源的主机</li><li>Static：在数据的headers中添加一个指定的字段</li><li>UUID：在数据的headers中添加一个id字段用于标记唯一性</li><li>Search and Replace：利用正则来搜索数据中符合格式的数据，然后将数据替换为指定的符号</li><li>Regex Filtering：利用正则来对数据进行过滤。只要数据符合正则的指定格式就会被拦截</li></ul><h3 id="Sink-Group"><a href="#Sink-Group" class="headerlink" title="Sink Group"></a>Sink Group</h3><h4 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h4><ol><li>只接受一个 Sink</li><li>这是默认的策略。即如果不配置Processor，用的是这个策略</li><li>模式：复制模式，<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a1.sinkgroups &#x3D; g1</span><br><span class="line">a1.sinkgroups.g1.sinks &#x3D; k1 k2 a1.sinkgroups.g1.processor.type &#x3D; failover a1.sinkgroups.g1.processor.priority.k1 &#x3D; 5 a1.sinkgroups.g1.processor.priority.k2 &#x3D; 10 a1.sinkgroups.g1.processor.maxpenalty &#x3D; 10000</span><br></pre></td></tr></table></figure><h3 id="Failover-Sink-Processor"><a href="#Failover-Sink-Processor" class="headerlink" title="Failover Sink Processor"></a>Failover Sink Processor</h3><h4 id="一、概述-1"><a href="#一、概述-1" class="headerlink" title="一、概述"></a>一、概述</h4></li><li>维护一个sink们的优先表。确保只要一个是可用的就事件就可以被处理</li><li>失败处理原理是，为失效的sink指定一个冷却时间，在冷却时间到达后再重新使用</li><li>sink们可以被配置一个优先级，数字越大优先级越高</li><li>如果sink发送事件失败，则下一个最高优先级的sink将会尝试接着发送事件</li><li>如果没有指定优先级，则优先级顺序取决于sink们的配置顺序，先配置的默认优先级高于后配置的</li><li>在配置的过程中，设置一个group processor ，并且为每个sink都指定一个优先级</li><li>优先级必须是唯一的</li><li>另外可以设置maxpenalty属性指定限定失败时间</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Flume&quot;&gt;&lt;a href=&quot;#Flume&quot; class=&quot;headerlink&quot; title=&quot;Flume&quot;&gt;&lt;/a&gt;Flume&lt;/h2&gt;&lt;h3 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概
      
    
    </summary>
    
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="导航" scheme="http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"/>
    
      <category term="分享" scheme="http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"/>
    
  </entry>
  
  <entry>
    <title>Mybatis</title>
    <link href="http://yoursite.com/2020/04/17/Mybatis/"/>
    <id>http://yoursite.com/2020/04/17/Mybatis/</id>
    <published>2020-04-16T16:00:00.000Z</published>
    <updated>2020-04-17T16:16:34.629Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是-Mybatis"><a href="#什么是-Mybatis" class="headerlink" title="什么是 Mybatis?"></a>什么是 Mybatis?</h2><ol><li>Mybatis是一个半ORM（对象关系映射）的框架，它内部封装了JDBC，开发时只需要关注sql语句本身，不需要花费精力去处理加载驱动、创建连接，创建statement等复杂过程。程序员直接编写原生态的sql，严格控制sql的执行性能，灵活度高。</li><li>Mybatis可以通过xml和注解的方式来配置和映射原生信息，将普通JavaBeans映射成数据库中的记录，避免了几乎所有的JDBC代码和手动设置参数以及获取结果集。</li><li>通过xml文件或者注解的方式执行各种statement配置起来，并通过java对象和statement中的sql的动态参数进行映射生成最终执行的sql语句，最后由mybatis框架执行sql并将结果映射为java对象并返回。（从执行sql到返回result的过程）</li></ol><h2 id="Mybaits-的优点"><a href="#Mybaits-的优点" class="headerlink" title="Mybaits 的优点:"></a>Mybaits 的优点:</h2><ol><li>基于sql语句编程相当的灵活，不会对应用程序或者数据库的现有设计造成任何的影响，SQL写在xml中，解决了sql与程序代码的耦合，便于统一管理；提供xml标签，支持编写动态的sql语句，并可重用。</li><li>与JDBC相比，减少了50%以上的代码量，消除了JDBC大量冗余的代码，不需要手动的建立连接和关闭连接</li><li>很好的与各种数据库兼容（因为MyBatis使用JDBC连接数据库，所以只要JDBC支持的数据库mybatis都支持</li><li>能够与Spring很好的集成；</li><li>提供映射标签，支持对象与数据库的ORM字段关系映射，提供对象关系映射标签支持对象关系组件映射</li></ol><h2 id="Mybatis框架的缺点"><a href="#Mybatis框架的缺点" class="headerlink" title="Mybatis框架的缺点"></a>Mybatis框架的缺点</h2><ol><li>sql语句的编写工作量较大，一些关联查询，多表查询，需要对sql语句进行优化，对开发人员的sql功底有一定的要求</li><li>sql语句依赖于数据库，导致数据库的移植性差，不能随意的更换数据库</li></ol><h2 id="MyBatis-框架适用场合"><a href="#MyBatis-框架适用场合" class="headerlink" title="MyBatis 框架适用场合:"></a>MyBatis 框架适用场合:</h2><ol><li>Mybatis只关注sql的本身，是一个足够灵活的DAO层解决方案</li><li>对性能要求很高，或者需求变化较多的项目，如互联网项目，mybatis将是不错的选择</li></ol><h2 id="MyBatis-与-Hibernate-有哪些不同"><a href="#MyBatis-与-Hibernate-有哪些不同" class="headerlink" title="MyBatis 与 Hibernate 有哪些不同?"></a>MyBatis 与 Hibernate 有哪些不同?</h2><ol><li>mybatis时半对象关系映射的框架，它需要开发人员自己编写sql语句</li><li>Mybatis直接编写原生态sql，严格控制sql的执行性能，灵活度高，非常适合对关系数据模型要求不高的软件开发，因为这类软件需求变化频繁，一但需求变化要求迅速输出成果。但灵活的前提是mybatis无法做到数据库无关性，如果需要实现支持多种数据库的软件，则需要自定义多套sql映射文件，工作量大。</li></ol><h2 id="和-的区别是什么"><a href="#和-的区别是什么" class="headerlink" title="{}和${}的区别是什么?"></a>{}和${}的区别是什么?</h2><ol><li>#{}是预编译处理，${}是字符串替换</li><li>Mybatis在处理#{}时，会将#{}替换成？号，调用PrepareStatement的set方法来赋值；</li><li>Mybatis在处理${},就是把${}替换成变量的值没有预编译的过程</li><li>使用#{}可以有效的防止sql注入，提高系统的安全性</li></ol><h2 id="当实体类中的属性名和表中的字段名不一样-，怎么办"><a href="#当实体类中的属性名和表中的字段名不一样-，怎么办" class="headerlink" title="当实体类中的属性名和表中的字段名不一样 ，怎么办 ?"></a>当实体类中的属性名和表中的字段名不一样 ，怎么办 ?</h2><p>第1种：通过在查询的sql语句中定义字段名的别名，让字段名的别名和实体类的属性名一致。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">select</span> <span class="attr">id</span>=<span class="string">"getOrder"</span> <span class="attr">parameterType</span>=<span class="string">"int"</span> <span class="attr">resultMap</span>=<span class="string">"orderresultmap"</span>&gt;</span></span><br><span class="line">select * from orders where order_id=#&#123;id&#125; <span class="tag">&lt;/<span class="name">select</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">resultMap</span> <span class="attr">type</span>=<span class="string">"me.gacl.domain.order"</span> <span class="attr">id</span>=<span class="string">"orderresultmap"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">!–用</span> <span class="attr">id</span> 属性来映射主键字段–&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">id</span> <span class="attr">property</span>=<span class="string">"id"</span> <span class="attr">column</span>=<span class="string">"order_id"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">!–用</span> <span class="attr">result</span> 属性来映射非主键字段，<span class="attr">property</span> 为实体类属性名，<span class="attr">column</span> 为数据表中的属性–&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">result</span> <span class="attr">property</span> = <span class="string">“orderno</span>" <span class="attr">column</span> =<span class="string">”order_no</span>"/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">result</span> <span class="attr">property</span>=<span class="string">"price"</span> <span class="attr">column</span>=<span class="string">"order_price"</span> /&gt;</span> <span class="tag">&lt;/<span class="name">reslutMap</span>&gt;</span></span><br></pre></td></tr></table></figure><p>第二种通过<code>&lt;resultMap&gt;</code>来映射字段名和实体类属性名一一对应的关系 </p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">select</span> <span class="attr">id</span>=<span class="string">”selectorder”</span> <span class="attr">parametertype</span>=<span class="string">”int”</span> <span class="attr">resultetype</span>=<span class="string">”</span> <span class="attr">me.gacl.domain.order</span>”&gt;</span></span><br><span class="line">select order_id id, order_no orderno ,order_price price form orders where order_id=#&#123;id&#125;;</span><br><span class="line"><span class="tag">&lt;/<span class="name">select</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="模糊查询-like-语句该怎么写"><a href="#模糊查询-like-语句该怎么写" class="headerlink" title="模糊查询 like 语句该怎么写?"></a>模糊查询 like 语句该怎么写?</h2><ul><li>第 1 种:在 Java 代码中添加 sql 通配符。<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">string wildcardname = “%smi%”;</span><br><span class="line">list<span class="tag">&lt;<span class="name">name</span>&gt;</span> names = mapper.selectlike(wildcardname);</span><br><span class="line"><span class="tag">&lt;<span class="name">select</span> <span class="attr">id</span>=<span class="string">”selectlike”</span>&gt;</span></span><br><span class="line">select * from foo where bar like #&#123;value&#125;</span><br></pre></td></tr></table></figure></li><li>第二种：在sql语句中拼接通配符，会引起sql注入<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">string wildcardname = “smi”;</span><br><span class="line">list<span class="tag">&lt;<span class="name">name</span>&gt;</span> names = mapper.selectlike(wildcardname);</span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">select</span> <span class="attr">id</span>=<span class="string">”selectlike”</span>&gt;</span></span><br><span class="line">select * from foo where bar like "%"#&#123;value&#125;"%" <span class="tag">&lt;/<span class="name">select</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="通常一个-Xml-映射文件，都会写一个-Dao-接口与之对应，-请问，这个-Dao-接口的工作原理是什么-Dao-接口里的方法，-参数不同时，方法能重载吗"><a href="#通常一个-Xml-映射文件，都会写一个-Dao-接口与之对应，-请问，这个-Dao-接口的工作原理是什么-Dao-接口里的方法，-参数不同时，方法能重载吗" class="headerlink" title="通常一个 Xml 映射文件，都会写一个 Dao 接口与之对应， 请问，这个 Dao 接口的工作原理是什么?Dao 接口里的方法， 参数不同时，方法能重载吗?"></a>通常一个 Xml 映射文件，都会写一个 Dao 接口与之对应， 请问，这个 Dao 接口的工作原理是什么?Dao 接口里的方法， 参数不同时，方法能重载吗?</h2></li><li>Dao接口即Mapper接口，接口的全限名，就是映射文件中namespace的值；接口的方法名，就是映射文件中Mapper的StateMent的id值，接口方法内的参数，就是传递给 sql 的参数。</li><li>Mapper 接口是没有实现类的，当调用接口方法时，接口全限名+方法名拼接字符 串作为 key 值，可唯一定位一个 MapperStatement。在 Mybatis 中，每一个 <code>&lt;select&gt;、&lt;insert&gt;、&lt;update&gt;、&lt;delete&gt;</code>标签，都会被解析为一个 MapperStatement 对象。</li><li>Mapper 接口里的方法，是不能重载的，因为是使用 全限名+方法名 的保存和寻 找策略。Mapper 接口的工作原理是 JDK 动态代理，Mybatis 运行时会使用 JDK 动态代理为 Mapper 接口生成代理对象 proxy，代理对象会拦截接口方法，转而 执行 MapperStatement 所代表的 sql，然后将 sql 执行结果返回。</li></ul><h2 id="Mybatis-是如何进行分页的-分页插件的原理是什么"><a href="#Mybatis-是如何进行分页的-分页插件的原理是什么" class="headerlink" title="Mybatis 是如何进行分页的?分页插件的原理是什么?"></a>Mybatis 是如何进行分页的?分页插件的原理是什么?</h2><ul><li>Mybatis 使用 RowBounds 对象进行分页，它是针对 ResultSet 结果集执行的内 存分页，而非物理分页。可以在 sql 内直接书写带有物理分页的参数来完成物理分 页功能，也可以使用分页插件来完成物理分页。</li><li>分页插件的基本原理是使用 Mybatis 提供的插件接口，实现自定义插件，在插件 的拦截方法内拦截待执行的 sql，然后重写 sql，根据 dialect 方言，添加对应的物 理分页语句和物理分页参数。</li></ul><h2 id="Mybatis-是如何将-sql-执行结果封装为目标对象并返回的-都有哪些映射形式"><a href="#Mybatis-是如何将-sql-执行结果封装为目标对象并返回的-都有哪些映射形式" class="headerlink" title="Mybatis 是如何将 sql 执行结果封装为目标对象并返回的? 都有哪些映射形式?"></a>Mybatis 是如何将 sql 执行结果封装为目标对象并返回的? 都有哪些映射形式?</h2><ol><li>使用<code>&lt;resultMap&gt;</code>标签，逐一定义数据库列名和对象属性名之间的映 射关系。</li><li>使用 sql 列的别名功能，将列的别名书写为对象属性名。</li><li>有了列名与属性名的映射关系后，Mybatis 通过反射创建对象，同时使用反射给 对象的属性逐一赋值并返回，那些找不到映射关系的属性，是无法完成赋值的。</li></ol><h2 id="如何执行批量插入"><a href="#如何执行批量插入" class="headerlink" title="如何执行批量插入?"></a>如何执行批量插入?</h2><p>首先,创建一个简单的 insert 语句:</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">insert</span> <span class="attr">id</span>=<span class="string">”insertname”</span>&gt;</span></span><br><span class="line">insert into names (name) values (#&#123;value&#125;) <span class="tag">&lt;/<span class="name">insert</span>&gt;</span></span><br></pre></td></tr></table></figure><p>然后在 java 代码中像下面这样执行批处理插入:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">list &lt; string &gt; names = <span class="keyword">new</span> arraylist(); names.add(“fred”); names.add(“barney”); names.add(“betty”); names.add(“wilma”);</span><br><span class="line"><span class="comment">// 注意这里 executortype.batch</span></span><br><span class="line">sqlsession sqlsession = sqlsessionfactory.opensession(executortype.batch); <span class="keyword">try</span> &#123;</span><br><span class="line">namemapper mapper = sqlsession.getmapper(namemapper<span class="class">.<span class="keyword">class</span>)</span>; <span class="keyword">for</span> (string name: names) &#123;</span><br><span class="line">mapper.insertname(name); &#125;</span><br><span class="line">sqlsession.commit(); </span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">sqlSession.rollback();</span><br><span class="line"><span class="keyword">throw</span> e; &#125;</span><br><span class="line"><span class="keyword">finally</span> &#123; sqlsession.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="如何获取自动生成的-主-键值"><a href="#如何获取自动生成的-主-键值" class="headerlink" title="如何获取自动生成的(主)键值?"></a>如何获取自动生成的(主)键值?</h2><ul><li>insert 方法总是返回一个 int 值 ，这个值代表的是插入的行数。<br>如果采用自增长策略，自动生成的键值在 insert 方法执行完后可以被设置到传入 的参数对象中。<br>示例：<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;insert id=”insertname” usegeneratedkeys=”<span class="keyword">true</span>” keyproperty=” id”&gt;</span><br><span class="line"><span class="function">insert into <span class="title">names</span> <span class="params">(name)</span> <span class="title">values</span> <span class="params">(#&#123;name&#125;)</span></span></span><br><span class="line"><span class="function">&lt;/insert&gt;</span></span><br><span class="line"><span class="function">name name </span>= <span class="keyword">new</span> name(); name.setname(“fred”);</span><br><span class="line"><span class="keyword">int</span> rows = mapper.insertname(name);</span><br><span class="line"><span class="comment">// 完成后,id 已经被设置到对象中</span></span><br><span class="line">system.out.println(“rows inserted = ” + rows); </span><br><span class="line">system.out.println(“generated key value = ” + name.getid());</span><br></pre></td></tr></table></figure><h2 id="在-mapper-中如何传递多个参数"><a href="#在-mapper-中如何传递多个参数" class="headerlink" title="在 mapper 中如何传递多个参数?"></a>在 mapper 中如何传递多个参数?</h2></li></ul><p>1.第一种： DAO层的函数</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">UserselectUser</span><span class="params">(String name,String area)</span></span>;</span><br><span class="line"><span class="comment">//对应的 xml,#&#123;0&#125;代表接收的是 dao 层中的第一个参数，#&#123;1&#125;代表 dao 层中第二 参数，更多参数一致往后加即可。</span></span><br><span class="line">&lt;select id="selectUser"resultMap="BaseResultMap"&gt; select * fromuser_user_t whereuser_name = #&#123;0&#125;</span><br><span class="line">anduser_area=#&#123;1&#125;</span><br><span class="line">&lt;/select&gt;</span><br></pre></td></tr></table></figure><ol start="2"><li>第二种：使用 @param 注解:<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">usermapper</span> </span>&#123;</span><br><span class="line"><span class="function">user <span class="title">selectuser</span><span class="params">(@param(“username”)</span> string</span></span><br><span class="line"><span class="function">username,@<span class="title">param</span><span class="params">(“hashedpassword”)</span> string hashedpassword)</span>; &#125;</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">select</span> <span class="attr">id</span>=<span class="string">”selectuser”</span> <span class="attr">resulttype</span>=<span class="string">”user”</span>&gt;</span> select id, username, hashedpassword</span><br><span class="line">from some_table</span><br><span class="line">where username = #&#123;username&#125;</span><br><span class="line">and hashedpassword = #&#123;hashedpassword&#125;</span><br><span class="line"><span class="tag">&lt;/<span class="name">select</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li>第三种:多个参数封装成 map<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line"><span class="comment">//映射文件的命名空间.SQL 片段的 ID，就可以调用对应的映射文件中的</span></span><br><span class="line">SQL</span><br><span class="line"><span class="comment">//由于我们的参数超过了两个，而方法中只有一个 Object 参数收集，因此</span></span><br><span class="line">我们使用 Map 集合来装载我们的参数</span><br><span class="line">Map &lt; String, Object &gt; map = <span class="keyword">new</span> HashMap(); map.put(<span class="string">"start"</span>, start);</span><br><span class="line">map.put(<span class="string">"end"</span>, end);</span><br><span class="line"><span class="keyword">return</span> sqlSession.selectList(<span class="string">"StudentID.pagination"</span>, map);</span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123; </span><br><span class="line">e.printStackTrace(); </span><br><span class="line">sqlSession.rollback();</span><br><span class="line"><span class="keyword">throw</span> e;</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123; </span><br><span class="line">MybatisUtil.closeSqlSession();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Mybatis-动态-sql-有什么用-执行原理-有哪些动态-sql"><a href="#Mybatis-动态-sql-有什么用-执行原理-有哪些动态-sql" class="headerlink" title="Mybatis 动态 sql 有什么用?执行原理?有哪些动态 sql?"></a>Mybatis 动态 sql 有什么用?执行原理?有哪些动态 sql?</h2></li><li>Mybatis 动态 sql 可以在 Xml 映射文件内，以标签的形式编写动态 sql，执行原理 是根据表达式的值 完成逻辑判断并动态拼接 sql 的功能。</li><li>Mybatis 提供了 9 种动态 sql 标签:trim | where | set | foreach | if | choose | when | otherwise | bind。</li></ol><h2 id="Xml-映射文件中，除了常见的-select-insert-updae-delete-标签之外，还有哪些标签"><a href="#Xml-映射文件中，除了常见的-select-insert-updae-delete-标签之外，还有哪些标签" class="headerlink" title="Xml 映射文件中，除了常见的 select|insert|updae|delete 标签之外，还有哪些标签?"></a>Xml 映射文件中，除了常见的 select|insert|updae|delete 标签之外，还有哪些标签?</h2><p>答:<code>&lt;resultMap&gt;、&lt;parameterMap&gt;、&lt;sql&gt;、&lt;include&gt;、 &lt;selectKey&gt;，加上动态 sql 的 9 个标签，其中&lt;sql&gt;为 sql 片段标签，通过 &lt;include&gt;标签引入 sql 片段，&lt;selectKey&gt;为不支持自增的主键生成策略标 签。</code></p><h2 id="Mybatis-的-Xml-映射文件中，不同的-Xml-映射文件，id-是否可以重复"><a href="#Mybatis-的-Xml-映射文件中，不同的-Xml-映射文件，id-是否可以重复" class="headerlink" title="Mybatis 的 Xml 映射文件中，不同的 Xml 映射文件，id 是否可以重复?"></a>Mybatis 的 Xml 映射文件中，不同的 Xml 映射文件，id 是否可以重复?</h2><ul><li>不同的 Xml 映射文件，如果配置了 namespace，那么 id 可以重复;如果没有配 置 namespace，那么 id 不能重复;</li><li>原因就是 namespace+id 是作为 Map&lt;String, MapperStatement&gt;的 key 使用的，如果没有 namespace，就剩下 id，那么，id 重复会导致数据互相覆盖。 有了 namespace，自然 id 就可以重复，namespace 不同，namespace+id 自然 也就不同。</li></ul><h2 id="为什么说-Mybatis-是半自动-ORM-映射工具-它与全自动-的区别在哪里"><a href="#为什么说-Mybatis-是半自动-ORM-映射工具-它与全自动-的区别在哪里" class="headerlink" title="为什么说 Mybatis 是半自动 ORM 映射工具?它与全自动 的区别在哪里?"></a>为什么说 Mybatis 是半自动 ORM 映射工具?它与全自动 的区别在哪里?</h2><p>Hibernate 属于全自动 ORM 映射工具，使用 Hibernate 查询关联对象或者关联 集合对象时，可以根据对象关系模型直接获取，所以它是全自动的。而 Mybatis 在查询关联对象或关联集合对象时，需要手动编写 sql 来完成，所以，称之为半自 动 ORM 映射工具。</p><h2 id="一对一、一对多的关联查询"><a href="#一对一、一对多的关联查询" class="headerlink" title="一对一、一对多的关联查询 ?"></a>一对一、一对多的关联查询 ?</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">mapper</span> <span class="attr">namespace</span>=<span class="string">"com.lcb.mapping.userMapper"</span>&gt;</span> </span><br><span class="line">    <span class="comment">&lt;!--association 一对一关联查询 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">select</span> <span class="attr">id</span>=<span class="string">"getClass"</span> <span class="attr">parameterType</span>=<span class="string">"int"</span> <span class="attr">resultMap</span>=<span class="string">"ClassesResultMap"</span>&gt;</span></span><br><span class="line">select * from class c,teacher t where c.teacher_id=t.t_id and c.c_id=#&#123;id&#125;</span><br><span class="line"><span class="tag">&lt;/<span class="name">select</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">resultMap</span> <span class="attr">type</span>=<span class="string">"com.lcb.user.Classes"</span> <span class="attr">id</span>=<span class="string">"ClassesResultMap"</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 实体类的字段名和数据表的字段名映射 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">id</span> <span class="attr">property</span>=<span class="string">"id"</span> <span class="attr">column</span>=<span class="string">"c_id"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">result</span> <span class="attr">property</span>=<span class="string">"name"</span> <span class="attr">column</span>=<span class="string">"c_name"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">association</span> <span class="attr">property</span>=<span class="string">"teacher"</span> <span class="attr">javaType</span>=<span class="string">"com.lcb.user.Teacher"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">id</span> <span class="attr">property</span>=<span class="string">"id"</span> <span class="attr">column</span>=<span class="string">"t_id"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">result</span> <span class="attr">property</span>=<span class="string">"name"</span> <span class="attr">column</span>=<span class="string">"t_name"</span>/&gt;</span> <span class="tag">&lt;/<span class="name">association</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">resultMap</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--collection 一对多关联查询 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">select</span> <span class="attr">id</span>=<span class="string">"getClass2"</span> <span class="attr">parameterType</span>=<span class="string">"int"</span> <span class="attr">resultMap</span>=<span class="string">"ClassesResultMap2"</span>&gt;</span></span><br><span class="line">select * from class c,teacher t,student s where c.teacher_id=t.t_id and c.c_id=s.class_id and c.c_id=#&#123;id&#125;</span><br><span class="line"><span class="tag">&lt;/<span class="name">select</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">resultMap</span> <span class="attr">type</span>=<span class="string">"com.lcb.user.Classes"</span> <span class="attr">id</span>=<span class="string">"ClassesResultMap2"</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">id</span> <span class="attr">property</span>=<span class="string">"id"</span> <span class="attr">column</span>=<span class="string">"c_id"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">result</span> <span class="attr">property</span>=<span class="string">"name"</span> <span class="attr">column</span>=<span class="string">"c_name"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">association</span> <span class="attr">property</span>=<span class="string">"teacher"</span></span></span><br><span class="line"><span class="tag"><span class="attr">javaType</span>=<span class="string">"com.lcb.user.Teacher"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">id</span> <span class="attr">property</span>=<span class="string">"id"</span> <span class="attr">column</span>=<span class="string">"t_id"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">result</span> <span class="attr">property</span>=<span class="string">"name"</span> <span class="attr">column</span>=<span class="string">"t_name"</span>/&gt;</span> <span class="tag">&lt;/<span class="name">association</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">collection</span> <span class="attr">property</span>=<span class="string">"student"</span> <span class="attr">ofType</span>=<span class="string">"com.lcb.user.Student"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">id</span> <span class="attr">property</span>=<span class="string">"id"</span> <span class="attr">column</span>=<span class="string">"s_id"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">result</span> <span class="attr">property</span>=<span class="string">"name"</span> <span class="attr">column</span>=<span class="string">"s_name"</span>/&gt;</span> <span class="tag">&lt;/<span class="name">collection</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">resultMap</span>&gt;</span> <span class="tag">&lt;/<span class="name">mapper</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="MyBatis-实现一对一有几种方式-具体怎么操作的"><a href="#MyBatis-实现一对一有几种方式-具体怎么操作的" class="headerlink" title="MyBatis 实现一对一有几种方式?具体怎么操作的?"></a>MyBatis 实现一对一有几种方式?具体怎么操作的?</h2><ul><li><p>有联合查询和嵌套查询,联合查询是几个表联合查询,只查询一次, 通过在 resultMap 里面配置 association 节点配置一对一的类就可以完成;</p></li><li><p>嵌套查询是先查一个表，根据这个表里面的结果的 外键 id，去再另外一个表里面 查询数据,也是通过 association 配置，但另外一个表的查询通过 select 属性配置。</p></li></ul><h2 id="MyBatis-实现一对多有几种方式-怎么操作的"><a href="#MyBatis-实现一对多有几种方式-怎么操作的" class="headerlink" title="MyBatis 实现一对多有几种方式,怎么操作的?"></a>MyBatis 实现一对多有几种方式,怎么操作的?</h2><p>有联合查询和嵌套查询。联合查询是几个表联合查询,只查询一次,通过在 resultMap 里面的 collection 节点配置一对多的类就可以完成;嵌套查询是先查 一个表,根据这个表里面的 结果的外键 id,去再另外一个表里面查询数据,也是通过 配置 collection,但另外一个表的查询通过 select 节点配置。</p><h2 id="Mybatis-是否支持延迟加载-如果支持，它的实现原理是-什么"><a href="#Mybatis-是否支持延迟加载-如果支持，它的实现原理是-什么" class="headerlink" title="Mybatis 是否支持延迟加载?如果支持，它的实现原理是 什么?"></a>Mybatis 是否支持延迟加载?如果支持，它的实现原理是 什么?</h2><p>答:Mybatis 仅支持 association 关联对象和 collection 关联集合对象的延迟加 载，association 指的就是一对一，collection 指的就是一对多查询。在 Mybatis 配置文件中，可以配置是否启用延迟加载 lazyLoadingEnabled=true|false。</p><p>它的原理是，使用 CGLIB 创建目标对象的代理对象，当调用目标方法时，进入拦 截器方法，比如调用 a.getB().getName()，拦截器 invoke()方法发现 a.getB()是 null 值，那么就会单独发送事先保存好的查询关联 B 对象的 sql，把 B 查询上来， 然后调用 a.setB(b)，于是 a 的对象 b 属性就有值了，接着完成 a.getB().getName() 方法的调用。这就是延迟加载的基本原理。</p><p>当然了，不光是 Mybatis，几乎所有的包括 Hibernate，支持延迟加载的原理都 是一样的。</p><h2 id="Mybatis-的一级、二级缓存"><a href="#Mybatis-的一级、二级缓存" class="headerlink" title="Mybatis 的一级、二级缓存:"></a>Mybatis 的一级、二级缓存:</h2><ol><li>一级缓存: 基于 PerpetualCache 的 HashMap 本地缓存，其存储作用域为 Session，当 Session flush 或 close 之后，该 Session 中的所有 Cache 就 将清空，默认打开一级缓存。</li><li>二级缓存与一级缓存其机制相同，默认也是采用 PerpetualCache，HashMap 存储，不同在于其存储作用域为 Mapper(Namespace)，并且可自定义存储源， 如 Ehcache。默认不打开二级缓存，要开启二级缓存，使用二级缓存属性类需要 实现 Serializable 序列化接口(可用来保存对象的状态),可在它的映射文件中配置 <cache/> ;</li><li>对于缓存数据更新机制，当某一个作用域(一级缓存 Session/二级缓存 Namespaces)的进行了 C/U/D 操作后，默认该作用域下所有 select 中的缓存将 被 clear。</li></ol><p>##24、什么是 MyBatis 的接口绑定?有哪些实现方式?<br>接口绑定，就是在 MyBatis 中任意定义接口,然后把接口里面的方法和 SQL 语句绑 定, 我们直接调用接口方法就可以,这样比起原来了SqlSession提供的方法我们可 以有更加灵活的选择和设置。</p><p>接口绑定有两种实现方式,一种是通过注解绑定，就是在接口的方法上面加上 @Select、@Update 等注解，里面包含 Sql 语句来绑定;另外一种就是通过 xml 里面写 SQL 来绑定, 在这种情况下,要指定 xml 映射文件里面的 namespace 必须 为接口的全路径名。当 Sql 语句比较简单时候,用注解绑定, 当 SQL 语句比较复杂 时候,用 xml 绑定,一般用 xml 绑定的比较多。</p><h2 id="使用-MyBatis-的-mapper-接口调用时有哪些要求"><a href="#使用-MyBatis-的-mapper-接口调用时有哪些要求" class="headerlink" title="使用 MyBatis 的 mapper 接口调用时有哪些要求?"></a>使用 MyBatis 的 mapper 接口调用时有哪些要求?</h2><p>1、Mapper 接口方法名和 mapper.xml 中定义的每个 sql 的 id 相同;<br>2、Mapper 接口方法的输入参数类型和 mapper.xml 中定义的每个 sql 的 parameterType 的类型相同;<br>3、Mapper 接口方法的输出参数类型和 mapper.xml 中定义的每个 sql 的 resultType 的类型相同;<br>4、Mapper.xml 文件中的 namespace 即是 mapper 接口的类路径。</p><h2 id="Mapper-编写有哪几种方式"><a href="#Mapper-编写有哪几种方式" class="headerlink" title="Mapper 编写有哪几种方式?"></a>Mapper 编写有哪几种方式?</h2><p>第一种:接口实现类继承 SqlSessionDaoSupport:使用此种方法需要编写 mapper 接口，mapper 接口实现类、mapper.xml 文件。</p><ol><li>在 sqlMapConfig.xml 中配置 mapper.xml 的位置<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">mappers</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">mapper</span> <span class="attr">resource</span>=<span class="string">"mapper.xml 文件的地址"</span> /&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">mapper</span> <span class="attr">resource</span>=<span class="string">"mapper.xml 文件的地址"</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">mappers</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li>定义 mapper 接口</li><li>实现类集成 SqlSessionDaoSupport<br>mapper 方法中可以 this.getSqlSession()进行数据增删改查。 </li><li>spring 配置<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">" "</span> <span class="attr">class</span>=<span class="string">"mapper 接口的实现"</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"sqlSessionFactory"</span></span></span><br><span class="line"><span class="tag"><span class="attr">ref</span>=<span class="string">"sqlSessionFactory"</span>&gt;</span><span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br></pre></td></tr></table></figure>第二种:使用 org.mybatis.spring.mapper.MapperFactoryBean:</li></ol><p>1、在 sqlMapConfig.xml 中配置 mapper.xml 的位置，如果 mapper.xml 和 mappre 接口的名称相同且在同一个目录，这里可以不用配置</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">mappers</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">mapper</span> <span class="attr">resource</span>=<span class="string">"mapper.xml 文件的地址"</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">mapper</span> <span class="attr">resource</span>=<span class="string">"mapper.xml 文件的地址"</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">mappers</span>&gt;</span></span><br></pre></td></tr></table></figure><p>2、定义 mapper 接口:<br>1、mapper.xml 中的 namespace 为 mapper 接口的地址<br>2、mapper 接口中的方法名和 mapper.xml 中的定义的 statement 的 id 保持一 致<br>3、Spring 中定义</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">""</span> <span class="attr">class</span>=<span class="string">"org.mybatis.spring.mapper.MapperFactoryBean"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"mapperInterface"</span> <span class="attr">value</span>=<span class="string">"mapper 接口地址"</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"sqlSessionFactory"</span> <span class="attr">ref</span>=<span class="string">"sqlSessionFactory"</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br></pre></td></tr></table></figure><p>2、定义 mapper 接口:<br>注意 mapper.xml 的文件名和 mapper 的接口名称保持一致，且放在同一个目录 3、配置 mapper 扫描器:</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">""</span> <span class="attr">class</span>=<span class="string">"org.mybatis.spring.mapper.MapperFactoryBean"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"mapperInterface"</span> <span class="attr">value</span>=<span class="string">"mapper 接口地址"</span> /&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"sqlSessionFactory"</span> <span class="attr">ref</span>=<span class="string">"sqlSessionFactory"</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">bean</span> <span class="attr">class</span>=<span class="string">"org.mybatis.spring.mapper.MapperScannerConfigurer"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"basePackage"</span> <span class="attr">value</span>=<span class="string">"mapper 接口包地址</span></span></span><br><span class="line"><span class="tag"><span class="string">"</span>&gt;</span><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"sqlSessionFactoryBeanName"</span></span></span><br><span class="line"><span class="tag"><span class="attr">value</span>=<span class="string">"sqlSessionFactory"</span>/&gt;</span> <span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br></pre></td></tr></table></figure><p>4、使用扫描器后从 spring 容器中获取 mapper 的实现对象。</p><h2 id="简述-Mybatis-的插件运行原理，以及如何编写一个插件。"><a href="#简述-Mybatis-的插件运行原理，以及如何编写一个插件。" class="headerlink" title="简述 Mybatis 的插件运行原理，以及如何编写一个插件。"></a>简述 Mybatis 的插件运行原理，以及如何编写一个插件。</h2><p>答:Mybatis 仅可以编写针对 ParameterHandler、ResultSetHandler、 StatementHandler、Executor 这 4 种接口的插件，Mybatis 使用 JDK 的动态代 理，为需要拦截的接口生成代理对象以实现接口方法拦截功能，每当执行这 4 种 接口对象的方法时，就会进入拦截方法，具体就是 InvocationHandler 的 invoke() 方法，当然，只会拦截那些你指定需要拦截的方法。</p><p>编写插件:实现 Mybatis 的 Interceptor 接口并复写 intercept()方法，然后在给 插件编写注解，指定要拦截哪一个接口的哪些方法即可，记住，别忘了在配置文 件中配置你编写的插件。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;什么是-Mybatis&quot;&gt;&lt;a href=&quot;#什么是-Mybatis&quot; class=&quot;headerlink&quot; title=&quot;什么是 Mybatis?&quot;&gt;&lt;/a&gt;什么是 Mybatis?&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;Mybatis是一个半ORM（对象关系映射）的框架，
      
    
    </summary>
    
    
      <category term="面试题" scheme="http://yoursite.com/categories/%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
    
    
      <category term="导航" scheme="http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"/>
    
      <category term="分享" scheme="http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce</title>
    <link href="http://yoursite.com/2020/04/11/MapReduce/"/>
    <id>http://yoursite.com/2020/04/11/MapReduce/</id>
    <published>2020-04-11T06:23:09.000Z</published>
    <updated>2020-04-17T13:16:06.589Z</updated>
    
    <content type="html"><![CDATA[<h2 id="MapReduce定义"><a href="#MapReduce定义" class="headerlink" title="MapReduce定义"></a>MapReduce定义</h2><ol><li>MapReduce是一个分布式运算程序放入编程框架，是用户开发“基于Hadoop的数据分析应用”的核心框架。</li><li>MapReduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个hadoop集群上。</li></ol><h2 id="MapReduce的优缺点"><a href="#MapReduce的优缺点" class="headerlink" title="MapReduce的优缺点"></a>MapReduce的优缺点</h2><h3 id="1-MapReduce易于编程"><a href="#1-MapReduce易于编程" class="headerlink" title="1. MapReduce易于编程"></a>1. MapReduce易于编程</h3><pre><code>它简单的实现一些接口，就可以完成一个分布式程序，这个分布式程序可以分布到大量廉价的PC机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特点是的MapReduce编程变得非常流行。</code></pre><h3 id="2-良好的扩展性"><a href="#2-良好的扩展性" class="headerlink" title="2.良好的扩展性"></a>2.良好的扩展性</h3><pre><code>当你的计算资源不能得到满足的时候，你可以通过简单的增加机器来扩展它的计算能力。</code></pre><h3 id="3-高容错性"><a href="#3-高容错性" class="headerlink" title="3.高容错性"></a>3.高容错性</h3><pre><code>MapReduce设计的初衷就是使程序能够部署在廉价的PC机器上，这就要求它具有很高的容错性。比如其中一台机器宕机了，他可以把上面的计算任务转移到另一个节点上运行，不至于这个任务执行失败，而且这个过程不需要人工参与，而完全是由Hadoop内部完成</code></pre><h3 id="4-适合PB级以上海量数据的离线处理"><a href="#4-适合PB级以上海量数据的离线处理" class="headerlink" title="4.适合PB级以上海量数据的离线处理"></a>4.适合PB级以上海量数据的离线处理</h3><pre><code>可以实现上千台服务器集群并发工作，提高数据处理能力。</code></pre><h3 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a>缺点：</h3><ol><li>不擅长实时计算——  MapReduce无法像Mysql一样，在毫秒或者秒级内返回结果。</li><li>不擅长流式计算—— 流式计算的输入数据是动态的，而MapReduce的输入数据集是静态的，不能动态变化。这是因为MapReduce自身的设计特点决定了数据源必须是静态的</li><li>不擅长DAG（有向图计算）</li></ol><h2 id="基本理论"><a href="#基本理论" class="headerlink" title="基本理论"></a>基本理论</h2><h3 id="一、数据本地化的策略"><a href="#一、数据本地化的策略" class="headerlink" title="一、数据本地化的策略"></a>一、数据本地化的策略</h3><ol><li>MapReduce的物理结构：JobTracker（Master）和TaskTracker（slave）</li><li>JobTracker在接受到任务之后会向NameNode发送请求获取文件信息，而JobTracker在收到文件信息之后会对文件进行切片（Split）。切片不是真正将数据切开，而是划分任务量。每一个切片就需要对应一个MapTask</li><li>实际开发中，Split=Block/n，默认split和block大小一样</li><li>数据本地化：</li></ol><ul><li>为了减少跨集群的数据传输，往往会将DataNode和TaskTracker部署在相同的节点上，即一个节点既是DataNode又是TaskTracker</li><li>JobTracker在分配任务的时候，会考虑哪个节点有要处理的数据就将任务分给谁，这样分配的目的是为了能够在处理数据的时候从节点上读取而不需要跨节点读取</li></ul><ol start="5"><li>如果文件为空，那么整个文件作为一个切片来进行处理</li><li>在MapReduce中，文件分为可切与不可切。如果不指定，则文件默认可切。如果文件不可切，则整个文件作为一个切片来进行处理</li><li>如果需要调小splitSize，那么需要减小maxSize -FileInputFormat.setMaxInputSplitSize();如果需要增大splitSize，那么需要增大minSize - FileInputFormat.setMinInputSplitSize()</li><li>在切片的时候要考虑阈值SPLIT_SLOP(1.1)</li></ol><h3 id="二、Job-MapReduce执行流程"><a href="#二、Job-MapReduce执行流程" class="headerlink" title="二、Job/MapReduce执行流程"></a>二、Job/MapReduce执行流程</h3><h4 id="提交流程"><a href="#提交流程" class="headerlink" title="提交流程"></a>提交流程</h4><ol><li>检查job指定的输入输出路径</li><li>计算切片</li><li>如果需要，为这个job设置分布式缓存的存根账户信息</li><li>将这个job的jar包和配置信息提交到HFDFS上</li><li>将job提交到jobtracker，并且选择监控这个job的执行状态</li></ol><h4 id="执行流程"><a href="#执行流程" class="headerlink" title="执行流程"></a>执行流程</h4><ul><li>拆分任务：JobTracker在收到Job任务之后，会将Job任务拆分成MapTask和ReduceTask。MapTask的数量由切片数量来决定，ReduceTask的数量由分区数量来决定</li><li>分配任务：JobTracker在拆分完成之后会等待TaskTracker的心跳。当JobTracker收到TaskTracker会拆分出来的子任务分配给taskTracker，注意每一个TaskTracker不一定只分配分配一个任务，可以能会被分配到多个任务。在分配的时候，MapTask尽量考虑数据本地化策略（在分配的时候，哪一个TaskTracker上有要处理的数据就将MapTask分配给哪一个TaskTracker，ReduceTask则会考虑分配给相对空闲的TaskTracker</li><li>下载jar包：TaskTracker领取到任务（MapTask或者ReduceTask）之后，会连接对应的节点来下载jar包。这一步体现的是逻辑移动数据固定的思想</li><li>执行任务：TaskTracker在下载完jar包之后，会在本节点内部开启一个JVM子进程来执行MapTask或者是ReduceTask。默认情况下，一个JVM子进程只执行一个任务。所以如果一个TaskTracker被分配到了多个任务，那么这个TaskTracker就会多次开启并且关闭JVM子进程</li></ul><h3 id="三、uber模式"><a href="#三、uber模式" class="headerlink" title="三、uber模式"></a>三、uber模式</h3><ol><li>uber默认是关闭的，即JVM子进程默认只能执行一个任务；当开启uber模式的时候就可以一个jvm子进程的复用，即开启JVM子进程之后，可以执行多个任务之后再关闭。</li></ol><h2 id="Shuffle"><a href="#Shuffle" class="headerlink" title="Shuffle"></a>Shuffle</h2><h3 id="一、Map端的Shuffle"><a href="#一、Map端的Shuffle" class="headerlink" title="一、Map端的Shuffle"></a>一、Map端的Shuffle</h3><ol><li>MapTask默认对切片进行按行处理,每处理一行，先将数据写到缓冲区（MemoryBuffer）中</li><li>每一个MapTask默认自带一个缓冲区，这个缓冲区在内存中，默认占用内存100m的大小</li><li>当缓冲区的时候达到指定阈值（默认是0.8，当缓冲区的使用达到80%时就会产生溢写操作），会将缓冲区的数据溢写（spill）到磁盘上，产生一个溢写文件（spillable file）</li><li>溢写后的数据会再次向缓冲区写，如果达到阈值，会再次溢写，每一次溢写，会产生一个新的溢写文件</li><li>当MapTask将所有的数据处理完成之后会将所有的溢写文件进行合并（merge），合并成一个结果文件（final out），如果maptask处理完之后缓冲区还有数据，会将缓冲区的数据直接合并到fileout中</li><li>不一定会产生溢写过程，输入的数据量并不能决定溢写次数，而是由MapTask处理逻辑来决定的；溢写文件的大小并不是完全由缓冲区大小和溢写阈值决定，还需要考虑序列化的影响</li><li>如果没产生溢写，结果都在缓冲区中，那么缓冲区的数据直接冲刷产生一个final out</li></ol><h3 id="三、Shuffle的调优"><a href="#三、Shuffle的调优" class="headerlink" title="三、Shuffle的调优"></a>三、Shuffle的调优</h3><ol><li>扩大缓冲区，实际开发中会将缓冲区设置为250M～400M</li><li>尽量增加Combiner</li><li>可以将MapTask的结果进行压缩，</li></ol><h3 id="InputFormat-–输入格式"><a href="#InputFormat-–输入格式" class="headerlink" title="InputFormat –输入格式"></a>InputFormat –输入格式</h3><ol><li>输入格式类是MapReduce中顶级的输入格式的父类</li><li>作用：</li></ol><ul><li>对文件切片</li><li>针对切片提供输入流用于读取切片</li></ul><ol start="3"><li><p>InputFormat将HDFS上的数据切片，然后读取切片的内容，将读取的内容传递给Mapper，所以InputFormat读出来的数据是什么格式，mapper接受的就是什么格式</p></li><li><p>如果没有指定，默认使用的是TextInputformat，所以mapper的接受类型也是&lt;longwritable,Text&gt;，-&gt;TextInputFormat底层实际上依靠了LineRecordReader来读取数据 - 在TextInputFormat中，从第二个切片开始，每一个MapTask处理的数据都是从当前切片的第二行开始到下一个切片的第一行</p></li><li><p>自定义输入格式：如果MapReduce中提供的输入格式不适合指定场景，那么就需要自己定义输入格式-定义类继承InputFormat，因为实际过程中切片比较复杂所以一般继承FileinputFormat这个类中已经覆盖了切片方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AuthInputFormat</span> <span class="keyword">extends</span> <span class="title">FileInputFormat</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> RecordReader&lt;Text, Text&gt; <span class="title">createRecordReader</span><span class="params">(InputSplit split, TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> AuthReader();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AuthReader</span> <span class="keyword">extends</span> <span class="title">RecordReader</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> LineReader reader;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">byte</span>[] blank = <span class="keyword">new</span> Text(<span class="string">" "</span>).getBytes();</span><br><span class="line">    <span class="keyword">private</span> Text key;</span><br><span class="line">    <span class="keyword">private</span> Text value;</span><br><span class="line">    <span class="keyword">private</span> Text tmp;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//初始化的方法</span></span><br><span class="line">    <span class="comment">//AuthReader在创建的时候会自动调用初始化方法</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">(InputSplit split, TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">//转化为文件切片</span></span><br><span class="line">        FileSplit fsplit = (FileSplit) split;</span><br><span class="line">        <span class="comment">//拿到切片的路径</span></span><br><span class="line">        Path path = fsplit.getPath();</span><br><span class="line">        <span class="comment">//链接HDFS</span></span><br><span class="line">        FileSystem fs = FileSystem.get(URI.create(path.toString()), context.getConfiguration());</span><br><span class="line">        <span class="comment">//获取输入流</span></span><br><span class="line">        InputStream in = fs.open(path);</span><br><span class="line">        <span class="comment">//输入流是一个字节流，原文件是字符流，三行组成一行数据</span></span><br><span class="line">        <span class="comment">//如果使用字节流需要判断什么时候读完三行数据</span></span><br><span class="line">        <span class="comment">//将字节流进行包装，最好还能按行读取</span></span><br><span class="line">        reader = <span class="keyword">new</span> LineReader(in);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//在执行过程中判断是否有下一个间值对需要处理</span></span><br><span class="line">    <span class="comment">//如果有，那么就会传递给map中</span></span><br><span class="line">    <span class="comment">//如果没有，执行结束</span></span><br><span class="line">    <span class="comment">//试着去读取数据，如果读到，那么代表有数据要处理返回true</span></span><br><span class="line">    <span class="comment">//如果没有读到，那代表没有数据需要处理返回false</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">nextKeyValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        key = <span class="keyword">new</span> Text();</span><br><span class="line">        value = <span class="keyword">new</span> Text();</span><br><span class="line">        tmp = <span class="keyword">new</span> Text();</span><br><span class="line">        <span class="comment">//会将读取到的一行数据放入传入的参数tmp</span></span><br><span class="line">        <span class="keyword">if</span> (reader.readLine(tmp) == <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        key.set(tmp.toString());</span><br><span class="line">        <span class="keyword">if</span> (reader.readLine(tmp) == <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        value.set(tmp.toString());</span><br><span class="line">        value.append(blank, <span class="number">0</span>, blank.length);</span><br><span class="line">        <span class="keyword">if</span> (reader.readLine(tmp) == <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        <span class="keyword">byte</span>[] bytes = tmp.getBytes();</span><br><span class="line">        value.append(bytes, <span class="number">0</span>, bytes.length);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Text <span class="title">getCurrentKey</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> key;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Text <span class="title">getCurrentValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> value;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">//获取执行进度的方法</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">float</span> <span class="title">getProgress</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (reader != <span class="keyword">null</span>) &#123;</span><br><span class="line">            reader.close();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>多源输入：在MapReduce中可以同时指定多个路径来同时处理，这多个路径中的文件格式可以不一样，每一个路径单独制定输入格式以及Mapper类，但是最终使用的一个Reducer</p></li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MultiDriver</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, ClassNotFoundException, InterruptedException </span>&#123;</span><br><span class="line">        System.setProperty(<span class="string">"hadoop_user_name"</span>,<span class="string">"root"</span>);</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line">        job.setJarByClass(MultiDriver<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(MultiReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        <span class="comment">//设置自定义输入格式</span></span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//同时指定多个输入路径 -多元输入</span></span><br><span class="line">        MultipleInputs.addInputPath(job,<span class="keyword">new</span> Path(<span class="string">"hdfs://192.168.243.134:9000/txt/score.txt"</span>), TextInputFormat<span class="class">.<span class="keyword">class</span>,<span class="title">MultiMapper1</span>.<span class="title">class</span>)</span>;</span><br><span class="line">        MultipleInputs.addInputPath(job,<span class="keyword">new</span> Path(<span class="string">"hdfs://192.168.243.134:9000/txt/score3.txt"</span>), AuthInputFormat<span class="class">.<span class="keyword">class</span>,<span class="title">MultiMapper2</span>.<span class="title">class</span>)</span>;</span><br><span class="line">        FileOutputFormat.setOutputPath(job,<span class="keyword">new</span> Path(<span class="string">"hdfs://192.168.243.134:9000//result/Multi"</span>));</span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="输出格式-OutputFormat"><a href="#输出格式-OutputFormat" class="headerlink" title="输出格式 OutputFormat"></a>输出格式 OutputFormat</h3><h4 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h4><ol><li>校验这个任务的输出路径，例如校验输出路径是否不存在</li><li>提供RecorderWriter输出流用于写出数据，这个数据是存储在文件系统中</li><li>如果不指定，默认值的是TextOutputFormat</li><li>OutputFormat是MapReduce中输出格式的顶级父类</li></ol><h3 id="数据倾斜"><a href="#数据倾斜" class="headerlink" title="数据倾斜"></a>数据倾斜</h3><ol><li>每一个任务处理的数据量不均匀，那么就产生了数据倾斜</li><li>数据本身就具有倾斜特性 - 倾斜度越大效率越低</li><li>Map端倾斜的产生条件：多源输入，文件大小不均等，文件不可切，这三个条件缺一不可，在实际开发过程中Map端的数据倾斜无法处理。</li><li>实际开发过程中，绝大部分的数据倾斜都产生在Reduce端，而直接导致Reduce端数据倾斜的原因是因为数据的分类（分区），本质原因是因为数据本省不均衡，Reduce端的倾斜可以使用二阶段聚合来解决。</li><li>join：如果需要处理多个文件，且文件之间有关系，可以把小文件先存根，先处理大文件，然后从根中取出处理</li></ol><h3 id="小文件"><a href="#小文件" class="headerlink" title="小文件"></a>小文件</h3><ol><li>危害</li></ol><ul><li>存储：每一个小文件对应一条元数据，如果HDFS上存储了大量的小文件，那么就会产生大量的元数据。元数据数量过多的时候，导致NameNode的内存被大量占用，还会导致NameNode的查询效率变低。</li><li>计算：每一个小文件对应一个切片，那么大量的小文件就会产生大量的切片对应了大量的MapTask，当切片过多导致MapTask（线程）的数量，线程数量过多可能会导致服务器的负载压力变大甚至崩溃</li></ul><ol start="2"><li>目前为止处理办法，合并和打包</li></ol><h3 id="推测执行机制"><a href="#推测执行机制" class="headerlink" title="推测执行机制"></a>推测执行机制</h3><ol><li>推测执行机制实际上是Hadoop针对慢任务优化方案，当MapReduce出现慢任务的时候，Hadoop自动将慢任务拷贝一份，谁先处理完就取那个节点的结果，剩下的就会被kill掉</li><li>出现慢任务的场景：</li></ol><ul><li>任务分配不均   正常情况下不会出现，Namenode默认会找空闲的节点处理</li><li>硬件环境不同  正常情况下也不会出现，服务器购买按批买，性能基本一致</li><li>数据倾斜   集中在这个场景，推测执行机制无效，所以一般是关闭这个机制</li></ul><ol start="3"><li>实际开发中，数据倾斜导致的慢任务的场景更多，此时推测执行机制是无效的，所以一般是会关闭推测执行机制。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;MapReduce定义&quot;&gt;&lt;a href=&quot;#MapReduce定义&quot; class=&quot;headerlink&quot; title=&quot;MapReduce定义&quot;&gt;&lt;/a&gt;MapReduce定义&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;MapReduce是一个分布式运算程序放入编程框架，是用
      
    
    </summary>
    
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
    
      <category term="导航" scheme="http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"/>
    
      <category term="分享" scheme="http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"/>
    
  </entry>
  
  <entry>
    <title>Dubbo</title>
    <link href="http://yoursite.com/2020/04/08/Dubbo/"/>
    <id>http://yoursite.com/2020/04/08/Dubbo/</id>
    <published>2020-04-08T06:23:09.000Z</published>
    <updated>2020-04-12T13:01:04.351Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、分布式系统"><a href="#一、分布式系统" class="headerlink" title="一、分布式系统"></a>一、分布式系统</h2><pre><code>分布式系统是若干独立计算机的集合，这些计算机对于用户来说就像单个相关系统，分布式系统（distributed system）是建立在网络之上的软件系统。</code></pre><h3 id="垂直应用架构"><a href="#垂直应用架构" class="headerlink" title="垂直应用架构"></a>垂直应用架构</h3><pre><code>1. 做不到界面+业务逻辑实现分离2. 应用不可能完全独立，大量的应用之间需要交互</code></pre><h3 id="分布式应用架构"><a href="#分布式应用架构" class="headerlink" title="分布式应用架构"></a>分布式应用架构</h3><pre><code>1. 可以做到点后端分离2. 应用之间的相互调用</code></pre><h2 id="二、RPC（远程过程调用）"><a href="#二、RPC（远程过程调用）" class="headerlink" title="二、RPC（远程过程调用）"></a>二、RPC（远程过程调用）</h2><h3 id="什么叫RPC"><a href="#什么叫RPC" class="headerlink" title="什么叫RPC"></a>什么叫RPC</h3><pre><code>RPC【Remote Procedure Call】是指远程过程调用，是一种进程间通信方式，他是一种技术的思想，而不是规范。它允许程序调用另一个地址空间（通常是共享网络的另一台机器上）的过程或函数，而不用程序员显式编码这个远程调用的细节。即程序员无论是调用本地的还是远程的函数，本质上编写的调用代码基本相同。</code></pre><h3 id="调用过程"><a href="#调用过程" class="headerlink" title="调用过程"></a>调用过程</h3><p><img src="/" class="lazyload" data-src="/Users/yuxiangrui/blog/source/picture/1.png"  alt="截屏2020-04-12上午10.31.38"></p><h3 id="Dubbo中的序列化"><a href="#Dubbo中的序列化" class="headerlink" title="Dubbo中的序列化"></a>Dubbo中的序列化</h3><ol><li>Dubbo RPC是Dubbo体系最核心的一种高性能，高吞吐量的远程调用方式，可以称之为多路复用的TCP长连接调用<ul><li>长连接：避免了每次调用新建TCP连接，提高了调用的响应速度</li><li>多路复用：单个TCP连接可以交替传输多个请求和响应的消息，降低了连接的等待闲置时间从而减少了同样并发数下的网络连接数，提高了系统吞吐量</li><li>Dubbo RPC 主要用于两个 Dubbo 系统之间的远程调用，特别适合高并发、小数据的互联网场景。而序列化对于远程调用的响应速度、吞吐量、网络带宽消耗等同样也起着至关重要的作用，是我们提升分布式系统性能的最关键因素之一。</li></ul></li></ol><h3 id="Dubbo-中支持的序列化方式："><a href="#Dubbo-中支持的序列化方式：" class="headerlink" title="Dubbo 中支持的序列化方式："></a>Dubbo 中支持的序列化方式：</h3><ul><li><p>dubbo 序列化：阿里尚未开发成熟的高效 java 序列化实现，阿里不建议在生产环境使用它</p></li><li><p>hessian2 序列化：hessian 是一种跨语言的高效二进制序列化方式。但这里实际不是原生的hessian2 序列化，而是阿里修改过的 hessian lite，它是 dubbo RPC 默认启用的序列化方式</p></li><li><p>json 序列化：目前有两种实现，一种是采用的阿里的 fastjson 库，另一种是采用 dubbo 中自己实现的简单 json 库，但其实现都不是特别成熟，而且 json 这种文本序列化性能一般不如上面两种二进制序列化。</p></li><li><p>java 序列化：主要是采用 JDK 自带的 Java 序列化实现，性能很不理想。</p><p>  在通常情况下，这四种主要序列化方式的性能从上到下依次递减。对于 dubbo RPC 这种追求高性能的远程调用方式来说，实际上只有 1、2 两种高效序列化方式比较般配，而第 1 个 dubbo 序列化由于还不成熟，所以实际只剩下 2 可用，所以 dubbo RPC 默认采用 hessian2 序列化。</p><p>  但 hessian 是一个比较老的序列化实现了，而且它是跨语言的，所以不是单独针对 Java 进行优化的。而 dubbo RPC 实际上完全是一种 Java to Java 的远程调用，其实没有必要采用跨语言的序列化方式（当然肯定也不排斥跨语言的序列化）。</p><p>  最近几年，各种新的高效序列化方式层出不穷，不断刷新序列化性能的上限，最典型的包括：</p></li><li><p>专门针对 Java 语言的：Kryo，FST 等等</p></li><li><p>跨语言的：Protostuff，ProtoBuf，Thrift，Avro，MsgPack 等等<br>这些序列化方式的性能多数都显著优于 hessian2（甚至包括尚未成熟的 dubbo 序列化）</p><p>  有鉴于此，我们为 dubbo 引入 Kryo 和 FST 这两种高效 Java 序列化实现，来逐步取代 hessian2。</p><p>  其中，Kryo 是一种非常成熟的序列化实现，已经在 Twitter、Groupon、Yahoo 以及多个著名开源项目（如 Hive、Storm）中广泛的使用。而 FST 是一种较新的序列化实现，目前还缺乏足够多的成熟使用案例。</p></li></ul><p><strong>吞吐量：吞吐量是指对网络、设备、端口、虚电路或其他设施，单位时间内成功地传送数据的数量（以比特、字节、分组等测量）。</strong></p><p><strong>在面向生产环境的应用中，目前更优先选择 Kryo。</strong></p><h2 id="Dubbo-Hystrix-实现服务熔断"><a href="#Dubbo-Hystrix-实现服务熔断" class="headerlink" title="Dubbo + Hystrix 实现服务熔断"></a>Dubbo + Hystrix 实现服务熔断</h2><h3 id="熔断器简介"><a href="#熔断器简介" class="headerlink" title="熔断器简介"></a>熔断器简介</h3><pre><code>在微服务架构中，根据业务来拆分成一个个的服务，服务与服务之间可以通过 RPC 相互调用。为了保证其高可用，单个服务通常会集群部署。由于网络原因或者自身的原因，服务并不能保证 100% 可用，如果单个服务出现问题，调用这个服务就会出现线程阻塞，此时若有大量的请求涌入，Servlet 容器的线程资源会被消耗完毕，导致服务瘫痪。服务与服务之间的依赖性，故障会传播，会对整个微服务系统造成灾难性的严重后果，这就是服务故障的 “雪崩” 效应。为了解决这个问题，业界提出了熔断器模型。</code></pre><p>Netflix 开源了 Hystrix 组件，实现了熔断器模式，Spring Cloud 对这一组件进行了整合。在微服务架构中，一个请求需要调用多个服务是非常常见的，如下图：<br><img src="/" class="lazyload" data-src="https://www.funtl.com/assets/Lusifer201805292246007.png"  alt=""></p><p>较底层的服务如果出现故障，会导致连锁故障。当对特定的服务的调用的不可用达到一个阀值（Hystrix 是 5 秒 20 次） 熔断器将会被打开。<br><img src="/" class="lazyload" data-src="https://www.funtl.com/assets/Lusifer201805292246008.png"  alt=""><br>熔断器打开后，为了避免连锁故障，通过 fallback 方法可以直接返回一个固定值。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;一、分布式系统&quot;&gt;&lt;a href=&quot;#一、分布式系统&quot; class=&quot;headerlink&quot; title=&quot;一、分布式系统&quot;&gt;&lt;/a&gt;一、分布式系统&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;分布式系统是若干独立计算机的集合，这些计算机对于用户来说就像单个相关系统，分布式系统
      
    
    </summary>
    
    
      <category term="分布式" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
    
      <category term="导航" scheme="http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"/>
    
      <category term="分享" scheme="http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"/>
    
  </entry>
  
  <entry>
    <title>HDFS</title>
    <link href="http://yoursite.com/2020/04/08/HDFS/"/>
    <id>http://yoursite.com/2020/04/08/HDFS/</id>
    <published>2020-04-08T06:23:09.000Z</published>
    <updated>2020-04-11T08:18:06.026Z</updated>
    
    <content type="html"><![CDATA[<h2 id="大数据简介"><a href="#大数据简介" class="headerlink" title="大数据简介"></a>大数据简介</h2><h3 id="一、大数据特征-6V"><a href="#一、大数据特征-6V" class="headerlink" title="一、大数据特征 - 6V"></a>一、大数据特征 - 6V</h3><ol><li>数据体量大，一般从TB级别开始计算</li><li>数据种类和来源多</li><li>数据的增长速度越来越快</li><li>数据的价值密度越来越低，但是这不意味着想要的数据越来越少，相反，想要的数据越来越多的，但是样本总量的增长速度是要高于想要的数据的增长速度的</li><li>数据的真实性/质量</li><li>数据的连通性</li><li>数据的动态性、数据的可视化、合法性</li></ol><h3 id="二、概述"><a href="#二、概述" class="headerlink" title="二、概述"></a>二、概述</h3><ol><li>Hadoop是Apache提供的一个开源的、可靠的、可扩展的系统架构，可以利用分布式架构来进行海量数据的存储以及计算 </li><li>Hadoop之父：Doug Cutting（道格·卡丁）</li><li>需要注意的是Hadoop处理的是离线数据，即在数据已知以及不要求实时性的场景下使用</li><li>Hadoop发行版:<br> a. Apache版本最基本的版本，入门学习较好。原生版本管理相对比较混乱<br> b. CDH版本是Cloudera提供的商业版本，相对Apache Hadoop更加的稳定和安全<br> c. Hortonworks文档相对完善友好</li></ol><h3 id="三、-版本："><a href="#三、-版本：" class="headerlink" title="三、 版本："></a>三、 版本：</h3><ol><li>Hadoop1.0：包含Common，HDFS，和MapReduce ，停止更新</li><li>Hadoop2.0：包含Common，HDFS，和MapReduce，YARN。1.0和2.0完全不兼容，Hadoop2.8之前的不包含ozone</li><li>Hadoop3.0：包含Common，HDFS，和MapReduce，YARN和ozone。</li></ol><h3 id="四、模块"><a href="#四、模块" class="headerlink" title="四、模块"></a>四、模块</h3><ol><li>Hadoop Common: 基本模块</li><li>Hadoop Distributed File System (HDFS™): 分布式存储</li><li>Hadoop YARN:进行任务调度和资源管理</li><li>Hadoop MapReduce: 分布式计算</li><li>Hadoop Ozone:对象存储</li></ol><h3 id="五、安装方式"><a href="#五、安装方式" class="headerlink" title="五、安装方式"></a>五、安装方式</h3><ol><li>单机模式：只能启动MapReduce</li><li>伪分布式：只能启动HDFS，MapReduce和YARN的大部分功能</li><li>完全分布式：能启动Hadoop的所有功能</li></ol><h2 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h2><h3 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h3><ol><li>HDFS是Hadoop提供一套用于进行分布式存储的文件系统</li><li>HDFS根据Google的GFS来进行实现的<h4 id="特点："><a href="#特点：" class="headerlink" title="特点："></a>特点：</h4></li><li>能够存储超大文件 - 切块</li><li>快速的应对和检测故障 - 心跳</li><li>能够在相对廉价的机器上来进行动态横向扩展</li><li>不支持低延迟响应</li><li>不建议存储小文件 - 每一个小文件对应一条元数据，大量的硒筽啊文件就会产生大量的元数据。元数据过多导致内存被大量占用，导致查询效率降低。</li><li>简化的一致性模型，一次写入多次读取，不允许修改，但允许追加写入。</li><li>不支持强事务或者不支持事务，在实际开发中，在样本量足够大的前提下，允许存在一定的容量误差</li></ol><h3 id="二、基本概念"><a href="#二、基本概念" class="headerlink" title="二、基本概念"></a>二、基本概念</h3><ol><li>HDFS主要包含2类进程：NameNode和DataNode</li><li>在HDFS中，上传的文件自动的进行切块，每一个数据块称之为一个Block</li><li>HDFS会自动对数据进行备份，每一个备份称之为副本（replication/replicas）默认为3</li><li>HDFS仿照Linux设置一套虚拟的文件系统,根路径/</li></ol><h3 id="三、Block"><a href="#三、Block" class="headerlink" title="三、Block"></a>三、Block</h3><ol><li>Block是HDFS中存储数据的基本单位，即在HDFS中所有数据都是以Block形式存储</li><li>Block默认是128M大小，可以通过dfs.blocksize(在hdfs-site.xml，单位是字节)来设置</li><li>如果一个文件不到一个Block的大小，那么注意，这个文件本身多大block就是多大</li><li>在HDFS中，会给每一个Block自动分配一个全局递增的Block ID</li><li>在HDFS中，会给每一个Block分配一个时间戳（Generation Stamp）编号</li><li>切块的意义<ul><li>为了能够存储超大文件</li><li>为了能够进行快速备份</li></ul></li></ol><h3 id="四、NameNode"><a href="#四、NameNode" class="headerlink" title="四、NameNode"></a>四、NameNode</h3><ol><li>NameNode是HDFS中的主节点（Master），作用：管理DataNode和记录与元数据（metadata）</li><li>元数据是描述数据的数据 —在HDFS中，元数据对存储的文件进行描述，主要包含<ul><li>文件的存储路径，例如/txt/a.txt</li><li>文件的权限</li><li>上传的用户和用户组</li><li>文件的大小</li><li>Block的大小</li><li>文件和BlockID的映射关系</li><li>副本数量</li><li>BlockID和DataNode的映射关系</li></ul></li><li>元数据是存储在内存以及磁盘中<ul><li>维系在内存中的目的是为了快速查询</li><li>维系在磁盘中的目的是为了崩溃恢复</li></ul></li><li>元数据在磁盘中的存储路径由hadoop.tmp.dir(core-site.xml)属性来决定的</li><li>元数据的记录和fsimage以及edits文件相关<ul><li>edits记录写操作</li><li>fsimage：记录元数据。fsimage中的元数据往往是落后于内存中的元数据</li></ul></li><li>当NameNode接受到写请求的时候，NameNode会先将这个写请求记录到edits-iprogress中，如果记录成功则修改内存中的元数据。内存中的元数据修改成功之后会给客户端返回一个成功信号。这个过程中fsimage文件中的元数据没有被修改</li><li>当达到指定条件的时候，触发edits_inprogress文件的滚动，edits_inprogress会滚动生成edits文件，会产生一个新的edits_inprogress。在edits_inprogress滚动过程中，会触发fsimage文件的更新，会根据edits_inprogress中的命令去修改fsimage文件中的元数据</li><li>edits_inprogress文件的滚动条件<ul><li>空间：当edits_inprogress文件达到指定大小（64m通过f s.checkpoint.size，单位是字节 - core-site.xml)的时候，会产生滚动</li><li>时间：当距离上一次滚动的时间间隔（默认3600s，通过f s.checkpoint.period来调节，默认单位是秒-core site.xml）达到指定大小，会产生滚动</li><li>重启：当NameNode重启的时候，自动触发edits——inprogress文件的滚动</li><li>强制滚动：可以利用 hadoop dfsadmin -rollEdits来强制滚动</li></ul></li><li>NameNode通过心跳机制来管理DataNode， DataNode定时的通过RPC的方式来给NameNode发送心跳。</li><li>默认情况下是每隔3s给NameNode发送心跳，可以通过dfs.heartbeat.interval来修改，单位是秒 —hdfs site.xml</li><li>NameNode超过10min没有收到DataNode的心跳，就会认为这个DataNode已经Lost（丢失），那么此时NameNode就会讲这个DataNode上的Block备份到其他节点，保证整个集群的副本数量</li><li>心跳信息<ul><li>当前DataNode的状态（预服役、服役、预退役）</li><li>当前DataNode存储的Block信息</li><li>clusterid：集群编号。当NameNode被格式化的时候，会自动计算产生一个clusterid，每格式化一次就会重新计算产生，当HDFS集群启动的时候，NameNode会将clusterid分发给每一个DataNode的信息之后，会先校验clusterid是否一致；同样，DataNode收到NameNode的指令的时候也会校验clusterid。DataNode只能接受一次clusterid</li></ul></li><li>当NameNode重启的时候，会自动触发edits_inprogress文件的滚动，产生一个新的edits文件和一个新的edits_inprogress文件，同时更新fsimage。当更新完fsimage之后，NameNode就会将fsimage文件中的内容加载到内存中，加载完成后，NameNode等待DataNode的心跳。如果在指定时间内没有收到心跳，则认为节点丢失重新备份。如果收到DataNode的心跳，那么NameNode会校验这个心跳信息，这个过程称之为安全模式。在安全模式中，NameNode如果校验失败，则试图恢复数据，恢复完成之后会重新校验；如果校验成功，则自动退出安全模式。</li><li>当NameNode重启的时候，自动进入安全模式，实际开发过程中，如果进入安全模式，需要等待NameNode自动退出安全模式，但是在合理的时间内，NameNode没有退出安全模式，就说明数据产生了不可挽回的丢失，此时需要强制退出安全模式（hadoop afsadmin -safemode leave）</li><li>在安全模式中，HDFS不对外提供写服务</li><li>正因为有安全模式的存在所以在为分布式中副本数量设置为1；</li></ol><h3 id="五、多副本的放置策略"><a href="#五、多副本的放置策略" class="headerlink" title="五、多副本的放置策略"></a>五、多副本的放置策略</h3><ol><li>第一个副本<ul><li>如果集群内部上传，那么谁上传第一个副本就放在谁身上</li><li>如果是集群外部上传，NameNode会选择相对较闲的DataNode的节点存储数据</li></ul></li><li>第二个副本<ul><li>Hadoop2.7之前：第二个副本是放在和第一个副本不同机架的节点上</li><li>Hadoop2.7开始：第二个副本是放在和第一个副本相同机架的节点上</li></ul></li><li>第三个副本<ul><li>Hadoop2.7之前：第三个副本是放在和第二个副本相同机架的节点上</li><li>Hadoop2.7开始：第三个副本是放在和第二个副本不同机架的节点上</li></ul></li><li>更多副本：那个几点相对空闲就放在谁身上</li></ol><h3 id="六、-机架感知策略"><a href="#六、-机架感知策略" class="headerlink" title="六、 机架感知策略"></a>六、 机架感知策略</h3><ol><li>在HDFS中默认没有启用，需要在hadoop-site.xml中配置topology.script.file.name开启机架感知策略</li><li>HDFS的机架感知策略是通过指定的脚本来进行配置，这个脚本可以是python和shell，所谓的机架就是一个映射，只需要将DataNode的主机名或者ip映射到值上就对应了机架</li><li>机架指的是逻辑机架不是物理机架，允许将一个或几个物理机架上的节点映射到同一个逻辑机架上，但是实际开发中，一般是一个物理机架对应一个逻辑机架</li></ol><h3 id="七、DataNode"><a href="#七、DataNode" class="headerlink" title="七、DataNode"></a>七、DataNode</h3><ol><li>DataNode是HDFS中的从节点，作用：存储数据，数据以block的形式来存储</li><li>DataNode会将数据存储在磁盘中，在磁盘上的存储位置由hadoop.tmp.dir属性来决定</li><li>DataNode的状态:预服役，服役，预退役，退役</li><li>DataNode会主动给NameNode发送心跳来进行注册</li></ol><h3 id="八、SecondaryNameNode"><a href="#八、SecondaryNameNode" class="headerlink" title="八、SecondaryNameNode"></a>八、SecondaryNameNode</h3><ol><li>SecondaryNameNode并不是NameNode备份，而是辅助NameNode进行edits_inprogress文件的滚动和fsimage的更新</li><li>在HDFS集群中，如果存在SecondaryNameNode，edits_inprogress文件的滚动和fsimage的更新由SecondaryNameNode来做，如果不存在，上述事情由NameNode自己来做，存活与否不影响集群向外提供服务，只会影响效率</li><li>在HDFS集群中，支持的结构一般是有两种<ul><li>1个NameNode+1个SecondaryNameNode+多个DataNode</li><li>2个NameNode+多个DataNode</li></ul></li><li>因为在HDFS中，NameNode是核心节点，所以一般要考虑NameNode的备份，那么此时就需要系用双NameNode机制，一个处于active状态，另一个处于备份状态，当active的NameNode宕机后，备份的立即切换为active，在实际工作中以第二方案为主，这样实现了集群的高可用（HA）。</li></ol><h3 id="九、回收站机制"><a href="#九、回收站机制" class="headerlink" title="九、回收站机制"></a>九、回收站机制</h3><ol><li>在HDFS中，回收站机制默认不开启，即删除命令会立即生效，</li><li>如果要开启回收站机制，要在core-site.xml中配置</li></ol><h3 id="十、dfs目录"><a href="#十、dfs目录" class="headerlink" title="十、dfs目录"></a>十、dfs目录</h3><ol><li>dfs目录实际上就是HDFS的数据目录，由hadoop.tmp.dir属性来决定存储路径</li><li>dfs子目录<br> a. data 对应datanode数据的存储路径<br> b. name 对应namenode数据的存储路径<br> c. namesecondary 对应namesecondary数据的存储路径</li><li>实际开发中，三个子目录应该出现在三个不同的节点上</li><li>in_use.lock用于标记是否已经启动对应的进程</li><li>每一个blk文件对应一个.meta,这个.meta文件可以认为是对blk的校验</li><li>HDFS在第一次启动的时候，间隔1min之后出发edits_inprogress文件的滚动，之后就按照指定的条件进行滚动</li><li>在HFDS中，每一次写操作看成一次事务，分配一个全局递增的编号，简称txid</li><li>文件上传完成之后不能修改</li><li>fsimage_XXXX.md5是利用了md5加密算法对fsimage_xxxx来进行校验</li><li>version文件中包含的主要内容<ul><li>clusterID：集群编号 - 本质上是进行校验的</li><li>storageType：节点类型</li><li>blockpooIID：块池编号</li></ul></li></ol><h2 id="Hadoop流程"><a href="#Hadoop流程" class="headerlink" title="Hadoop流程"></a>Hadoop流程</h2><h3 id="写流程-put"><a href="#写流程-put" class="headerlink" title="写流程 -put"></a>写流程 -put</h3><ol><li>客户端发起RPC请求到NameNode要求上传文件</li><li>NameNode收到请求之后会进行校验<ul><li>校验上传的路径是否有写入权限</li><li>检验上传的路径下是否有同名文件</li></ul></li><li>如果校验失败，直接报错，如果校验成功，则NameNode会修改edits_inprogress,更新内存中的元数据，最后会给客户端返回一个允许上传的信号。</li><li>客户端收到信号之后，会给NameNode发送请求，请求获取第一个Block的存储位置</li><li>NameNode收到请求之后，会等待DataNode的心跳，然后选择符合要求的节点（副本放置策略），将节点位置（默认是3个）放入队列中返回给客户端</li><li>客户端收到队列之后，会从队列中将所有的位置去除，然后选择一个较近的节点将第一个Block的第一个副本写入，客户端同时会告诉第一个副本所在的节点两个副本的存储位置，第一个副本所在的节点就会通过管道（pipeline，本质就是NIO Channel）将第二个副本写入对应的节点同时告诉第二个副本所在的节点最后一个副本的存储位置，第二个副本所在的节点通过管道将第三个副本写入对应的节点；第三个副本写完之后，会给第二个副本所在的节点返回一个ack（确认字符）信号，第二个副本所在的节点会给第一个副本所在的节点返回一个ack；第一个副本所在的节点会给客户端返回一个ack</li><li>客户端收到ack之后，会向namenode发送请求要下一个Block的存储位置，重复4，5，6三个步骤</li><li>当写完所有的数据之后，客户端会给NameNode发送信号，请求NameNode关闭文件。文件一旦关闭就不能再修改了。</li></ol><h3 id="读流程-get"><a href="#读流程-get" class="headerlink" title="读流程 -get"></a>读流程 -get</h3><ol><li>客户端发起RPC请求到NameNode要求下载指定文件</li><li>NameNode在收到请求之后会查询元数据确定是否有指定的文件，如果没有直接报错，如果有，则给客户端返回一个信号表示允许读取</li><li>客户端收到信号之后，会再给NameNode发送请求，请求获取这个文件的第一个block的存储位置</li><li>NameNode在收到请求之后，会将第一个Block的存储位置（默认是3个）放到一个队列中返回客户端</li><li>客户端收到队列之后，会从队列中将block的位置全部取出来，从中选择一个较近（网络拓扑距离）的DataNode来读取第一个Block</li><li>客户端读取完Block之后，会进行checksum（校验和 - 实际上就是利用.meta文件进行校验）验证，如果校验失败，则客户端会给NameNode发送信号，重新选择节点重新读取；如果校验成功，那么客户端会再向NameNode要下一个Block的地址，重复4，5，6三个步骤</li><li>当客户端读取完最后一个Block的时候，NameNode就会关闭文件（实际就是关流）<h3 id="删流程-rmr"><a href="#删流程-rmr" class="headerlink" title="删流程 -rmr"></a>删流程 -rmr</h3></li><li>客户端发起RPC请求到NameNode要求删除指定的文件</li><li>NameNode收到请求后，先记录edits_inprogress文件，然后修改内存中的元数据，然后NameNode就会给客户端返回一个成功信号，但是这个过程中，注意文件并没有从HDFS上移除而是依然存储在DataNode上</li><li>NameNode等待DataNode的心跳，收到心跳信息之后，NameNode就会校验这个心跳信息（比对元数据和心跳信息中的BlockID是否一致），然后NameNode就会进行心跳响应，要求DataNode删除对应的Block</li><li>DataNode收到心跳响应之后才会删除Block，此时文件才真正的从HDFS上移除</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;大数据简介&quot;&gt;&lt;a href=&quot;#大数据简介&quot; class=&quot;headerlink&quot; title=&quot;大数据简介&quot;&gt;&lt;/a&gt;大数据简介&lt;/h2&gt;&lt;h3 id=&quot;一、大数据特征-6V&quot;&gt;&lt;a href=&quot;#一、大数据特征-6V&quot; class=&quot;headerlink&quot; 
      
    
    </summary>
    
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
    
      <category term="导航" scheme="http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"/>
    
      <category term="分享" scheme="http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"/>
    
  </entry>
  
  <entry>
    <title>Gateway</title>
    <link href="http://yoursite.com/2020/04/08/Gateway/"/>
    <id>http://yoursite.com/2020/04/08/Gateway/</id>
    <published>2020-04-08T06:23:09.000Z</published>
    <updated>2020-04-10T11:53:58.924Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><ol><li><p>由于有如此众多的客户端和服务器，在云体系结构中包括一个API网关通常会很有帮助。网关可以负责保护和路由消息，隐藏服务，限制负载以及许多其他有用的事情。Spring Cloud Gateway为您提供对API层的精确控制，集成了Spring Cloud服务发现和客户端负载平衡解决方案，以简化配置和维护。</p></li><li><p>SpringCloud Gateway 是Spring cloud的一个全新的项目，基于Spring Framework 5，Project Reactor和Spring Boot 2.0等技术开发的网关，它旨在为为微服务架构提供简单有效的统一的API路由管理方式。</p></li><li><p>SpringCloud Gateway作为Spring cloud 生态系统中的网关，目标是替代zuul，在Springcloud2.0以上的版本中，没有对新版本的zuul2.0一盒收纳柜最新高性能版本进行集成，仍然还是使用的是Zuul1.x非Reactor模式的老版本。而为了提升网关的性能，SpringCloud Gateway 是基于WebFlux框架实现的，而WebFlux框架使用了高性能的Reactor模式通信框架Netty。</p></li><li><p>Spring Cloud Gateway的目标提供统一的路由方式且基于Filter链的方式提供网关的基本功能，例如：安全，监控/指标，和限流。</p></li></ol><h2 id="Spring-Cloud架构亮点"><a href="#Spring-Cloud架构亮点" class="headerlink" title="Spring Cloud架构亮点"></a>Spring Cloud架构亮点</h2><p><img src="/" class="lazyload" data-src="https://spring.io/images/cloud-diagram-1a4cad7294b4452864b5ff57175dd983.svg"  alt="SpringCloud架构图"></p><h2 id="特征"><a href="#特征" class="headerlink" title="特征"></a>特征</h2><h3 id="Spring-Cloud-Gateway功能："><a href="#Spring-Cloud-Gateway功能：" class="headerlink" title="Spring Cloud Gateway功能："></a>Spring Cloud Gateway功能：</h3><ul><li>基于Spring Framework 5，Project Reactor和Spring Boot 2.0构建</li><li>能够匹配任何请求属性上的路由。</li><li>谓词和过滤器特定于路由。</li><li>Hystrix断路器集成。</li><li>Spring Cloud DiscoveryClient集成</li><li>易于编写的谓词和过滤器</li><li>请求速率限制</li><li>路径改写</li></ul><h2 id="三大核心概念"><a href="#三大核心概念" class="headerlink" title="三大核心概念"></a>三大核心概念</h2><h3 id="Route（-路由）"><a href="#Route（-路由）" class="headerlink" title="Route（ 路由）"></a>Route（ 路由）</h3><pre><code>路由是构建网关的基本模块，它由ID，目标URI，一系列的断言和过滤器组成，如果断言为true则匹配该路由。</code></pre><h3 id="Predicate（断言）"><a href="#Predicate（断言）" class="headerlink" title="Predicate（断言）"></a>Predicate（断言）</h3><pre><code>开发人员可以匹配HTTP请求中的所有内容（例如请求头或请求参数），如果请求与断言相匹配则进行路由。</code></pre><h3 id="Filter（过滤）"><a href="#Filter（过滤）" class="headerlink" title="Filter（过滤）"></a>Filter（过滤）</h3><pre><code>值得是Spring框架中GatewayFilter的实例，使用过滤器，可以在请求被路由前或者之后对请求进行修改。</code></pre><h2 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h2><p><img src="/" class="lazyload" data-src="https://cloud.spring.io/spring-cloud-static/spring-cloud-gateway/2.2.2.RELEASE/reference/html/images/spring_cloud_gateway_diagram.png"  alt=""></p><pre><code>Clients make requests to Spring Cloud Gateway. If the Gateway Handler Mapping determines that a request matches a route, it is sent to the Gateway Web Handler. This handler runs the request through a filter chain that is specific to the request. The reason the filters are divided by the dotted line is that filters can run logic both before and after the proxy request is sent. All “pre” filter logic is executed. Then the proxy request is made. After the proxy request is made, the “post” filter logic is run.</code></pre><ol><li>客户端向Spring Cloud Gateway发出请求。然后在Gate Handler Mapping中找到与请求相匹配的路由，将其发送到Gateway Web Handler。</li><li>Handler再通过指定的过滤器来讲请求发送到实际的服务执行业务逻辑，然后返回。</li><li>过滤器之间用虚线分开是因为过滤器可能会在发送代理之前（“pre”）或之后（”post“）执行业务逻辑。</li><li>Filter在”pre“类型的过滤器可以做参数校验、权限校验、流量监控、日志输出、协议转换等</li><li>在”post“类型的过滤器中可以做响应内容、响应头修改，日志输出，流量监控等有着非常重要的作用。</li><li>核心逻辑：<strong>路由转发+执行过滤器链</strong></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;由于有如此众多的客户端和服务器，在云体系结构中包括一个API网关通常会很有帮助。网关可以负责保护和路由消息，隐藏服务，限制
      
    
    </summary>
    
    
      <category term="Springcloud" scheme="http://yoursite.com/categories/Springcloud/"/>
    
    
      <category term="导航" scheme="http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"/>
    
      <category term="分享" scheme="http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"/>
    
  </entry>
  
  <entry>
    <title>ZooKeeper</title>
    <link href="http://yoursite.com/2020/04/07/ZooKeeper/"/>
    <id>http://yoursite.com/2020/04/07/ZooKeeper/</id>
    <published>2020-04-07T01:45:00.000Z</published>
    <updated>2020-04-08T03:57:28.624Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><h3 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h3><ol><li>ZooKeeper是由Yahoo（雅虎）开发的，后来提供给Apache管理的一套开源的，用于进行分布式的框架</li><li>Zookeeper是根据Google的关于Chubby Lock来设计实现的</li><li>Zookeeper是是一个中心化分布式框架的管理框架</li><li>zookeeper.out：启动日志，可以查看启动失败原因</li></ol><h3 id="二、安装"><a href="#二、安装" class="headerlink" title="二、安装"></a>二、安装</h3><ol><li>单机模式：在一台机器上安装，往往只能启动这个框架的一部分的功能</li><li>伪分布式：在集群中安装，能够启动这个框架的所有功能</li><li>完全分布式：在集群中安装，能启动这个框架的所有功能</li></ol><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="一、基本理论"><a href="#一、基本理论" class="headerlink" title="一、基本理论"></a>一、基本理论</h3><ol><li>Zookeeper本身是一个树状结构 - znode树</li><li>根结点时/</li><li>每一个节点称之为Znode节点</li><li>每一个持久节点都可以挂载字节点，每一个临时节点都没有字节点</li><li>每一个节点都需要携带数据，这个数据往往是对节点的描述</li><li>不存在相对路径的说法,所有节点都需要从/计算</li><li>zookeeper将数据维系在磁盘以及内存中<ul><li>维系在内存中的目的是为了快速读写</li><li>维系在磁盘中的目的是为了防止奔溃</li></ul></li><li>zookeeper在磁盘中存储的位置在dataDir指定</li><li>理论上来说，是可以做缓存服务器来使用的 ，但实际开发中不会这么做。因为zookeeper的主要作用作为协调框架来使用的，如果作为缓存服务器使用会占用大量内存,降低zookeeper的协调效率</li><li>在节点下不能存在同名节点</li><li>在zookeeper中，会讲每一次的写操作看作是一个事务，然后给这个事务分配一个全局递增的编号，称之为事务id，简称Zxid</li></ol><h3 id="二、命令"><a href="#二、命令" class="headerlink" title="二、命令"></a>二、命令</h3><ol><li>ls /  查看根目录的字节点</li><li>create /video ‘manage video servers’  创建节点</li><li>delete /  删除节点下无字节点</li><li>rmr /video  递归删除</li><li>set /news ‘manage news servers’  修改节点信息</li><li>get /news  获取节点信息</li><li>create -e  /video ‘’  创建临时节点</li></ol><h3 id="三、节点信息"><a href="#三、节点信息" class="headerlink" title="三、节点信息"></a>三、节点信息</h3><ol><li>cZxid  创建事务id</li><li>ctime 创建时间</li><li>mZxid  修改事务id</li><li>mtime 修改时间</li><li>pZxid  字节点个数变化的事务id</li><li>cversion  字节点个数变化的次数</li><li>dataVersion  数据变化的次数</li><li>aclVersion  权限变化次数</li><li>ephemeralOwner</li><li>dataLength</li><li>numChildren  字节点的数目</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">manage news servers</span><br><span class="line">cZxid &#x3D; 0x9</span><br><span class="line">ctime &#x3D; Fri Apr 03 10:44:15 CST 2020</span><br><span class="line">mZxid &#x3D; 0xa</span><br><span class="line">mtime &#x3D; Fri Apr 03 10:44:42 CST 2020</span><br><span class="line">pZxid &#x3D; 0xd</span><br><span class="line">cversion &#x3D; 3</span><br><span class="line">dataVersion &#x3D; 1</span><br><span class="line">aclVersion &#x3D; 0</span><br><span class="line">ephemeralOwner &#x3D; 0x0</span><br><span class="line">dataLength &#x3D; 19</span><br><span class="line">numChildren &#x3D; 3</span><br></pre></td></tr></table></figure><h3 id="四、节点类型"><a href="#四、节点类型" class="headerlink" title="四、节点类型"></a>四、节点类型</h3><ul><li>顺序节点<ol><li>create -s /news/n ‘ ‘   s指的是sequence </li></ol></li><li>非顺序节点<ol start="2"><li>create /news/n ‘’</li></ol></li></ul><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><ol><li>在配置zookeeper的时候，并没有指定那台服务器节点成为leader，那台服务器是follower，但是zookeeper集群启动之后自动出现了leader和follower，那么说明zookeeper在启动过程中出现了选举过程。</li><li>在zookeeper集群刚刚启动的时候，这个时候每一个节点都会推荐自己成为leader，并且将自己的选举信息发送给其他节点。</li><li>节点之间会两两比较选举信息，比较成功的会进行下一轮选举，经过多轮选举，最终胜出的节点成为leader。</li></ol><h3 id="细节"><a href="#细节" class="headerlink" title="细节"></a>细节</h3><ol><li>选举信息：<br> a. 每一台服务器（节点）的最大事务id<br> b. 选举编号 -myid<br> c. 逻辑时钟值 -控制节点的选举轮数</li><li>比较原则：<br> a.先比较两个节点的事务id，谁大谁赢 - 事务id越大，说明接受的写操作越频繁<br> b. 如果节点的事务id越大一样，则比较myid，谁大谁赢 - 在zoo.cfg,要求server.x中的x不一致<br> c. 一个节点要想成为leader，至少要胜过一半的节点 - 过半性<br> d. 在zookeeper集群中为了保证稳定性，一旦确定leader，新加入的节点自动成为follower<br> e. 为了避免出现单点故障问题，一旦老leader出现问题，zookeeper集群会自动触发选举机制，会自动选觉一个新的节点成为leader。<br> f. 因为集群中出现多个leader，这个现象称之为脑裂<br> g.脑裂产生条件<ul><li>网络产生了分裂/隔离</li><li>分裂之后进行了选举<br>h. 在zookeeper中，当存活的解ID哪个数不足一半的时候，那么存活的节点之间不选觉也不对外提供服务 - 过半性<br>i. 在zookeeper集群中一般把节点个数设置为奇数以满足过半性<br>j.在zookeeper集群中选举出leader会分配一个全局递增的编号，也会通知其他节点，称之为epochid。在zookeeper集群中如果出现多个leader，会保留epochid最大的id为leader，同时将其他的leader节点的状态进行切换，切换成follower<br>k. 集群中节点的状态</li><li>voting/looking  选举状态</li><li>follower  追随者/跟随者</li><li>leader  领导者</li><li>observer  观察者<br>l.选觉状态下不对外提供服务</li></ul></li></ol><h2 id="ZAB"><a href="#ZAB" class="headerlink" title="ZAB"></a>ZAB</h2><h3 id="一、概述-1"><a href="#一、概述-1" class="headerlink" title="一、概述"></a>一、概述</h3><ol><li>ZAB（Zookeeper Atomic Broadcast）是一套专门为zookeeper设计的用于进行<strong>原子广播</strong>和<strong>崩溃恢复</strong>的协议。</li><li>ZAB基于2pc算法来进行设计，利用了过半性和PAXOS算法来进行了改善 - <strong>核心思想：一票否决</strong><h3 id="二、原子广播"><a href="#二、原子广播" class="headerlink" title="二、原子广播"></a>二、原子广播</h3></li><li>作用：保证数据的一致性 - 在zookeeper中访问任意一个节点获取到的数据都是相同的</li><li>原子广播是基于2pc算法来设计的，然后引入过半性来进行改进</li><li>2pc - two phase commit -二阶段提交 -实际过程中要么<strong>请求-提交</strong>，要么<strong>请求-中止</strong><ul><li>请求阶段：协调者收到任务之后，会将任务发给每一个参与者，等待参与者的反馈</li><li>调教阶段：如果协调者收到所有参与者的yes，那么协调者要求所有的参与者执行刚才的任务</li><li>中止阶段：如果协调者如果没有接受到所有参与者接受请求的信息，就会拒绝这次任务</li></ul></li><li>原子广播的流程：</li><li>记录日志失败的可能性<ul><li>日志文件被占用 - 此时记录的时候却是以只读模式 - 在计算机中，一个文件一旦被某个进程打开其他进程往往以只读模式来打开</li><li>磁盘管道损坏</li><li>磁盘存储已满</li></ul></li><li>当follower记录日志失败，但是leader却还要求follower执行这个任务的时候，follower就会向leader发送请求，重新获取任务然后leader会将任务放入到队列中发送给follower，重新记录日志；如果再次记录失败，会将重新给leader发送请求重新记录直到成功为止。 - 如果是第一种可能性导致日志记录失败，只要日志未见被释放那么日志就有可能重新记录成功；但是如果是后两种可能，那么就属于硬件环境问题</li></ol><h3 id="三、崩溃恢复"><a href="#三、崩溃恢复" class="headerlink" title="三、崩溃恢复"></a>三、崩溃恢复</h3><ol><li>作用：避免单点故障，保证集群的高可用</li><li>在zookeeper集群中，当leader节点丢失或宕机时，集群不会因为一个节点而不服务，而是会通过选举机制重新选取leader节点，这个过程叫崩溃恢复。</li><li>每一个新上任的leader会把自己的epochid分发给每一个follower，follower在收到epochid会把它存储到acceptepochid中。</li><li>当一个节点重启炼乳集群之后，这个节点会先寻找当前节点的最大事务id，将自己的最大事务id发送给leader进行比较，如果发现不一致，leader就会将所缺的操作重新放到队列中发送给follower重新补奇，补齐过程中这个follower暂时不会对外提供服务</li><li>在集群中，事务id实际上有64位二进制数字（16个十六进制数字）组成，其中高32位代表epochid，低32位代表的是事务id</li><li>version-2 中log中记录写操作，snap记录整个zookeeper的树结构</li></ol><h3 id="四、观察者"><a href="#四、观察者" class="headerlink" title="四、观察者"></a>四、观察者</h3><ol><li>观察者的特点：既不参与投票也不参与选举，但是会监听 投票或者选举结果，根据结果来执行对应的操作</li><li>在实际开发中，如果集群庞大，为了提高效率，往往将90%的节点设置成observer。如果在异构网络中，也会将绝大部分的节点设置为观察者</li><li>在zookeeper中observer不参与选举但是选举出leader之后，观察者听从leader命令；如果leader和follow二决定执行或者不指定某个操作的时候，observer也需要跟着执行或不执行。</li><li>observer没有投票权和选举权，所以observer是否存活并不影响过半。</li></ol><h3 id="五、-特性"><a href="#五、-特性" class="headerlink" title="五、 特性"></a>五、 特性</h3><ol><li>过半性：过半选举，过半存活，过半执行</li><li>原子性：要么所有的节点都执行请求，要么都不执行</li><li>数据一致性：访问任意节点所拿到的数据是一致的</li><li>顺序性：leader会讲请求发送到队列中发送给follower，所以保证leader和follower的请求的执行顺序是相同的</li><li>实时性：实时监控zookeeper的集群变化</li><li>可靠性：奔溃恢复 - 不会因为一个节点当即就导致整个集群不服务</li></ol><h2 id="AVRO"><a href="#AVRO" class="headerlink" title="AVRO"></a>AVRO</h2><h3 id="一、概述-2"><a href="#一、概述-2" class="headerlink" title="一、概述"></a>一、概述</h3><ol><li>AVRO是由Apache提供的一套用于序列化和RPC的机制</li><li>AVRO原本是Hadoop的子组件，后来AVRO被独立出来成为了一个顶级项目</li></ol><h3 id="二、序列化"><a href="#二、序列化" class="headerlink" title="二、序列化"></a>二、序列化</h3><ol><li>序列化实际上是指按照指定的格式将数据转化为其他形式</li><li>序列化的作用：为了方便数据的存储和传输</li><li>序列化的衡量标准：<ul><li>序列化花费的时间，占用的资源等</li><li>序列化之后产生的数据量的大小</li><li>考虑序列化机制本身是否跨平台跨语言<ul><li>如果数据要在不同的语言之间传输，那么意味着数据要做到数据与语言无关</li><li>数字，布尔值，字符/字符串</li><li>字符/字符串与编码有关-只要两种计算机语言用同一套码表就可以进行数据的传输</li></ul></li></ul></li><li>实际开发过程中，绝大部分的序列化机制都考虑转化为字符串-AVRO实际就是将对象转化为jason</li></ol><h3 id="三、RPC"><a href="#三、RPC" class="headerlink" title="三、RPC"></a>三、RPC</h3><ol><li>RPC(Remote Procedure Call,远程过程调用)允许程序员在一个节点（服务器）上去远程调用另一个节点上的方法而不用显示的实现这个方法</li><li>特点：简单，高效，通用</li><li>RPC的stub（存根）就是限制不同的节点上的方法签名是一致的，在Java中一般用接口</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;h3 id=&quot;一、概述&quot;&gt;&lt;a href=&quot;#一、概述&quot; class=&quot;headerlink&quot; title=&quot;一、概述&quot;&gt;&lt;/a&gt;一、概述&lt;/h
      
    
    </summary>
    
    
      <category term="大数据" scheme="http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="导航" scheme="http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"/>
    
      <category term="分享" scheme="http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"/>
    
  </entry>
  
  <entry>
    <title>JUC</title>
    <link href="http://yoursite.com/2020/03/31/JUC/"/>
    <id>http://yoursite.com/2020/03/31/JUC/</id>
    <published>2020-03-31T07:54:44.071Z</published>
    <updated>2020-04-02T15:36:45.595Z</updated>
    
    <content type="html"><![CDATA[<h2 id="原子性操作–Atomic-原子性"><a href="#原子性操作–Atomic-原子性" class="headerlink" title="原子性操作–Atomic 原子性"></a>原子性操作–Atomic 原子性</h2><ol><li>原子性操作实际上就是针对属性来提供线程安全的方法，在底层会自动采用CAS来保证线程安全。</li><li>volatile是Java提供的轻量级的线程同步机制</li></ol><h2 id="阻塞队列-BlockingQueue"><a href="#阻塞队列-BlockingQueue" class="headerlink" title="阻塞队列 BlockingQueue"></a>阻塞队列 BlockingQueue</h2><h3 id="原则："><a href="#原则：" class="headerlink" title="原则："></a>原则：</h3><ol><li>遵循FIFO</li><li>往往是有界的，容量固定不变</li><li>具有阻塞特性：如果队列已满，则试图放入的线程会被阻塞；如果队列为空，就会阻塞尝试获取的线程</li><li>不允许元素null</li></ol><h3 id="方法："><a href="#方法：" class="headerlink" title="方法："></a>方法：</h3><ol start="6"><li><table><thead><tr><th></th><th>抛出异常</th><th>返回特殊值</th><th>产生阻塞</th><th>定时阻塞</th></tr></thead><tbody><tr><td>添加</td><td>add - IllegalStateException</td><td>offer - false</td><td>put</td><td>offer</td></tr><tr><td>获取</td><td>remove - NoSuchElementException</td><td>poll - null</td><td>take</td><td>poll</td></tr></tbody></table></li></ol><h2 id="ConcurrentMap-并发映射"><a href="#ConcurrentMap-并发映射" class="headerlink" title="ConcurrentMap -并发映射"></a>ConcurrentMap -并发映射</h2><ol><li>ConcurrentHashMap底层基于数组h+链表来存储数据，数组的每一个位置称之为桶</li><li>默认初始容量16（桶的数量），加载因子0.75</li><li>如果指定容量,底层会进行计算，实际容量一定是2的n次方</li><li>每次扩容每次增加一倍的桶数,扩容之后需要rehash操作<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ConcurrentHashMap</span><span class="params">(<span class="keyword">int</span> initialCapacity)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (initialCapacity &lt; <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException();</span><br><span class="line">        <span class="keyword">int</span> cap = ((initialCapacity &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; <span class="number">1</span>)) ?</span><br><span class="line">                   MAXIMUM_CAPACITY :</span><br><span class="line">                   tableSizeFor(initialCapacity + (initialCapacity &gt;&gt;&gt; <span class="number">1</span>) + <span class="number">1</span>));</span><br><span class="line">        <span class="keyword">this</span>.sizeCtl = cap;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">//底层算法</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> <span class="title">tableSizeFor</span><span class="params">(<span class="keyword">int</span> c)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> n = c - <span class="number">1</span>;</span><br><span class="line">        n |= n &gt;&gt;&gt; <span class="number">1</span>;</span><br><span class="line">        n |= n &gt;&gt;&gt; <span class="number">2</span>;</span><br><span class="line">        n |= n &gt;&gt;&gt; <span class="number">4</span>;</span><br><span class="line">        n |= n &gt;&gt;&gt; <span class="number">8</span>;</span><br><span class="line">        n |= n &gt;&gt;&gt; <span class="number">16</span>;</span><br><span class="line">        <span class="keyword">return</span> (n &lt; <span class="number">0</span>) ? <span class="number">1</span> : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></li><li>分段锁，封桶锁机制，不是把整个映射锁起来</li><li>在jdk1.7引入读写锁<h2 id="ConcurrentNavigableMap-并发导航映射"><a href="#ConcurrentNavigableMap-并发导航映射" class="headerlink" title="ConcurrentNavigableMap - 并发导航映射"></a>ConcurrentNavigableMap - 并发导航映射</h2></li><li>ConcurrentNavigableMap提供了用于截取子映射的方法</li><li>ConcurrentNavigableMap本身是一个接口，所以更多的是使用它的实现类ConcurrentSkipListMap - 并发跳跃表映射</li></ol><h2 id="跳跃表"><a href="#跳跃表" class="headerlink" title="跳跃表"></a>跳跃表</h2><ol><li>针对有序集合</li><li>适合于增删少，查询多的场景</li><li>跳跃表可以进行多层提取，最上层的跳跃表的元素个数不能少于2个</li><li>跳跃表是典型的“以空间换时间“的产物</li><li>在跳跃表中新添元素提取到上层跳跃表，遵循抛硬币原则</li><li>跳跃表的时间复杂度是O(logn)<h2 id="ExecutorService"><a href="#ExecutorService" class="headerlink" title="ExecutorService"></a>ExecutorService</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3></li><li>本质上是一个线程池<br>线程池：减少服务器的线程的创建和销毁，做到连接的复用</li><li>刚开始创建时空的，里面不包含任何线程</li><li>每接受一个请求，线程池都会创建一个线程(核心线程）来处理这个请求，在创建线程的时候就需要指定线程的数量</li><li>在核心线程达到指定数量之前，来一个请求就会创建一个新的线程来处理这个请求，直到线程满了就不再创建</li><li>核心线程处理完这个请求不会销毁，等待下一个请求</li><li>如果核心线程全部沾满，新来的请求会暂存在工作队列，工作队列本身是一个阻塞式队列</li><li>当核心线程有空的时候，就会从工作队列中取出交给核心线程处理</li><li>如果工作队列被全部占用，有接受到新的请求，会将这个请求叫个临时线程来处理</li><li>当临时线程处理完请求之后不会立即结束，而是会存活一段时间，如果在这个时间段内接受到新的请求，会处理新的请求，如果没有接收到请求，就会被kill掉</li><li>临时线程不会从工作队列中拿出请求处理</li><li>当临时线程全部占用，那么新来的请求就会交给执行助手来拒绝处理，在实际开发中，如果需要拒绝请求，可能会产生多部操作，例如记录日志，页面跳转，请求重发</li><li>ScheduledExecutorService：定时调度执行器服务。在线程池的基础上加入了定时调度效果。这个线程池本身是很多定时调度机制的底层实现</li></ol><h2 id="内存模型"><a href="#内存模型" class="headerlink" title="内存模型"></a>内存模型</h2><pre><code>栈内存：执行方法- 计算 栈内存是线程独享的堆内存：存储对象，线程共享方法区：存储类信息（字节码，静态，常量）。线程共享的本地方法栈：执行本地方法，线程独享本地方法是指在Java中用native声明但使用其他语言实现的方法pc计数器/寄存器：指令计数，线程独享</code></pre><p>如果需要计算一台服务器的线程承载量，考虑线程独享部分。<br>在jdk1.8中，栈内存最小128k<br>pc计数器一般只占几个字节，可以忽略</p><h2 id="红黑树"><a href="#红黑树" class="headerlink" title="红黑树"></a>红黑树</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>a. 红黑树本质上一棵自平衡二叉查找树</p><h3 id="b-特征："><a href="#b-特征：" class="headerlink" title="b.特征："></a>b.特征：</h3><ul><li>i.    所有节点的颜色非红即黑</li><li>ii.    根节点为黑色</li><li>iii.    红节点的子节点一定黑节点，黑节点的子节点可以是红节点也可以是黑节点</li><li>iv.    最底层的叶子节点一定是黑色的空节点</li><li>v.    从根节点到任意一个叶子节点的路径中经过的黑色节点个数一致，即黑节点高度是一致的</li><li>vi.    新添的节点颜色一定是红的<h3 id="c-修正："><a href="#c-修正：" class="headerlink" title="c. 修正："></a>c. 修正：</h3></li><li>i.    涂色：父子节点为红，叔父节点为红，将父节点核叔父节点涂成黑色，然后将祖父节点涂成红色</li><li>ii.    左旋：父子节点为红，叔父节点为黑，且子节点是右子叶，那么以子节点为轴进行左旋</li><li>iii.    右旋：父子节点为红，叔父节点为黑，且子节点是左子叶，那么以父节点为轴进行右旋<h3 id="d-红黑树的时间复杂度是O-logn"><a href="#d-红黑树的时间复杂度是O-logn" class="headerlink" title="d. 红黑树的时间复杂度是O(logn)"></a>d. 红黑树的时间复杂度是O(logn)</h3></li></ul><h2 id="Callable"><a href="#Callable" class="headerlink" title="Callable"></a>Callable</h2><h3 id="定义："><a href="#定义：" class="headerlink" title="定义："></a>定义：</h3><ol><li>Callable是Java提供的一种定义线程的方式,在使用的时候通过泛型执行返回值类型</li></ol><h3 id="Callable和Runnable的区别"><a href="#Callable和Runnable的区别" class="headerlink" title="Callable和Runnable的区别"></a>Callable和Runnable的区别</h3><table><thead><tr><th></th><th align="left">Runnable</th><th>Callable</th></tr></thead><tbody><tr><td>返回值</td><td align="left">没有返回值</td><td>通过泛型来指定返回值的类型</td></tr><tr><td>启动方式</td><td align="left">1. 通过Thread的start方法启动 2. 通过线程池的submit或者是execute方法执行</td><td>通过线程池的submit方法执行</td></tr><tr><td>容错机制</td><td align="left">不允许抛出异常，所以不能利用全局方式(例如Spring中的异常通知)处理</td><td>允许抛出异常，所以可以利用切面或者是全局方式来处理异常</td></tr></tbody></table><hr><h2 id="分叉合并池"><a href="#分叉合并池" class="headerlink" title="分叉合并池"></a>分叉合并池</h2><ol><li>Fork：分叉。将一个大任务拆分成多个小任务交给多个线程执行</li><li>Join：合并。将拆分出来的小任务的执行结果进行汇总</li><li>分叉合并的目的是为了提高CPU的利用率</li><li>在数据量相对小的时候，循环会比分叉合并快；数据量越大，分叉合并的优势越明显</li><li>分叉合并在进行的时候，导致其他程序的执行效率显著降低，所以分叉合并一般是在周末的凌晨来进行</li><li>分叉合并在分配子线程的时候，尽量做到每个核上的任务均匀：少的多分，多的少分</li><li>在分叉合并中，为了减少”慢任务”带来的效率降低，采取”work-stealing”(工作窃取)策略：当一个核将它的任务队列处理完成之后，这个核并不会闲下来，会随机扫描一个核，然后从被扫描核的任务队列尾端”偷取”一个任务回来执行</li></ol><h2 id="Lock-锁"><a href="#Lock-锁" class="headerlink" title="Lock - 锁"></a>Lock - 锁</h2><h3 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h3><ol><li>相对于synchronized更加灵活和精细</li><li>ReadWriteLock - 读写锁<ul><li>a.    在使用的时候，需要用这个接口的实现类ReentrantReadWriteLock</li><li>b.    在加锁的时候，需要先通过对应的方法来获取读锁或者写锁</li></ul></li><li>公平和非公平策略<ul><li>a.    非公平策略下，线程会直接抢占执行权，在资源有限的前提下，线程之间抢到的次数不一样</li><li>b.    公平策略下，线程不是直接抢占执行权，而是去抢占入队顺序。宪曾之间的执行次数是基本一致的</li><li>c.    默认情况下，使用的是非公平策略</li><li>d.    相对而言，非公平策略的效率会更高一些</li></ul></li></ol><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><ol><li>CountDownLatch：闭锁/线程递减锁。对线程进行计数，在计数归零之前会让线程陷入阻塞，直到计数归零才会放开阻塞 - 一波线程结束之后开启另一波线程</li><li>CyclicBarrier：栅栏。对线程进行计数，在计数归零之前让线程陷入阻塞，直到计数归零为止才会放开阻塞 - 线程到达同一个地点之后再分别执行</li><li>Exchanger：交换机。用于交换2个线程之间的信息</li><li>Semaphore：信号量。线程只有获取到信号之后才能执行，执行完成之后需要释放信号。如果所有的信号都被占用，那么后来的线程就会被阻塞</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;原子性操作–Atomic-原子性&quot;&gt;&lt;a href=&quot;#原子性操作–Atomic-原子性&quot; class=&quot;headerlink&quot; title=&quot;原子性操作–Atomic 原子性&quot;&gt;&lt;/a&gt;原子性操作–Atomic 原子性&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;原子性操作实际上
      
    
    </summary>
    
    
      <category term="JUC" scheme="http://yoursite.com/categories/JUC/"/>
    
    
      <category term="导航" scheme="http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"/>
    
      <category term="分享" scheme="http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"/>
    
  </entry>
  
  <entry>
    <title>高并发基础</title>
    <link href="http://yoursite.com/2020/03/31/%E9%AB%98%E5%B9%B6%E5%8F%91%E5%9F%BA%E7%A1%80/"/>
    <id>http://yoursite.com/2020/03/31/%E9%AB%98%E5%B9%B6%E5%8F%91%E5%9F%BA%E7%A1%80/</id>
    <published>2020-03-31T01:11:40.883Z</published>
    <updated>2020-04-01T01:08:18.732Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>a. 同步：一个对象或者一段逻辑在一段时间只允许被一个线程访问<br>b. 异步：一个对象或者一段逻辑在一段时间只允许被多个线程访问<br>c. 阻塞：一个线程只要没有拿到想要的结果就会一直等在这儿<br>d. 非阻塞：一个线程不管有没有拿到结果，都会继续执行下面的程序，不会在那边的等待</p><h3 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h3><p>a. BIO ： 同步式阻塞线程<br>b. NIO ： 同步式非阻塞式线程 JDK1.4<br>c. AIO ： 异步式非阻塞式线程 JDK1.8，目前没有成熟的框架</p><h3 id="BIO的缺点"><a href="#BIO的缺点" class="headerlink" title="BIO的缺点"></a>BIO的缺点</h3><ol><li>一对一连接：每次有一个客户端发起连接，服务端就要产生一个线程来处理这个连接；如果有大量的客户端发起连接，就会产生大量的线程，可能导致服务端崩溃。</li><li>无效连接 如果客户端连接后不做任何操作但是一直保持连接，服务端的处理线程就不会释放，导致服务端</li><li>阻塞 效率低<h3 id="NIO三大组件"><a href="#NIO三大组件" class="headerlink" title="NIO三大组件"></a>NIO三大组件</h3><h4 id="Buffer-缓冲区"><a href="#Buffer-缓冲区" class="headerlink" title="Buffer-缓冲区"></a>Buffer-缓冲区</h4></li><li>作用：存储数据</li><li>底层是基于数组来存储数据,针对基本数据类型，提供7个子类：ByteBuffer，ShortBuffer，InteBuffer，LongBuffer，FloatBuffer，DoubleBuffer，CharBuffer</li><li>重要位置<br>a.capacity 容量位<br>b.position 操作位<br>c.limit 限制位<br>d. mark 标记位<h3 id="4-重要操作"><a href="#4-重要操作" class="headerlink" title="4.重要操作"></a>4.重要操作</h3><h3 id="反转缓冲区"><a href="#反转缓冲区" class="headerlink" title="反转缓冲区"></a>反转缓冲区</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//byteBuffer.flip();   </span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> Buffer <span class="title">flip</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        limit = position;</span><br><span class="line">        position = <span class="number">0</span>;</span><br><span class="line">        mark = -<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h3 id="clear清空缓冲区"><a href="#clear清空缓冲区" class="headerlink" title="clear清空缓冲区"></a>clear清空缓冲区</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> Buffer <span class="title">clear</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        position = <span class="number">0</span>;</span><br><span class="line">        limit = capacity;</span><br><span class="line">        mark = -<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h3 id="reset-重置缓冲区"><a href="#reset-重置缓冲区" class="headerlink" title="reset 重置缓冲区"></a>reset 重置缓冲区</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> Buffer <span class="title">reset</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> m = mark;</span><br><span class="line">        <span class="keyword">if</span> (m &lt; <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> InvalidMarkException();</span><br><span class="line">        position = m;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h3 id="rewind-重绕缓冲区"><a href="#rewind-重绕缓冲区" class="headerlink" title="rewind 重绕缓冲区"></a>rewind 重绕缓冲区</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> Buffer <span class="title">rewind</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        position = <span class="number">0</span>;</span><br><span class="line">        mark = -<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h3 id="Channel-双向传输网路通信"><a href="#Channel-双向传输网路通信" class="headerlink" title="Channel 双向传输网路通信"></a>Channel 双向传输网路通信</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//client端</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">client</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">//开启通道</span></span><br><span class="line">        SocketChannel sc = SocketChannel.open();</span><br><span class="line">        <span class="comment">//手动设置非阻塞</span></span><br><span class="line">        sc.configureBlocking(<span class="keyword">false</span>);</span><br><span class="line">        <span class="comment">//一旦设置为非阻塞，无论是否建立连接，都会往下执行</span></span><br><span class="line">        <span class="comment">//发起连接，channel默认是阻塞的</span></span><br><span class="line">        sc.connect(<span class="keyword">new</span> InetSocketAddress(<span class="string">"localhost"</span>,<span class="number">8100</span>));</span><br><span class="line">        <span class="comment">//判断是否建立连接</span></span><br><span class="line">        <span class="keyword">while</span> (!sc.isConnected())&#123;</span><br><span class="line">            <span class="comment">//试图再次建立连接,这个方法会自动计数，如果多次建立无果，会自动抛出异常</span></span><br><span class="line">            sc.finishConnect();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//写出数据，都需要将数据以字节</span></span><br><span class="line">        sc.write(ByteBuffer.wrap(<span class="string">"one world one dream"</span>.getBytes()));</span><br><span class="line">        ByteBuffer buffer = ByteBuffer.allocate(<span class="number">1024</span>);</span><br><span class="line">        Thread.sleep(<span class="number">10</span>);</span><br><span class="line">        sc.read(buffer);</span><br><span class="line">        System.out.println(<span class="keyword">new</span> java.lang.String(buffer.array(),<span class="number">0</span>,buffer.position()));</span><br><span class="line">        sc.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//sever端</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">server</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="comment">//开启通道</span></span><br><span class="line">        ServerSocketChannel ssc = ServerSocketChannel.open();</span><br><span class="line">        <span class="comment">//绑定监听端口</span></span><br><span class="line">        ssc.bind(<span class="keyword">new</span> InetSocketAddress(<span class="number">8100</span>));</span><br><span class="line">        <span class="comment">//手动设置非阻塞</span></span><br><span class="line">        ssc.configureBlocking(<span class="keyword">false</span>);</span><br><span class="line">        <span class="comment">//无论是否建立连接</span></span><br><span class="line">        <span class="comment">//接受连接</span></span><br><span class="line">        SocketChannel sc = ssc.accept();</span><br><span class="line">        <span class="keyword">while</span> (sc==<span class="keyword">null</span>)&#123;</span><br><span class="line">            sc=ssc.accept();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//准备容器存储数据</span></span><br><span class="line">        ByteBuffer buffer = ByteBuffer.allocate(<span class="number">1024</span>);</span><br><span class="line">        sc.read(buffer);</span><br><span class="line">        System.out.println(<span class="keyword">new</span> String(buffer.array(),<span class="number">0</span>,buffer.position()));</span><br><span class="line">        sc.write(ByteBuffer.wrap(<span class="string">"hello sever"</span>.getBytes()));</span><br><span class="line">        ssc.close();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Selector-多路复用选择器"><a href="#Selector-多路复用选择器" class="headerlink" title="Selector 多路复用选择器"></a>Selector 多路复用选择器</h3></li><li></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;基本概念&quot;&gt;&lt;a href=&quot;#基本概念&quot; class=&quot;headerlink&quot; title=&quot;基本概念&quot;&gt;&lt;/a&gt;基本概念&lt;/h2&gt;&lt;p&gt;a. 同步：一个对象或者一段逻辑在一段时间只允许被一个线程访问&lt;br&gt;b. 异步：一个对象或者一段逻辑在一段时间只允许被多个
      
    
    </summary>
    
    
      <category term="高并发" scheme="http://yoursite.com/categories/%E9%AB%98%E5%B9%B6%E5%8F%91/"/>
    
    
      <category term="导航" scheme="http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"/>
    
      <category term="分享" scheme="http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"/>
    
  </entry>
  
  <entry>
    <title>InterviewDay01</title>
    <link href="http://yoursite.com/2020/03/18/interview/"/>
    <id>http://yoursite.com/2020/03/18/interview/</id>
    <published>2020-03-18T15:48:06.390Z</published>
    <updated>2020-04-17T13:19:53.586Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-volatile是什么？"><a href="#1-volatile是什么？" class="headerlink" title="1.volatile是什么？"></a>1.volatile是什么？</h2><h3 id="volatile是一种轻量级的同步机制"><a href="#volatile是一种轻量级的同步机制" class="headerlink" title="volatile是一种轻量级的同步机制"></a>volatile是一种轻量级的同步机制</h3><h3 id="volatile有三个特性："><a href="#volatile有三个特性：" class="headerlink" title="volatile有三个特性："></a>volatile有三个特性：</h3><ol><li><p>可见性<br>当多线程修改物理内存种的变量值时，会把内存中的变量值拷贝到各自的工作内存中，用volatile关键字修饰的变量，当一个线程在自己的工作内存中修改完成，写入物理内存后会通知其他线程数据已经修改，这就是可见性</p></li><li><p>不保证原子性<br>在多线程都在实现如num++；可能回导致运算时加塞的情况，导致最后数据的不一致，这类可以用atomicInteger.getAndIncrement()实现原子性</p></li><li><p>禁止指令重拍<br>为了性能的优化，jvm在不改变正确语义的前提下，会对代码的执行顺序进行优化，底层进行指令的重拍，在多线程的情况下可能导致数据的错误，加上volatile可以禁止指令重拍</p><h2 id="Synchronized和Volatile的比较"><a href="#Synchronized和Volatile的比较" class="headerlink" title="Synchronized和Volatile的比较"></a>Synchronized和Volatile的比较</h2><p> 1）Synchronized保证内存可见性和操作的原子性<br> 2）Volatile只能保证内存可见性<br> 3）Volatile不需要加锁，比Synchronized更轻量级，并不会阻塞线程（volatile不会造成线程的阻塞；synchronized可能会造成线程的阻塞。）<br> 4）volatile标记的变量不会被编译器优化,而synchronized标记的变量可以被编译器优化（如编译器重排序的优化）.<br> 5）volatile是变量修饰符，仅能用于变量，而synchronized是一个方法或块的修饰符。<br>   volatile本质是在告诉JVM当前变量在寄存器中的值是不确定的，使用前，需要先从主存中读取，因此可以实现可见性。而对n=n+1,n++等操作时，volatile关键字将失效，不能起到像synchronized一样的线程同步（原子性）的效果。</p></li></ol><h2 id="单例模式在多线程环境下可能存在安全问题"><a href="#单例模式在多线程环境下可能存在安全问题" class="headerlink" title="单例模式在多线程环境下可能存在安全问题"></a>单例模式在多线程环境下可能存在安全问题</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//推荐使用双重检查,解决多线程单例模式的安全问题</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Singletlon</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Singtlelon instance =<span class="keyword">null</span>;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">Singtlon</span><span class="params">()</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Singtlelon <span class="title">getInstance</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(instance==<span class="keyword">null</span>)&#123;</span><br><span class="line">            <span class="keyword">synchronized</span>(Singtlon<span class="class">.<span class="keyword">class</span>)</span>&#123;</span><br><span class="line">                <span class="keyword">if</span>(instance==<span class="keyword">null</span>)&#123;</span><br><span class="line">                    instance=<span class="keyword">new</span> Singletlon();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> instance;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2-CAS是什么？"><a href="#2-CAS是什么？" class="headerlink" title="2.CAS是什么？"></a>2.CAS是什么？</h2><p>compare andswap 比较且交换<br>在多线程的环境下，对数据惊醒修改，会将线程工作内存的数值与物理内存的期望值相比较如果相同就交换到更新值，返回true，如果不想等，则返回false<br>底层主要是通过unsafe类来实现原子性</p><h2 id="3-底层是通过do-while的自旋锁实现的，在多线程高并发的情况下，导致循环时间过长，引起cpu的开销很大"><a href="#3-底层是通过do-while的自旋锁实现的，在多线程高并发的情况下，导致循环时间过长，引起cpu的开销很大" class="headerlink" title="3. 底层是通过do while的自旋锁实现的，在多线程高并发的情况下，导致循环时间过长，引起cpu的开销很大"></a>3. 底层是通过do while的自旋锁实现的，在多线程高并发的情况下，导致循环时间过长，引起cpu的开销很大</h2><p>只能保证一个变量的原子操作<br>会导致ABA问题，只关心首尾的数值，忽略中间数值已经被修改过，这样会造成数据错乱的问题</p><h2 id="如何解决ABA问题"><a href="#如何解决ABA问题" class="headerlink" title="如何解决ABA问题"></a>如何解决ABA问题</h2><h2 id="用AtomicStampedReference-AtomicMarkableReference原子引用解决ABA问题"><a href="#用AtomicStampedReference-AtomicMarkableReference原子引用解决ABA问题" class="headerlink" title="用AtomicStampedReference/AtomicMarkableReference原子引用解决ABA问题"></a>用AtomicStampedReference/AtomicMarkableReference原子引用解决ABA问题</h2><p>类似于版本控制，时间戳管控</p><h2 id="集合类不安全之并发修改异常"><a href="#集合类不安全之并发修改异常" class="headerlink" title="集合类不安全之并发修改异常"></a>集合类不安全之并发修改异常</h2><h3 id="arraylist在多线程的情况下造成安全并发的问题，常见的异常ConcurrentModificationException-并发修改异常"><a href="#arraylist在多线程的情况下造成安全并发的问题，常见的异常ConcurrentModificationException-并发修改异常" class="headerlink" title="arraylist在多线程的情况下造成安全并发的问题，常见的异常ConcurrentModificationException 并发修改异常"></a>arraylist在多线程的情况下造成安全并发的问题，常见的异常ConcurrentModificationException 并发修改异常</h3><h4 id="如何解决arraylist线程不安全的问题"><a href="#如何解决arraylist线程不安全的问题" class="headerlink" title="如何解决arraylist线程不安全的问题"></a>如何解决arraylist线程不安全的问题</h4><ol><li>使用vector代替arraylist，因为vector是线程安全的，底部在add方法上加入了synchronized同步锁</li><li>使用Collections集合工具类，调用Collections上的synchronizedList()这个方法可以解决arraylist线程不安全的问题</li><li>使用JUC下的CopyOnWriteArrayList这个类来解决线程不安全的问题,底层使用来lock锁来完成这一机制<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> E <span class="title">set</span><span class="params">(<span class="keyword">int</span> index, E element)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> ReentrantLock lock = <span class="keyword">this</span>.lock;</span><br><span class="line">        lock.lock();</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Object[] elements = getArray();</span><br><span class="line">            E oldValue = get(elements, index);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (oldValue != element) &#123;</span><br><span class="line">                <span class="keyword">int</span> len = elements.length;</span><br><span class="line">                Object[] newElements = Arrays.copyOf(elements, len);</span><br><span class="line">                newElements[index] = element;</span><br><span class="line">                setArray(newElements);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// Not quite a no-op; ensures volatile write semantics</span></span><br><span class="line">                setArray(elements);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> oldValue;</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            lock.unlock();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></li></ol><h2 id="HsahSet底层是HashMap，因为hashset的底层将value写成来一个最终常量，hashset存储数据不关心value值"><a href="#HsahSet底层是HashMap，因为hashset的底层将value写成来一个最终常量，hashset存储数据不关心value值" class="headerlink" title="HsahSet底层是HashMap，因为hashset的底层将value写成来一个最终常量，hashset存储数据不关心value值"></a>HsahSet底层是HashMap，因为hashset的底层将value写成来一个最终常量，hashset存储数据不关心value值</h2><h2 id="基本数据类型传值（相当于复制了一份）-，对象的引用是地址的传递（相当于指针）"><a href="#基本数据类型传值（相当于复制了一份）-，对象的引用是地址的传递（相当于指针）" class="headerlink" title="基本数据类型传值（相当于复制了一份） ，对象的引用是地址的传递（相当于指针）"></a>基本数据类型传值（相当于复制了一份） ，对象的引用是地址的传递（相当于指针）</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">String str =“aaa”<span class="comment">//存储在常量池</span></span><br><span class="line"><span class="comment">//会在常量池中找相应的值，有就找没有就会新建，注意内存的分析</span></span><br></pre></td></tr></table></figure><h2 id="公平锁和非公平锁"><a href="#公平锁和非公平锁" class="headerlink" title="公平锁和非公平锁"></a>公平锁和非公平锁</h2><h3 id="公平锁：线程按照指定的顺序来占锁，按顺序执行"><a href="#公平锁：线程按照指定的顺序来占锁，按顺序执行" class="headerlink" title="公平锁：线程按照指定的顺序来占锁，按顺序执行"></a>公平锁：线程按照指定的顺序来占锁，按顺序执行</h3><h3 id="非公平锁，允许其他线程加塞，上来就抢占锁，占不到按顺序执行。如synchronized"><a href="#非公平锁，允许其他线程加塞，上来就抢占锁，占不到按顺序执行。如synchronized" class="headerlink" title="非公平锁，允许其他线程加塞，上来就抢占锁，占不到按顺序执行。如synchronized"></a>非公平锁，允许其他线程加塞，上来就抢占锁，占不到按顺序执行。如synchronized</h3><h2 id="Java-ReentrantLock-而言"><a href="#Java-ReentrantLock-而言" class="headerlink" title="Java ReentrantLock 而言"></a>Java ReentrantLock 而言</h2><p>可以通过构造函数来指定是公平锁还是非公平锁，默认是非公平锁，如果参数列表指定true则为公平锁，非公平锁吞吐量比较大，执行效率高。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//无参构造</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Creates an instance of &#123;<span class="doctag">@code</span> ReentrantLock&#125;.</span></span><br><span class="line"><span class="comment">     * This is equivalent to using &#123;<span class="doctag">@code</span> ReentrantLock(false)&#125;.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">ReentrantLock</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        sync = <span class="keyword">new</span> NonfairSync();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//含参构造</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Creates an instance of &#123;<span class="doctag">@code</span> ReentrantLock&#125; with the</span></span><br><span class="line"><span class="comment">     * given fairness policy.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> fair &#123;<span class="doctag">@code</span> true&#125; if this lock should use a fair ordering policy</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">ReentrantLock</span><span class="params">(<span class="keyword">boolean</span> fair)</span> </span>&#123;</span><br><span class="line">        sync = fair ? <span class="keyword">new</span> FairSync() : <span class="keyword">new</span> NonfairSync();</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h2 id="可重入锁（ReentrantLock，递归锁）"><a href="#可重入锁（ReentrantLock，递归锁）" class="headerlink" title="可重入锁（ReentrantLock，递归锁）"></a>可重入锁（ReentrantLock，递归锁）</h2><p>定义：线程可以进入任何一个它已经拥有的锁所同步着的代码块<br>ReentrantLock and synchronized 是可重入锁，默认非公平，防止死锁，相当于用的是同一把锁</p><h2 id="自旋锁"><a href="#自旋锁" class="headerlink" title="自旋锁"></a>自旋锁</h2><p>定义：是指尝试获取锁的线程不会立即阻塞，而是采用循环的方式去尝试获取锁，这样的好处是减少线程上下文切换的消耗，缺点是循环会消耗cpu</p><h2 id="独占锁ReentrantLock和synchronized"><a href="#独占锁ReentrantLock和synchronized" class="headerlink" title="独占锁ReentrantLock和synchronized"></a>独占锁ReentrantLock和synchronized</h2><p>该锁一次只能被一个线程所占有</p><h2 id="共享锁"><a href="#共享锁" class="headerlink" title="共享锁"></a>共享锁</h2><p>多个线程同时读一个资源类没有任何问题，所以为了满足并发量，读取共享资源应该可以同时进行。</p><ol><li>读读可以共存</li><li>读写不可以共存</li><li>写写不可以共存</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-volatile是什么？&quot;&gt;&lt;a href=&quot;#1-volatile是什么？&quot; class=&quot;headerlink&quot; title=&quot;1.volatile是什么？&quot;&gt;&lt;/a&gt;1.volatile是什么？&lt;/h2&gt;&lt;h3 id=&quot;volatile是一种轻量级的同步
      
    
    </summary>
    
    
      <category term="Interview" scheme="http://yoursite.com/categories/Interview/"/>
    
    
      <category term="导航" scheme="http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"/>
    
      <category term="分享" scheme="http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"/>
    
  </entry>
  
  <entry>
    <title>事务</title>
    <link href="http://yoursite.com/2020/02/29/%E4%BA%8B%E5%8A%A1/"/>
    <id>http://yoursite.com/2020/02/29/%E4%BA%8B%E5%8A%A1/</id>
    <published>2020-02-28T16:00:00.000Z</published>
    <updated>2020-02-29T03:09:41.839Z</updated>
    
    <content type="html"><![CDATA[<h2 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h2><p>逻辑上的一组操作，要么同时成功，要么同时失败</p><h2 id="事务的四大特性（ACID）"><a href="#事务的四大特性（ACID）" class="headerlink" title="事务的四大特性（ACID）"></a>事务的四大特性（ACID）</h2><ol><li>原子性<br>一个事务是一个不可分割的整体，要么同时成功，要么同时失败</li><li>一致性<br>一个事务执行之前和之后数据也应该是完整的</li><li>隔离性<br>多个并发的事务应该是独立的，互不影响的。</li><li>持久性<br>事务一旦提交，事务对数据库的影响就真实发生了，无论做任何操作，这种影响无法被撤销。<h3 id="隔离性"><a href="#隔离性" class="headerlink" title="隔离性"></a>隔离性</h3>脏读：<br>一个用户读取到另一个用户还未提交的数据，产生脏读<br>不可重复读<br>一个事务可以读取另一个事务已经提交的数据<br>虚读（幻读）<br>一个事务可以读取到另一个事务对整表数据增删改<h2 id="数据库事务的隔离级别"><a href="#数据库事务的隔离级别" class="headerlink" title="数据库事务的隔离级别"></a>数据库事务的隔离级别</h2>read uncommited; - 读未提交 会出现脏读，不可重复读，虚读的问题<br>read commited  -  读已提交 可以防止脏读，<br>repeatable red    可重复读  可以防止脏读和不可重复读<br>serializable  - 序列化  都可以避免，但效率低下<h2 id="锁机制"><a href="#锁机制" class="headerlink" title="锁机制"></a>锁机制</h2> 共享锁<br> 共享锁和共享锁可以共存<br> 共享锁和排他锁不能共存<br> 排它锁<br> 排他锁和共享锁不能共存<br> 排他锁和排他锁不能共存</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;事务&quot;&gt;&lt;a href=&quot;#事务&quot; class=&quot;headerlink&quot; title=&quot;事务&quot;&gt;&lt;/a&gt;事务&lt;/h2&gt;&lt;p&gt;逻辑上的一组操作，要么同时成功，要么同时失败&lt;/p&gt;
&lt;h2 id=&quot;事务的四大特性（ACID）&quot;&gt;&lt;a href=&quot;#事务的四大特性（AC
      
    
    </summary>
    
    
      <category term="Web" scheme="http://yoursite.com/categories/Web/"/>
    
    
      <category term="导航" scheme="http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"/>
    
      <category term="分享" scheme="http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"/>
    
  </entry>
  
  <entry>
    <title>JavaSE04</title>
    <link href="http://yoursite.com/2020/02/28/JavaSE04/"/>
    <id>http://yoursite.com/2020/02/28/JavaSE04/</id>
    <published>2020-02-28T14:59:51.201Z</published>
    <updated>2020-02-28T14:59:51.201Z</updated>
    
    <content type="html"><![CDATA[<hr><p>title: 线程<br>date: 2020-02-28<br>tags:<br>    - 分享<br>    - 导航<br>categories: Java基础<br>–</p><h2 id="程序"><a href="#程序" class="headerlink" title="程序"></a>程序</h2><p>为完成特定的任务，用某种语言编写一组指令的集合。一段静态的代码，静态对象</p><h2 id="进程"><a href="#进程" class="headerlink" title="进程"></a>进程</h2><p>正在运行的程序</p><h2 id="线程"><a href="#线程" class="headerlink" title="线程"></a>线程</h2><p>进程可以进一步细化为线程，是一个程序内部的执行路径</p><p>-</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;hr&gt;
&lt;p&gt;title: 线程&lt;br&gt;date: 2020-02-28&lt;br&gt;tags:&lt;br&gt;    - 分享&lt;br&gt;    - 导航&lt;br&gt;categories: Java基础&lt;br&gt;–&lt;/p&gt;
&lt;h2 id=&quot;程序&quot;&gt;&lt;a href=&quot;#程序&quot; class=&quot;heade
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>JavaSE03</title>
    <link href="http://yoursite.com/2020/02/28/JavaSE03/"/>
    <id>http://yoursite.com/2020/02/28/JavaSE03/</id>
    <published>2020-02-28T13:02:22.563Z</published>
    <updated>2020-02-28T13:02:22.563Z</updated>
    
    <content type="html"><![CDATA[<hr><p>title: Collection(集合)<br>date: 2020-02-28<br>tags:<br>    - 分享<br>    - 导航<br>categories: Java基础<br>–</p><h2 id="泛型"><a href="#泛型" class="headerlink" title="泛型"></a>泛型</h2><p>泛型的本质是数据类型的参数化，在编译器阶段处理</p><p>-</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;hr&gt;
&lt;p&gt;title: Collection(集合)&lt;br&gt;date: 2020-02-28&lt;br&gt;tags:&lt;br&gt;    - 分享&lt;br&gt;    - 导航&lt;br&gt;categories: Java基础&lt;br&gt;–&lt;/p&gt;
&lt;h2 id=&quot;泛型&quot;&gt;&lt;a href=&quot;#泛型&quot; 
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>JVM</title>
    <link href="http://yoursite.com/2020/02/28/JVM/"/>
    <id>http://yoursite.com/2020/02/28/JVM/</id>
    <published>2020-02-27T16:00:00.000Z</published>
    <updated>2020-04-24T14:43:03.503Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
      <category term="JVM虚拟机" scheme="http://yoursite.com/categories/JVM%E8%99%9A%E6%8B%9F%E6%9C%BA/"/>
    
    
      <category term="导航" scheme="http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"/>
    
      <category term="分享" scheme="http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"/>
    
  </entry>
  
  <entry>
    <title>JavaSE01</title>
    <link href="http://yoursite.com/2020/02/26/JavaSE01/"/>
    <id>http://yoursite.com/2020/02/26/JavaSE01/</id>
    <published>2020-02-25T16:00:00.000Z</published>
    <updated>2020-02-26T13:35:16.531Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-简单说一下什么是跨平台"><a href="#1-简单说一下什么是跨平台" class="headerlink" title="1. 简单说一下什么是跨平台"></a>1. 简单说一下什么是跨平台</h2><p>由于各种系统所支持的指令集不是完全一致，所以在操作系统上加个虚拟机可以来提供统一的接口，屏蔽系统之间的差异。<br>2.Java有几种基本数据类型<br>8种基本数据类型<br>数据类型    字节    默认值<br> byte        1     0<br> short        2    0<br> int        4    0<br> long        8    0<br> double        4    0.0d<br> float        8       0.0f<br> char        2       ‘\u0000’<br> boolean    1    false</p><h2 id="3-面向对象的特征"><a href="#3-面向对象的特征" class="headerlink" title="3.面向对象的特征"></a>3.面向对象的特征</h2><ol><li><p>封装 把描述一个对象的属性和行为封装在一个模块中，也就是封装到一个类中，用变量来定义对象的属性，用方法来定义对象的行为，方法可以直接访问同一对象的属性</p></li><li><p>继承 发生在父子类中，子类可以继承父类的特征和行为，子类继承父类的非private修饰的方法，子类也可以对父类的方法进行重写，缺点是增加了代码之间的耦合性</p></li><li><p>多态 不同子类型的对象对同一消息作出不同的响应<br>分为编译时多态和运行时多态<br>编译时多态：常见的方法的重载，也就是一个类中存在多个方法名相同，而参数列表不同的方法<br>运行时多态 方法的重写实现的是运行时多态，重写指的是子类重写父类的方法，重写的原则是，子类和父类的方法签名相同，也就是子类方法的返回值类型，方法的参数列表，以及方法名要与父类方法相同，子类抛出的异常小于等于父类抛出的异常，子类方法的访问修饰权限要&gt;=父类的访问修饰符权限，<br>向上造型：用父类型引用子类型对象，这样调用同样的方法就会根据子类对象的不同而表现出不一样的行为。</p></li><li><p>抽象<br>抽象是将一类对象的共同特征总结出来构造类的过程，包括数据抽象和行为抽象两方面。抽象只关注对象有那些属性和行为，并不关注这些行为的细节是什么？</p><h2 id="4-为什么要有包装类型"><a href="#4-为什么要有包装类型" class="headerlink" title="4.为什么要有包装类型"></a>4.为什么要有包装类型</h2><p>让基本数据类型也具有对象的特征。<br>jdk1.5的新的性<br>自动装箱：<code>Integer i = 1 ;</code> 把基本数据类型转化为包装类，底层用的<code>Integer.valueof(1)</code><br>自动拆箱：<code>int i =new Integer(6)</code> 把包装类型转化为基本数据类型 <code>i.intValue();</code> </p><h3 id="二者的区别："><a href="#二者的区别：" class="headerlink" title="二者的区别："></a>二者的区别：</h3></li><li><p>声明方式不同：基本类型不使用new关键字，而包装类型需要使用new关键字在堆中分配内存</p></li><li><p>存储方式及位置不同：基本类型是直接将变量存储在栈中，而包装类型是将对象放在堆中，然后通过引用来使用；</p></li><li><p>初始值不同：基本类型的初始值为0，boolean为false，而包装类型的初始值为null</p></li><li><p>使用方式不同：基本类型直接赋值直接使用就好，而包装类型在集合如Collection、Map时会使用到。</p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-简单说一下什么是跨平台&quot;&gt;&lt;a href=&quot;#1-简单说一下什么是跨平台&quot; class=&quot;headerlink&quot; title=&quot;1. 简单说一下什么是跨平台&quot;&gt;&lt;/a&gt;1. 简单说一下什么是跨平台&lt;/h2&gt;&lt;p&gt;由于各种系统所支持的指令集不是完全一致，所以在操
      
    
    </summary>
    
    
      <category term="Java基础" scheme="http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"/>
    
    
      <category term="导航" scheme="http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"/>
    
      <category term="分享" scheme="http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"/>
    
  </entry>
  
  <entry>
    <title>Interface(接口）</title>
    <link href="http://yoursite.com/2020/02/26/JavaSE02/"/>
    <id>http://yoursite.com/2020/02/26/JavaSE02/</id>
    <published>2020-02-25T16:00:00.000Z</published>
    <updated>2020-02-28T12:27:41.878Z</updated>
    
    <content type="html"><![CDATA[<h2 id="抽象类和抽象方法"><a href="#抽象类和抽象方法" class="headerlink" title="抽象类和抽象方法"></a>抽象类和抽象方法</h2><h3 id="抽象类：如果一个类中有一个或多个抽象方法，那么这个类必须是抽象类。"><a href="#抽象类：如果一个类中有一个或多个抽象方法，那么这个类必须是抽象类。" class="headerlink" title="抽象类：如果一个类中有一个或多个抽象方法，那么这个类必须是抽象类。"></a>抽象类：如果一个类中有一个或多个抽象方法，那么这个类必须是抽象类。</h3><ol><li>抽象类不能创建对象，因为抽象类创建对象没有意义,但可以创建抽象类的子类对象</li><li>子类继承抽象类需要重写抽象类中的抽象方法，如果不重写这个类必须是抽象类，否则会产生编译时异常</li><li>抽象类中可以没有抽象方法,</li><li>可以向上造型，引用父类类型创建子类对象，调用子类方法</li><li>abstract和static不能同时使用<br>###抽象方法：没有方法体的方法，用abstract关键字修饰<h2 id="接口"><a href="#接口" class="headerlink" title="接口"></a>接口</h2></li><li>interface关键字修饰，产生一个完全的抽象类，接口中的方法都是抽象方法，接口也不能被实例化</li><li>在接口中定义的方法必须被public修饰，而抽象类中的方法可以被其他访问权限修饰符修饰，如果不写java默认会用public修饰 </li><li>接口中的变量默认被static和final关键字修饰，可以直接被接口名调用，不能被修改</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;抽象类和抽象方法&quot;&gt;&lt;a href=&quot;#抽象类和抽象方法&quot; class=&quot;headerlink&quot; title=&quot;抽象类和抽象方法&quot;&gt;&lt;/a&gt;抽象类和抽象方法&lt;/h2&gt;&lt;h3 id=&quot;抽象类：如果一个类中有一个或多个抽象方法，那么这个类必须是抽象类。&quot;&gt;&lt;a hre
      
    
    </summary>
    
    
      <category term="Java基础" scheme="http://yoursite.com/categories/Java%E5%9F%BA%E7%A1%80/"/>
    
    
      <category term="导航" scheme="http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"/>
    
      <category term="分享" scheme="http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"/>
    
  </entry>
  
  <entry>
    <title>LinuxDay03</title>
    <link href="http://yoursite.com/2020/02/25/LinuxDay03/"/>
    <id>http://yoursite.com/2020/02/25/LinuxDay03/</id>
    <published>2020-02-25T10:45:09.000Z</published>
    <updated>2020-02-25T13:53:45.484Z</updated>
    
    <content type="html"><![CDATA[<h2 id="网络地址"><a href="#网络地址" class="headerlink" title="网络地址"></a>网络地址</h2><h3 id="配置网络的时候需要配置下面的信息"><a href="#配置网络的时候需要配置下面的信息" class="headerlink" title="配置网络的时候需要配置下面的信息"></a>配置网络的时候需要配置下面的信息</h3><ol><li>IP地址：PC在网络中的通信地址</li><li>子网掩码: 子网掩码用于划分网络，将一个IP地址中的网络位和主机位进行划分。是一个32位的地址。</li><li>网关： 网络的关口，用于数据转发，通常理解为路由器的地址，大部分硬件厂家默认地址，192.168.0.1 |192.168.1.1</li><li>DNS: 用于解析域名的作用，Domain Name System 域名解析系统<h2 id="DNS"><a href="#DNS" class="headerlink" title="DNS"></a>DNS</h2><h3 id="静态"><a href="#静态" class="headerlink" title="静态"></a>静态</h3>优点：可以是我们PC/服务器有一个更快的解析速度。维护方式手动配置hosts文件，配置ip和域名的映射<br>缺点：hosts一般都是为本机系统所有，维护上万台的服务器的集群很困难<h3 id="动态"><a href="#动态" class="headerlink" title="动态"></a>动态</h3>优点：只需要给服务器指明DNS服务器地址即可，无需手动配置hosts文件<br>缺点：有一定的响应时间（延迟），若DNS服务器党机，那么此域名就没有办法访问<h2 id="桥接模式和NAT模式的优缺点"><a href="#桥接模式和NAT模式的优缺点" class="headerlink" title="桥接模式和NAT模式的优缺点"></a>桥接模式和NAT模式的优缺点</h2><h3 id="桥接模式："><a href="#桥接模式：" class="headerlink" title="桥接模式："></a>桥接模式：</h3>优点：同一个局域网中任意一台物理机想要访问虚拟机时，只要拥有账户和密码，就可以直接进行通信<br>缺点：如果宿主主机没有连接网络，那么虚拟机也就不存在与该真实网络的环境中，换句话，虚拟机使用桥接模式的时候，它的网络依赖于宿主的网络的环境。<h3 id="NAT"><a href="#NAT" class="headerlink" title="NAT"></a>NAT</h3>优点：可以无视物理机网络环境，即便时物理机没有网络，也不影响本机和虚拟机进行通信。因为虚拟机真正通信网卡是VMNet8提供（网络环境）<br>缺点：其他物理机想要访问NAT模式下的虚拟机时，比较麻烦<h2 id="Linux的网络知识"><a href="#Linux的网络知识" class="headerlink" title="Linux的网络知识"></a>Linux的网络知识</h2>通过域名查看IP<br> host <a href="http://www.baidu.com" target="_blank" rel="noopener">www.baidu.com</a><h3 id="远程拷贝"><a href="#远程拷贝" class="headerlink" title="远程拷贝"></a>远程拷贝</h3>本机与Linux之间的文件传输，在本机的控制台上,本机的文件的路径为绝对路径<br>例如：本机传送文件到Linux中 命令：scp /Users/yuxiangrui/Desktop/1.jpg <a href="mailto:root@192.168.144.134">root@192.168.144.134</a>:/home/<br> 本机传送文件夹，命令：scp -r /Users/yuxiangrui/Desktop/1.jpg <a href="mailto:root@192.168.143.134">root@192.168.143.134</a><br> linux中传送文件到本机 scp <a href="mailto:root@92.168.143.134">root@92.168.143.134</a>：/home/1.jpg /Users/yuxiangrui/Desktop/<br> linux中传送文件夹到本机 scp -r <a href="mailto:root@92.168.143.134">root@92.168.143.134</a>：/home/1.jpg /Users/yuxiangrui/Desktop/<h3 id="登陆远程服务器"><a href="#登陆远程服务器" class="headerlink" title="登陆远程服务器"></a>登陆远程服务器</h3> ssh <a href="mailto:root@192.168.143.134">root@192.168.143.134</a><h2 id="SSH免密登陆"><a href="#SSH免密登陆" class="headerlink" title="SSH免密登陆"></a>SSH免密登陆</h2>Linux免密登陆使用的RSA算法<br> RSA本身是一种非对称加密算法，会生成公钥和私钥<br>生成密钥<br> ssh-keygen<br>这样.ssh目录下就会有公钥和私钥的文件<br> 将公钥注册到其他服务器上<br>ssh-copy-id {UserName}@IP<h2 id="wget"><a href="#wget" class="headerlink" title="wget"></a>wget</h2> wget <a href="http://www.baidu.com" target="_blank" rel="noopener">http://www.baidu.com</a><h2 id="查看进程"><a href="#查看进程" class="headerlink" title="查看进程"></a>查看进程</h2>ps -aux<h2 id="查询特定的进程"><a href="#查询特定的进程" class="headerlink" title="查询特定的进程"></a>查询特定的进程</h2>ps -aux｜grep sshd</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;网络地址&quot;&gt;&lt;a href=&quot;#网络地址&quot; class=&quot;headerlink&quot; title=&quot;网络地址&quot;&gt;&lt;/a&gt;网络地址&lt;/h2&gt;&lt;h3 id=&quot;配置网络的时候需要配置下面的信息&quot;&gt;&lt;a href=&quot;#配置网络的时候需要配置下面的信息&quot; class=&quot;head
      
    
    </summary>
    
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
      <category term="导航" scheme="http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"/>
    
      <category term="分享" scheme="http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"/>
    
  </entry>
  
  <entry>
    <title>LinuxDay02</title>
    <link href="http://yoursite.com/2020/02/24/LinuxDay02/"/>
    <id>http://yoursite.com/2020/02/24/LinuxDay02/</id>
    <published>2020-02-24T11:07:09.000Z</published>
    <updated>2020-02-24T14:06:52.707Z</updated>
    
    <content type="html"><![CDATA[<h2 id="用户、用户组概念"><a href="#用户、用户组概念" class="headerlink" title="用户、用户组概念"></a>用户、用户组概念</h2><h2 id="Linux系统是一个多用户多任务的分时操作系统，任何一个要使用系统资源的用户都必须首先向系统管理员申请账号，然后以这个账号的身份进入系统。用户在登陆时建入正确的用户名和口令后，就能够进入系统和自己的主目录。"><a href="#Linux系统是一个多用户多任务的分时操作系统，任何一个要使用系统资源的用户都必须首先向系统管理员申请账号，然后以这个账号的身份进入系统。用户在登陆时建入正确的用户名和口令后，就能够进入系统和自己的主目录。" class="headerlink" title="Linux系统是一个多用户多任务的分时操作系统，任何一个要使用系统资源的用户都必须首先向系统管理员申请账号，然后以这个账号的身份进入系统。用户在登陆时建入正确的用户名和口令后，就能够进入系统和自己的主目录。"></a>Linux系统是一个多用户多任务的分时操作系统，任何一个要使用系统资源的用户都必须首先向系统管理员申请账号，然后以这个账号的身份进入系统。用户在登陆时建入正确的用户名和口令后，就能够进入系统和自己的主目录。</h2><h2 id="用户管理"><a href="#用户管理" class="headerlink" title="用户管理"></a>用户管理</h2><h2 id="gt-gt-gt-用法"><a href="#gt-gt-gt-用法" class="headerlink" title="&gt; &gt;&gt; 用法"></a>&gt; &gt;&gt; 用法</h2><p>ls -l &gt; a.txt 把显示的内容覆盖写入a.txt，如果没有这个文件则创建此文件</p><blockquote><blockquote><p>追加，例如ls -l &gt;&gt; a.txt把当前显示的内容追加到a.txt的末尾。</p></blockquote></blockquote><h2 id="echo"><a href="#echo" class="headerlink" title="echo"></a>echo</h2><p>输出打印 echo “hello”</p><h2 id="head"><a href="#head" class="headerlink" title="head"></a>head</h2><p>显示文件的前10行head -n 10 /etc/peofile</p><h2 id="tail"><a href="#tail" class="headerlink" title="tail"></a>tail</h2><p>tail -n 5 /etc/profile 查看文件的后5行<br>tail -f 实时监控文件是否变化</p><h2 id="chmod-修改用户的权限"><a href="#chmod-修改用户的权限" class="headerlink" title="chmod 修改用户的权限"></a>chmod 修改用户的权限</h2><p>4:可读 2:可写 1:可执行0:没有任何权限 设定用户权限可读可写可执行，用户组全前线可读可执行 其他没有权限 chmod 750 文件or文件夹</p><h2 id="usermod-将用户加入用户组"><a href="#usermod-将用户加入用户组" class="headerlink" title="usermod 将用户加入用户组"></a>usermod 将用户加入用户组</h2><p>usermod -g A zhangsan 将张三加入A组中</p><h2 id="chown-修改文件目录属主"><a href="#chown-修改文件目录属主" class="headerlink" title="chown 修改文件目录属主"></a>chown 修改文件目录属主</h2><p>chown root：big  A 将A的属主改成big</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;用户、用户组概念&quot;&gt;&lt;a href=&quot;#用户、用户组概念&quot; class=&quot;headerlink&quot; title=&quot;用户、用户组概念&quot;&gt;&lt;/a&gt;用户、用户组概念&lt;/h2&gt;&lt;h2 id=&quot;Linux系统是一个多用户多任务的分时操作系统，任何一个要使用系统资源的用户都必须
      
    
    </summary>
    
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
      <category term="导航" scheme="http://yoursite.com/tags/%E5%AF%BC%E8%88%AA/"/>
    
      <category term="分享" scheme="http://yoursite.com/tags/%E5%88%86%E4%BA%AB/"/>
    
  </entry>
  
</feed>
